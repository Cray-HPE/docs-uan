[
{
	"uri": "/docs-uan/en-254/test-plan/",
	"title": "Test-plan",
	"tags": [],
	"description": "",
	"content": "Test-plan Topics:\n Test-plan  "
},
{
	"uri": "/docs-uan/en-254/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Troubleshooting Topics:\n Troubleshoot UAN Boot Issues Troubleshoot UAN CFS And Network Configuration Issues Troubleshoot UAN Disk Configuration Issues  "
},
{
	"uri": "/docs-uan/en-254/upgrade/",
	"title": "Upgrade",
	"tags": [],
	"description": "",
	"content": "Upgrade Topics:\n Merge UAN Configuration Data Notable Changes Upgrades  "
},
{
	"uri": "/docs-uan/en-254/install/",
	"title": "Install",
	"tags": [],
	"description": "",
	"content": "Install Topics:\n Install The UAN Product Stream  "
},
{
	"uri": "/docs-uan/en-254/installation_prereqs/",
	"title": "Installation Prereqs",
	"tags": [],
	"description": "",
	"content": "Installation Prereqs Topics:\n Configure The Bios Of A Gigabyte UAN Configure The Bios Of An HPE UAN Configure The BMC For UANs With Ilo Hardware And Software Prerequisites Prepare For UAN Product Installation Images  "
},
{
	"uri": "/docs-uan/en-254/operations/",
	"title": "Operations",
	"tags": [],
	"description": "",
	"content": "Operations Topics:\n About UAN Configuration Boot UANs Build A New UAN Image Using The Cos Recipe Configure Interfaces On UANs Configure Pluggable Authentication Modules (pam) On UANs Create UAN Boot Images Mount A New File System On An UAN UAN Ansible Roles  "
},
{
	"uri": "/docs-uan/en-254/advanced/",
	"title": "Advanced",
	"tags": [],
	"description": "",
	"content": "Advanced Topics:\n Customizing UAN Images Manually Enabling CAN Chn Repurposing Compute As UAN Sles Image  "
},
{
	"uri": "/docs-uan/en-254/operations/create_uan_boot_images/",
	"title": "Create UAN Boot Images",
	"tags": [],
	"description": "",
	"content": "Create UAN Boot Images This procedure updates the configuration management git repository to match the installed version of the UAN product. That updated configuration is then used to create UAN boot images and a BOS session template.\nUAN specific configuration, and other required configurations related to UANs are covered in this topic. Refer to HPE Cray EX System Software Getting Started Guide for further information on configuring other HPE products (for example, workload managers and the HPE Cray Programming Environment) that may be configured on the UANs.\nThis is the overall workflow for preparing UAN images to boot UANs:\n Clone the UAN configuration git repository and create a branch based on the branch imported by the UAN installation. Update the configuration content and push the changes to the newly created branch. Use Shasta Admin Toolkit (SAT) command sat bootprep, to automate the creation of IMS image, CFS configurations, and BOS session templates.  Once the UAN BOS session template is created, the UANs will be ready to be booted by a BOS session.\nReplace PRODUCT_VERSION and CRAY_EX_HOSTNAME in the example commands in this procedure with the current UAN product version installed (See Step 1) and the hostname of the HPE Cray EX system, respectively.\nPREPARE CFS CONFIGURATION\n  Obtain the artifact IDs and other information from the cray-product-catalog Kubernetes ConfigMap. Record the following information:\n the clone_url the import_branch value  Upon successful installation of the UAN product, the UAN configuration is cataloged in this ConfigMap. This information is required for this procedure.\nPRODUCT_VERSION will be replaced by a numbered version string, such as 2.1.7 or 2.3.0.\nncn-m001# kubectl get cm -n services cray-product-catalog -o json | jq -r .data.uan PRODUCT_VERSION: configuration: clone_url: https://vcs.CRAY_EX_HOSTNAME/vcs/cray/uan-config-management.git commit: 6658ea9e75f5f0f73f78941202664e9631a63726 import_branch: cray/uan/PRODUCT_VERSION import_date: 2021-02-02 19:14:18.399670 ssh_url: git@vcs.CRAY_EX_HOSTNAME:cray/uan-config-management.git   Optional Generate the password hash for the root user. Replace PASSWORD with the root password you wish to use. If an upgrade or image rebuild is being performed, the root password may have already been added to vault.\nncn-m001# openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c4) PASSWORD   Optional Obtain the HashiCorp Vault root token.\nncn-m001# kubectl get secrets -n vault cray-vault-unseal-keys -o jsonpath=\u0026#39;{.data.vault-root}\u0026#39; | base64 -d; echo   Optional Write the password hash obtained in Step 2 to the HashiCorp Vault.\nThe vault login command will request a token. That token value is the output of the previous step. The vault read secret/uan command verifies that the hash was stored correctly. This password hash will be written to the UAN for the root user by CFS.\nncn-m001# kubectl exec -it -n vault cray-vault-0 -- sh export VAULT_ADDR=http://cray-vault:8200 vault login vault write secret/uan root_password=\u0026#39;HASH\u0026#39; vault read secret/uan   Optional Write any uan_ldap sensitive data, such as the ldap_default_authtok value, to the HashiCorp Vault.\nThe vault login command will request a token. That token value is the output of the Step 3. The vault read secret/uan_ldap command verifies that the uan_ldap data was stored correctly. Any values stored here will be written to the UAN /etc/sssd/sssd.conf file in the [domain] section by CFS.\nThis example shows storing a value for ldap_default_authtok. If more than one variable needs to be stored, they must be written in space separated key=value pairs on the same vault write secret/uan_ldap command line.\nncn-m001# kubectl exec -it -n vault cray-vault-0 -- sh export VAULT_ADDR=http://cray-vault:8200 vault login vault write secret/uan_ldap ldap_default_authtok=\u0026#39;TOKEN\u0026#39; vault read secret/uan_ldap   Obtain the password for the crayvcs user from the Kubernetes secret for use in the next command.\nncn-m001# VCS_USER=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_username}} | base64 --decode) VCS_PASS=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode)   Clone the UAN configuration management repository. Replace CRAY_EX_HOSTNAME in the clone url with api-gw-service-nmn.local when cloning the repository.\nThe repository is in the VCS/Gitea service and the location is reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.clone_url key. The CRAY_EX_HOSTNAME from the clone_url is replaced with api-gw-service-nmn.local in the command that clones the repository.\nncn-m001# git clone https://$VCS_USER:$VCS_PASS@api-gw-service-nmn.local/vcs/cray/uan-config-management.git . . . ncn-m001# cd uan-config-management \u0026amp;\u0026amp; git checkout cray/uan/PRODUCT_VERSION \u0026amp;\u0026amp; git pull Branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; set up to track remote branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; from \u0026#39;origin\u0026#39;. Already up to date.   Create a branch using the imported branch from the installation to customize the UAN image.\nThis will be reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.import_branch key under the UAN section. The format is cray/uan/PRODUCT_VERSION. In this guide, an integration branch is used for examples, but the name can be any valid git branch name.\nModifying the cray/uan/PRODUCT_VERSION branch that was created by the UAN product installation is not allowed by default.\nncn-m001# git checkout -b integration \u0026amp;\u0026amp; git merge cray/uan/PRODUCT_VERSION Switched to a new branch \u0026#39;integration\u0026#39; Already up to date.   Apply any site-specific customizations and modifications to the Ansible configuration for the UAN nodes and commit the changes.\nThe default Ansible play to configure UAN nodes is site.yml in the base of the uan-config-management repository. The roles that are executed in this play allow for custom configuration as required for the system.\nConsult the individual Ansible role README.md files in the uan-config-management repository roles directory to configure individual role variables. Roles prefixed with uan_ are specific to UAN configuration and include network interfaces, disk, LDAP, software packages, and message of the day roles.\nNOTE: Admins must ensure the uan_can_setup variable is set to the correct value for the site. This variable controls how the nodes are configured for user access. When uan_can_setup is yes, user access is over the CAN or CHN, based on the BICAN System Default Route setting in SLS. When uan_can_setup is no, the Admin must configure the user access interface and default route. See Configure Interfaces on UANs\nWarning: Never place sensitive information such as passwords in the git repository.\nThe following example shows how to add a vars.yml file containing site-specific configuration values to the Application_UAN group variable location.\nThese and other Ansible files do not necessarily need to be modified for UAN image creation. See About UAN Configuration for instructions for site-specific UAN configuration, including CAN/CHN configuration.\nncn-m001# vim group_vars/Application_UAN/vars.yml ncn-m001# git add group_vars/Application_UAN/vars.yml ncn-m001# git commit -m \u0026#34;Add vars.yml customizations\u0026#34; [integration ecece54] Add vars.yml customizations 1 file changed, 1 insertion(+) create mode 100644 group_vars/Application_UAN/vars.yml   Push the changes to the repository using the proper credentials, including the password obtained previously.\nncn-m001# git push --set-upstream origin integration Username for \u0026#39;https://api-gw-service-nmn.local\u0026#39;: crayvcs Password for \u0026#39;https://crayvcs@api-gw-service-nmn.local\u0026#39;: . . . remote: Processed 1 references in total To https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git * [new branch] integration -\u0026gt; integration Branch \u0026#39;integration\u0026#39; set up to track remote branch \u0026#39;integration\u0026#39; from \u0026#39;origin\u0026#39;. The configuration parameters have been stored in a branch in the UAN git repository. The next phase of the process uses sat bootprep to handle creating the CFS configurations, IMS images, and BOS sessiontemplates for UANs.\n  CREATE UAN IMAGES\nWith Shasta Admin Toolkit (SAT) version 2.2.16 and later, it is recommended that administrators create an input file for use with sat bootprep. Use the following command to determine which version of SAT is installed:\nncn-m001# sat showrev --products --filter \u0026#39;product_name=\u0026#34;sat\u0026#34;\u0026#39; A sat bootprep input file will have three sections: configurations, images, and session_templates. These sections create CFS configurations, IMS images, and BOS session templates respectively. Each section may have multiple elements to create more than one CFS, IMS, or BOS artifact. The format is similar to the input files for CFS, IMS, and BOS, but SAT will automate the process with fewer steps. Follow the subsections below to create a UAN bootprep input file.\nRefer to HPE Cray EX System Software Getting Started Guide for further information on configuring other HPE products, as this procedure documents only the required configuration of the UAN.\nSAT Bootprep Configuration\nThe SAT bootprep input file should have a configuration section that specifies each layer to be included in the CFS configuration for the UAN images for image customization and node personalization. This section will result in a CFS configuration named uan-config. The versions of each layer may be gathered using sat showrev.\nNote that the Slingshot Host Software CFS layer is listed first. This is required as the UAN layer will attempt to install DVS and Lustre packages that require SHS be installed first. The correct playbook for Cassini or Mellanox must also be specified. Consult the Slingshot Host Software documentation for more information.\nconfigurations: - name: uan-config layers: - name: slingshot-host-software playbook: shs_mellanox_install.yml product: name: slingshot-host-software version: 2.0.0 branch: integration ... add configuration layers for other products here, if desired ... - name: uan playbook: site.yml product: name: uan version: 2.4.0 branch: integration SAT Bootprep Image\nThe SAT bootprep input file should have a section that specifies which IMS images to create for UAN nodes. UANs are built using the COS recipe, so the section below specifies which image recipe to use based on what is provided by COS. To determine which COS recipes are available run the following command:\nncn-m001# sat showrev --products --filter \u0026#39;product_name=\u0026#34;cos\u0026#34;\u0026#39; This example will create an IMS image with the name cray-shasta-uan-sles15sp3.x86_64-2.3.25. An appropriate name should be used to correctly identify the UAN image being built. Also note that the CFS configuration uan-config is being referenced so that CFS image customization will be run using that configuration along with the specified node groups.\nimages: - name: cray-shasta-uan-sles15sp3.x86_64-2.3.25 ims: is_recipe: true name: cray-shasta-compute-sles15sp3.x86_64-2.3.25 configuration: uan-config configuration_group_names: - Application - Application_UAN SAT Bootprep Session Template\nThe final section of the SAT bootprep input file creates BOS session templates. This section references the named IMS image that sat bootprep generates, as well as a CFS configuration. The boot_sets key \u0026ldquo;uan\u0026rdquo; may be changed as needed. If there are more than one boot_sets in the session template, each key will need to be unique.\nsession_templates: - name: uan-2.4.0 image: cray-shasta-uan-sles15sp3.x86_64-2.3.25 configuration: uan-config bos_parameters: boot_sets: uan: kernel_parameters: spire_join_token=${SPIRE_JOIN_TOKEN} node_roles_groups: - Application_UAN Run SAT Bootprep\nInitiate the sat bootprep command to generate the configurations and artifacts needed to boot UANs. This command may take some time as it will initiate IMS image creation and CFS image customization.\nncn-m001# sat bootprep run uan-bootprep.yaml If changes are necessary to complete sat bootprep with the provided input file, make adjustments to the CFS layers or input file as needed and rerun the sat bootprep command. If any artifacts are going to be overwritten, SAT will prompt for confirmation before taking action. This is useful when making CFS changes as SAT will automatically configure the layers to use the latest git commits if the branches are specified correctly.\nOnce sat bootprep completes successfully, save the input file to a known location. This input file will be useful to regenerate artifacts as changes are made or different product layers are added.\nFinally, perform Boot UANs to boot the UANs with the new BOS session template.\n"
},
{
	"uri": "/docs-uan/en-254/upgrade/merge_uan_configuration_data/",
	"title": "Merge UAN Configuration Data",
	"tags": [],
	"description": "",
	"content": "Merge UAN Configuration Data Perform this procedure to update the UAN product configuration.\nBefore performing this procedure, perform Install the UAN Product Stream\nIn this procedure, an upgrade from UAN 2.0.0 to UAN 2.3.1 is being performed. Administrators should replace the versions seen in this procedure with the versions being upgraded on the system. Additionally, this guide describes upgrading an integration branch. Each CFS branch responsible for configuring the various types of Application and UANs should be upgraded.\n  Start a typescript to capture the commands and output from the procedure.\nncn-m001# script -af product-uan.$\\(date +%Y-%m-%d\\).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39;   Obtain the URL of the UAN configuration management repository in VCS (the Gitea service).\nThis URL is reported as the value of the configuration.clone_url key in the cray-product-catalog Kubernetes ConfigMap.\n  Obtain the crayvcs password.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\  --template={{.data.vcs_password}} | base64 --decode   Clone the UAN configuration management repository. Replace the hostname reported in the URL obtained in the previous step with api-gw-service-nmm.local when cloning the repository.\nncn-m001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git . . . ncn-m001# cd uan-config-management \u0026amp;\u0026amp; git checkout cray/uan/PRODUCT_VERSION \u0026amp;\u0026amp; git pull Branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; set up to track remote branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; from \u0026#39;origin\u0026#39;. Already up to date.   Checkout the branch currently used to hold UAN configuration.\nThe following example assumes that branch is integration.\nncn-m001# git checkout integration Switched to branch \u0026#39;integration\u0026#39; Your branch is up to date with \u0026#39;origin/integration\u0026#39;.   Merge the new install branch to the current branch. Write a commit message when prompted.\nncn-m001# git merge cray/uan/PRODUCT_VERSION   Push the changes to VCS. Enter the crayvcs password when prompted.\nncn-m001# git push   Retrieve the commit ID from the merge and store it for later use.\nncn-m001# git rev-parse --verify HEAD   Update any CFS configurations used by the UANs with the commit ID from the previous step.\n  Download the JSON of the current UAN CFS configuration to a file.\nThe CFS configuration uan-config-2.0.0 is an example, the site should use the CFS configuration used for the UANs being upgraded.\nThis file will be named uan-config-2.3.1.json since it will be modified and then used for the updated UAN version.\nncn-m001# cray cfs configurations describe uan-config-2.0.0 \\ --format=json \u0026amp;\u0026gt;uan-config-2.3.1.json   Remove the unneeded lines from the JSON file.\nThe lines to remove are:\n the lastUpdated line the last name line  These must be removed before uploading the modified JSON file back into CFS to update the UAN configuration.\nncn-m001# cat uan-config-2.3.1.json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-03-27T02:32:10Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;aa5ce7d5975950ec02493d59efb89f6fc69d67f1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;uan-integration-2.0.0\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;uan-config-2.0.0\u0026#34; }   Replace the commit value in the JSON file with the commit ID obtained with git rev-parse --verify HEAD.\nThe name value after the commit line may also be updated to match the new UAN product version, if desired. This is not necessary as CFS does not use this value for the configuration name.\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-configmanagement.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;aa5ce7d5975950ec02493d59efb89f6fc69d67f1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;uan-integration-2.0.0\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ] }   Create a new UAN CFS configuration with the updated JSON file.\nThe following example uses uan-config-2.3.1 for the name of the new CFS configuration, to match the JSON file name.\nncn-m001# cray cfs configurations update uan-config-2.3.1 \\ --file uan-config-2.3.1.json     Finish the typescript file started at the beginning of this procedure.\nncn-m001# exit   Deploy the updated UAN product software to the User Access Nodes.\nContinue with \u0026ldquo;Build a New UAN Image Using the COS Recipe\u0026rdquo; and \u0026ldquo;Create UAN Boot Images\u0026rdquo; in the publication HPE Cray User Access Node (UAN) Software Administration Guide to upgrade the boot images and perform CFS image customization for the UAN images.\n  "
},
{
	"uri": "/docs-uan/en-254/operations/mount_a_new_file_system_on_an_uan/",
	"title": "Mount A New File System On A UAN",
	"tags": [],
	"description": "",
	"content": "Mount a New File System on a UAN Perform this procedure to create a mount point for a new file system on a UAN.\n  Perform Steps 1-9 of Create UAN Boot Images.\n  Create a directory for Application role nodes.\nncn-w001# mkdir -p group_vars/Application   Define the home directory information for the new file system in the filesystems.yml file.\nncn-w001# vi group_vars/Application/filesystems.yml --- filesystems: - src: 10.252.1.1:/home mount_point: /home fstype: nfs4 opts: rw,noauto state: mounted   Add the change from the working directory to the staging area.\nncn-w001# git add -A   Commit the file to the working branch.\nncn-w001# git commit -am \u0026#39;Added file system info\u0026#39;   Resume Create UAN Boot Images at Step 10.\n  "
},
{
	"uri": "/docs-uan/en-254/upgrade/notable_changes/",
	"title": "Notable Changes",
	"tags": [],
	"description": "",
	"content": "Notable Changes The following guide describes changes included in a particular UAN version that may be of note during an install or upgrade.\nWhen an upgrade is being performed, please review the notable changes for all of the UAN versions up to the version being installed. If a particular version does not appear in this guide, it may have only had minor changes. For a full account of the changes involved in a release, consult the ChangeLog.md file at the root of the UAN product repository.\nUAN 2.2  UAN 2.2 was an internal release and was not made generally available.  UAN 2.3.1  UAN 2.3 no longer ships a default recipe or image. To build a UAN image, administrators should select a COS recipe to build as the base of their UAN and Application Nodes. The role uan_packages will now install the rpms needed by UAN and Application Nodes The role uan_packages supports GPG checking when the CSM version is 1.2 or greater.  uan_disable_gpg_check: yes must be set if CSM is earlier than 1.2 uan_disable_gpg_check: no should be set if CSM is 1.2 or greater    UAN 2.4.0  UAN 2.4.0 adds support for a Bifurcated Customer Access Network (BiCAN) and the ability to specify a default route other than the CAN or CHN when they are selected.  Application nodes may now choose to implement user access over either the existing Customer Access Network (CAN), the new Customer High Speed Network (CHN), or a direct connection to the customers user network. By default, a direct connection is selected as it was in previous releases.  uan_can_setup, when set to yes, selects the customer access network implementation based on the setting of the BICAN System Default Route in SLS. Application nodes may now set a default route other than the CAN or CHN default route when uan_can_setup: yes. uan_customer_default_route: true will allow a customer defined default route to be set using the customer_uan_routes structure when uan_can_setup is set to yes.     sat bootprep is now used in the documentation to streamline the IMS, CFS, and BOS commands to create and customize images and creating sessiontemplates.  UAN 2.4.1  The UAN CFS playbook now supports a section for Compute nodes. The Compute section will run the role uan_interfaces to provide Customer High Speeed Network (CHN) routing.  CHN on the Compute nodes requires:  Customer High Speed Network has been enabled in CSM. See \u0026ldquo;Enabling Customer High Speed Network Routing\u0026rdquo; in the CSM Documentation UAN CFS configurd with uan_can_setup: yes Fully configured HSN SLS has IP assignments for compute nodes on hsn0     Updates to GPU roles to match COS 2.3  UAN 2.4.2  There is a known issue with the version of GPU support included in the UAN CFS repo. The result is that both AMD and Nvidia SDKs are not able to be projected at the same time. Until this is resolved in a later release, modify the site.yml in the UAN CFS repo to only include either amd or nvidia.  UAN 2.4.3  A new CFS role, uan_hardening adds iptables rules that will block SSH traffic to NCNs. See the README.md in the uan_hardening role for more information.  UAN 2.5.3  A technical preview of a standard SLES image for UAN/Application nodes is included. Support SLES15SP4 COS based images  "
},
{
	"uri": "/docs-uan/en-254/test-plan/test-plan/",
	"title": "Test Plan For User Access Node (UAN) And Repurposed Compute Node As Uan",
	"tags": [],
	"description": "",
	"content": "Test Plan for User Access Node (UAN) and Repurposed Compute Node as UAN The following is the test plan for the User Access Node (UAN) or a Compute Node that has been repurposed to function as a UAN. The tests are grouped in three categories:\n Unit tests Integration tests Functional tests  Unit Tests When possible, unit tests are run when the various components of UAN are built (uan-rpms, uan, and uan-product-stream). The uan repository contains the ansible code used when UANs boot and configure and much of the testing for that compoment must occur on a fully configured system. Future enhancements are underway to test the various compoments of UAN on a Virtual Shasta enviroment in GCP.\n   Summary Description Automated Notes     uan builds Verify the uan repo is able to generate artifacts yes Run make in a local checkout   uan-rpms builds Verify the uan-rpms repo is able to generate rpms yes Run make in a local checkout   uan-product-stream builds Verify the uan-product-stream repo is able to generate a release yes Tag a new release in uan-product-stream or run make in a local checkout    Integration Tests The following integration tests verify that the UAN software interacts correctly with the rest of the products. The result of the integration tests will be a fully built and customized UAN image that boots and configures correctly. Depending on the configuration of the test system, extra integrations tests may be performed (HSN booting, WLM configuration, GPU configuration, etc).\n   Summary Description Automated Notes     Install the UAN product Run the install procedure Yes Run install.sh from Install_the_UAN_Product_Stream.md   Build a COS recipe for UAN Verify the COS recipe can be built as the base for a UAN image no Build_a_New_UAN_Image_Using_the_COS_Recipe.md   Run UAN CFS Verify the CFS layers run correctly (SHS, UAN, WLM, CPE, etc) no Create_UAN_Boot_Images.md   Boot UANs Verify the UAN boots successfully no Boot_UANS.md   Enable HSN booting Verify the UAN is able to HSN boot no Consult COS documentation   Enable GPU Verify the UAN is able to configure GPUs no Consult GPU documentation    Functional Tests With a fully configured Shasta system, the following functional tests determines that a UAN is able to perform its intended capabilities. These tests apply to both native UANs and Compute Nodes which have been repurposed as UANs.\n   Summary Description Automated Notes     User Authentication Verify a user is able to ssh to the UAN using LDAP authentication. no ssh user@uan   Job launch Verify a user is able to submit a basic job. no srun hostname   Verify CPE Verify the Cray Programming Environment is available no module list   Verify GPU functionality Run the test suite if GPUs are configured yes /opt/cray/uan/tests/validate-gpu.sh \u0026lt;nvidia|amd\u0026gt;   Verify CAN or CHN Configuration Inspect the network interfaces and default routes used for CAN or CHN no For systems running CAN, there must be a can0 interface present and the default route should be over that device.For systems running CHN (including Compute Nodes repurposed as UANs), the hsn0 interface must have a CHN IP in addition to the HSN IP and the default route should be over the hsn0 device.    "
},
{
	"uri": "/docs-uan/en-254/troubleshooting/troubleshoot_uan_boot_issues/",
	"title": "Troubleshoot UAN Boot Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAN Boot Issues The UAN boot process BOS boots UANs. BOS uses session templates to define various parameters such as:\n Which nodes to boot Which image to boot Kernel parameters Whether to perform post-boot configuration (Node Personalization) of the nodes by CFS. Which CFS configuration to use if Node Personalization is enabled.  UAN boots are performed in three phases:\n PXE booting an iPXE binary that will load the initrd of the UAN image that will boot. Booting the initrd (dracut) image which configures the UAN for booting the UAN image. This process consists of two phases.  Configuring the UAN node to use the Content Projection Service (CPS) and Data Virtualization Service (DVS). These services manage the UAN image rootfs mounting and make that image available to the UAN nodes. Mounting the rootfs   Booting the UAN image rootfs.  PXE Issues Most PXE boot failures are the result of misconfigured network switches and/or BIOS settings. The UAN must PXE boot over the Node Management Network (NMN) and the switches must be configured to allow connectivity to the NMN. The cable for the NMN must be connected to the first port of the OCP card on HPE DL325 and DL385 servers or to the first port of the built-in LAN-On-Motherboard (LOM) on Gigabyte servers. See \u0026ldquo;Prepare for UAN Product Installation\u0026rdquo; in the UAN Installation Guide for details on the switch and BIOS settings required to configure the UAN for PXE booting.\nUANs may fail to boot when the BIOS EFITIME is too far away from the time on management nodes. If there are x509 certificate problems, check that the BIOS time is correct. See \u0026ldquo;Configure the BIOS of an HPE UAN\u0026rdquo; or \u0026ldquo;Configure the BIOS of a Gigabyte UAN\u0026rdquo; in the UAN Installation Guide for examples of checking settings in the BIOS.\nInitrd (Dracut) Issues Dracut failures are often caused by the wrong interface being named nmn0, or to multiple entries for the UAN xname in DNS. The latter is a result of multiple interfaces making DHCP requests. Either condition can cause IP address mismatches in the dvs_node_map. DNS configures entries based on DHCP leases.\nWhen dracut starts, it renames the network device named by the ifmap=netX:nmn0 kernel parameter to nmn0. This interface is the only one dracut will enable DHCP on. The ip=nmn0:dhcp kernel parameter limits dracut to DHCP only nmn0. The ifmap value must be set correctly in the kernel_parameters field of the BOS session template.\nSee Create UAN Boot Images for details on how to configure the BOS session template. For UAN nodes that have more than one PCI card installed, ifmap=net2:nmn0 is the correct setting. If only one PCI card is installed, ifmap=net0:nmn0 is normally the correct setting.\nUANs require CPS and DVS to boot from images. These services are configured in dracut to retrieve the rootfs and mount it. If the image fails to download, check that DVS and CPS are both healthy, and DVS is running on all worker nodes. Run the following commands to check DVS and CPS:\nncn-m001# kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; --no-headers ncn-w001 ncn-w002 ncn-m001# for node in `kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; \\ --no-headers`; do ssh $node \u0026#34;lsmod | grep \u0026#39;^dvs \u0026#39;\u0026#34; done ncn-w001 ncn-w002 If DVS and CPS are both healthy, then both of these commands will return all the worker NCNs in the HPE Cray EX system.\nImage Boot Issues Once dracut exits, the UAN will boot the rootfs image. Failures seen in this phase tend to be failures of spire-agent, cfs-state-reporter, or both. The cfs-state-reporter tells BOA that the node is ready and allows BOA to start CFS for Node Personalization. If cfs-state-reporter does not start, check if the spire-agent has started. The cfs-state-reporter depends on the spire-agent. Running systemctl status spire-agent will show that that service is enabled and running if there are no issues with that service. Similarly, running systemctl status cfs-state-reporter will show a status of SUCCESS.\nuan# systemctl status spire-agent ● spire-agent.service - SPIRE Agent Loaded: loaded (/usr/lib/systemd/system/spire-agent.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2021-02-24 14:27:33 CST; 19h ago Main PID: 3581 (spire-agent) Tasks: 57 CGroup: /system.slice/spire-agent.service └─3581 /usr/bin/spire-agent run -expandEnv -config /root/spire/conf/spire-agent.conf uan# systemctl status cfs-state-reporter ● cfs-state-reporter.service - cfs-state-reporter reports configuration level of the system Loaded: loaded (/usr/lib/systemd/system/cfs-state-reporter.service; enabled; vendor preset: enabled) Active: inactive (dead) since Wed 2021-02-24 14:29:51 CST; 19h ago Main PID: 3827 (code=exited, status=0/SUCCESS) There may be errors related to failing to load kernel modules during the boot:\nFAILED Failed to start Load Kernel Modules. See \u0026#39;systemctl status systemd-modules-load.service\u0026#39; for details. Provided the UAN boots and completes post boot customizations, these messages may be ignored.\n"
},
{
	"uri": "/docs-uan/en-254/troubleshooting/troubleshoot_uan_cfs_and_network_configuration_issues/",
	"title": "Troubleshoot UAN CFS And Network Configuration Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAN CFS and Network Configuration Issues Examine the UAN CFS pod logs to help troubleshoot CFS and networking issues on UANs.\nRead About UAN Configuration before starting this procedure.\n  Obtain the name of the CFS session that failed by running the following command on a management or worker NCN:\nThis example sorts the list of CFS sessions so that the most recent one is at the bottom.\nncn# kubectl -n services get pods --sort-by=.metadata.creationTimestamp | grep ^cfs   View the Ansible log of the CFS session found in the previous step (CFS_SESSION in the following example). Use the information in log to guide troubleshooting.\nncn# kubectl -n services logs -f -c ansible-0 CFS_SESSION   Optional: Troubleshoot uan_interfaces issues by logging into the affected node (usually with the conman console) and using standard network debugging techniques.\nNMN and CAN/CHN network setup errors can also result from incorrect switch configuration and network cabling.\n  "
},
{
	"uri": "/docs-uan/en-254/troubleshooting/troubleshoot_uan_disk_configuration_issues/",
	"title": "Troubleshoot UAN Disk Configuration Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAN Disk Configuration Issues Perform this procedure to enable uan_disk_config to run successfully by erasing existing disk partitions. UAN disk configuration will fail if the disk on the node is already partitioned. Manually erase any existing partitions to fix the issue.\nThis procedure currently only addresses uan_disk_config errors due to existing disk partitions.\nRefer to About UAN Configuration for an explanation of UAN disk configuration.\nThe most common cause of failure in the uan_disk_config role is the disk having been previously configured without a /scratch and /swap partition. Existing partitions prevent the parted command from dividing the disk into those two equal partitions. The solution is to log into the node and run parted manually to remove the existing partitions on that disk.\n  Examine the CFS log and identify the failed disk device.\n  Log into the affected UAN as root.\n  Use parted to manually remove any existing partitions.\nThe following example uses /dev/sdb as the disk device. Also, as partitions are removed, the remaining partitions are renumbered. Therefore, rm 1 is issued twice to remove both partitions.\nuan# parted GNU Parted 3.2 Using /dev/sda Welcome to GNU Parted! Type \u0026#39;help\u0026#39; to view a list of commands. (parted) select /dev/sdb Using /dev/sdb (parted) print Model: ATA VK000480GWSRR (scsi) Disk /dev/sdb: 480GB Sector size (logical/physical): 512B/4096B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 240GB 240GB primary ext4 type=83 2 240GB 480GB 240GB primary ext4 type=83 (parted) rm 1 (parted) rm 1 (parted) print Model: ATA VK000480GWSRR (scsi) Disk /dev/sdb: 480GB Sector size (logical/physical): 512B/4096B Partition Table: msdos Disk Flags: (parted) quit uan01:~ #   Either reboot the affected UAN or launch a CFS session against it to rerun the uan_disk_config role.\n  "
},
{
	"uri": "/docs-uan/en-254/operations/uan_ansible_roles/",
	"title": "UAN Ansible Roles",
	"tags": [],
	"description": "",
	"content": "UAN Ansible Roles uan_disk_config The uan_disk_config role configures swap and scratch disk partitions on UAN nodes.\nRequirements There must be disk devices found on the UAN node by the device_filter module or this role will exit with failure. This condition can be ignored by setting uan_require_disk to false. See variable definitions below.\nSee the library/device_filter.py file for more information on this module.\nThe device that is found will be unmounted if mounted and a swap partition will be created on the first half of the disk, and a scratch partition on the second half. ext4 filesystems are created on each partition.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\n uan_require_disk  Boolean to determine if this role continues to setup disk if no disks were found by the device filter. Set to true to exit with error when no disks are found.\nExample:\nuan_require_disk: false  uan_device_name_filter  Regular expression of disk device name for this role to filter. Input to the device_filter module.\nExample:\nuan_device_name_filter: \u0026#34;^sd[a-f]$\u0026#34;  uan_device_host_filter  Regular expression of host for this role to filter. Input to the device_filter module.\nExample:\nuan_device_host_filter: \u0026#34;  uan_device_model_filter  Regular expression of device model for this role to filter. Input to the device_filter module.\nExample:\nuan_device_model_filter: \u0026#34;  uan_device_vendor_filter  Regular expression of disk vendor for this role to filter. Input to the device_filter module.\nExample:\nuan_device_vendor_filter: \u0026#34;  uan_device_size_filter  Regular expression of disk size for this role to filter. Input to the device_filter module.\nExample:\nuan_device_size_filter: \u0026#34;\u0026lt;1TB\u0026#34;  uan_swap  Filesystem location to mount the swap partition.\nExample:\nuan_swap: \u0026#34;/swap\u0026#34;  uan_scratch  Filesystem location to mount the scratch partition.\nExample:\nuan_scratch: \u0026#34;/scratch\u0026#34;  swap_file  Name of the swapfile to create. Full path is \u0026lt;uan_swap\u0026gt;/\u0026lt;swapfile\u0026gt;.\nExample:\nswap_file: \u0026#34;swapfile\u0026#34;  swap_dd_command  dd command to create the swapfile.\nExample:\nswap_dd_command: \u0026#34;/usr/bin/dd if=/dev/zero of={{ uan_swap }}/{{ swap_file }} bs=1GB count=10\u0026#34;  swap_swappiness  Value to set the swapiness in sysctl.\nExample:\nswap_swappiness: \u0026#34;10\u0026#34;   Dependencies library/device_filter.py is required to find eligible disk devices.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_disk_config } This role is included in the UAN site.yml play.\nuan_interfaces The uan_interfaces role configures site/customer-defined network interfaces and Shasta Customer Access Network (CAN) network interfaces on UAN nodes.\nRequirements None.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_can_setup uan_can_setup is a boolean variable controlling the configuration of user access to UAN nodes. When true, user access is configured over either the Customer Access Network (CAN) or Customer High Speed Network (CHN), depending on which is configured on the system.\nWhen uan_can_setup is false, user access over the CAN or CHN is not configured on the UAN nodes and no default route is configured. The Admin must then specify the default route in customer_uan_routes.\nThe default value of uan_can_setup is no.\nuan_can_setup: no uan_customer_default_route uan_customer_default_route is a boolean variable that allows the default route to be set by the customer_uan_routes data when uan_can_setup is true.\nBy default, no default route is setup unless uan_can_setup is true, which sets the default route to the CAN or CHN.\nuan_customer_default_route: no  sls_nmn_name  sls_nmn_name is the Node Management Network name used by SLS. This value should not be changed.\nExample:\nsls_nmn_name: \u0026#34;NMN\u0026#34;  sls_nmn_svcs_name  is the Node Management Services Network name used by SLS. This value should not be changed.\n uan_required_dns_options  uan_required_dns_options is a list of DNS options. By default, single-request is set and must not be removed.\nExample:\nuan_required_dns_options: - \u0026#39;single-request\u0026#39;  customer_uan_interfaces  customer_uan_interfaces is as list of interface names used for constructing ifcfg-\u0026lt;customer_uan_interfaces.name\u0026gt; files. Define ifcfg fields for each interface here. Field names are converted to uppercase in the generated ifcfg-\u0026lt;name\u0026gt; file(s).\nInterfaces should be defined in order of dependency.\nExample:\ncustomer_uan_interfaces: - name: \u0026#34;net1\u0026#34; settings: bootproto: \u0026#34;static\u0026#34; device: \u0026#34;net1\u0026#34; ipaddr: \u0026#34;1.2.3.4\u0026#34; startmode: \u0026#34;auto\u0026#34; - name: \u0026#34;net2\u0026#34; settings: bootproto: \u0026#34;static\u0026#34; device: \u0026#34;net2\u0026#34; ipaddr: \u0026#34;5.6.7.8\u0026#34; startmode: \u0026#34;auto\u0026#34;  customer_uan_routes  customer_uan_routes is as list of interface routes used for constructing ifroute-\u0026lt;customer_uan_routes.name\u0026gt; files.\nExample:\ncustomer_uan_routes: - name: \u0026#34;net1\u0026#34; routes: - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; - name: \u0026#34;net2\u0026#34; routes: - \u0026#34;default 10.103.8.20 255.255.255.255 - table 3\u0026#34; - \u0026#34;10.103.8.128/25 10.103.8.20 255.255.255.255 net2\u0026#34;  customer_uan_rules  customer_uan_rules is as list of interface rules used for constructing ifrule-\u0026lt;customer_uan_routes.name\u0026gt; files.\nExample:\ncustomer_uan_rules: - name: \u0026#34;net1\u0026#34; rules: - \u0026#34;from 10.1.0.0/16 lookup 1\u0026#34; - name: \u0026#34;net2\u0026#34; rules: - \u0026#34;from 10.103.8.0/24 lookup 3\u0026#34;  customer_uan_global_routes  customer_uan_global_routes is a list of global routes used for constructing the \u0026ldquo;routes\u0026rdquo; file.\nExample:\ncustomer_uan_global_routes: - routes: - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34;  external_dns_searchlist  external_dns_searchlist is a list of customer-configurable fields to be added to the /etc/resolv.conf DNS search list.\nExample:\nexternal_dns_searchlist: - \u0026#39;my.domain.com\u0026#39; - \u0026#39;my.other.domain.com\u0026#39;  external_dns_servers  external_dns_servers is a list of customer-configurable fields to be added to the /etc/resolv.conf DNS server list.\nExample:\nexternal_dns_servers: - \u0026#39;1.2.3.4\u0026#39; - \u0026#39;5.6.7.8\u0026#39;  external_dns_options  external_dns_options is a list of customer-configurable fields to be added to the /etc/resolv.conf DNS options list.\nExample:\nexternal_dns_options: - \u0026#39;single-request\u0026#39;  uan_access_control  uan_access_control is a boolean variable to control whether non-root access control is enabled. Default is no.\nExample:\nuan_access_control: no  api_gateways  api_gateways is a list of API gateway DNS names to block non-user access\nExample:\napi_gateways: - \u0026#34;api-gw-service\u0026#34; - \u0026#34;api-gw-service.local\u0026#34; - \u0026#34;api-gw-service-nmn.local\u0026#34; - \u0026#34;kubeapi-vip\u0026#34;  api_gw_ports  api_gw_ports is a list of gateway ports to protect.\nExample:\napi_gw_ports: \u0026#34;80,443,8081,8888\u0026#34;  sls_url  sls_url is the SLS URL.\nExample:\nsls_url: \u0026#34;http://cray-sls\u0026#34;   Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_interfaces } This role is included in the UAN site.yml play.\nuan_ldap The uan_ldap role configures LDAP and AD groups on UAN nodes.\nRequirements NSCD, pam-config, sssd.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\n uan_ldap_setup  A boolean variable to selectively skip the setup of LDAP on nodes it would otherwise be configured due to uan_ldap_config being defined. The default setting is to setup LDAP when uan_ldap_config is not empty.\nExample:\nuan_ldap_setup: yes  uan_ldap_config  Configures LDAP domains and servers. If this list is empty, no LDAP configuration will be applied to the UAN targets and all role tasks will be skipped.\nExample:\nuan_ldap_config: - domain: \u0026#34;mydomain-ldaps\u0026#34; search_base: \u0026#34;dc=...,dc=...\u0026#34; servers: [\u0026#34;ldaps://123.123.123.1\u0026#34;, \u0026#34;ldaps://213.312.123.132\u0026#34;] chpass_uri: [\u0026#34;ldaps://123.123.123.1\u0026#34;] - domain: \u0026#34;mydomain-ldap\u0026#34; search_base: \u0026#34;dc=...,dc=...\u0026#34; servers: [\u0026#34;ldap://123.123.123.1\u0026#34;, \u0026#34;ldap://213.312.123.132\u0026#34;]  uan_ad_groups  Configures active directory groups on UAN nodes.\nExample:\nuan_ad_groups: - { name: admin_grp, origin: ALL } - { name: dev_users, origin: ALL }  uan_pam_modules  Configures PAM modules on the UAN nodes in /etc/pam.d.\nExample:\nuan_pam_modules: - name: \u0026#34;common-account\u0026#34; lines: - \u0026#34;account required\\tpam_access.so\u0026#34;   Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_ldap } This role is included in the UAN site.yml play.\nuan_motd The uan_motd role appends text to the /etc/motd file.\nRequirements None.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_motd_content: []  uan_motd_content  Contains text to be added to the end of the /etc/motd file.\n  Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_motd, uan_motd_content: \u0026#34;MOTD CONTENT\u0026#34; } This role is included in the UAN site.yml play.\nuan_packages The uan_packages role installs additional RPMs on UANs using the Ansible zypper module.\nPackages that are required for UANs to function should be preferentially installed during image customization and/or image creation.\nInstalling RPMs during post-boot node configuration can cause high system loads on large systems.\nThis role will only run on SLES-based nodes.\nRequirements Zypper must be installed.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_additional_sles15_packages: []  uan_additional_sles15_packages  contains the list of RPM packages to install.\n  Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_packages, uan_additional_sles15_packages: [\u0026#39;vim\u0026#39;] } This role is included in the UAN site.yml play.\nuan_shadow The uan_shadow role configures the root password on UAN nodes.\nRequirements The root password hash has to be installed in HashiCorp Vault at secret/uan root_password.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\n uan_vault_url  The URL for the HashiCorp Vault\nExample:\nuan_vault_url: \u0026#34;http://cray-vault.vault:8200\u0026#34;  uan_vault_role_file  The required Kubernetes role file for HashiCorp Vault access.\nExample:\nuan_vault_role_file: /var/run/secrets/kubernetes.io/serviceaccount/namespace  uan_vault_jwt_file  The path to the required Kubernetes token file for HashiCorp Vault access.\nExample:\nuan_vault_jwt_file: /var/run/secrets/kubernetes.io/serviceaccount/token  uan_vault_path  The path to use for storing data for UANs in HashiCorp Vault.\nExample:\nuan_vault_path: secret/uan  uan_vault_key  The key used for storing the root password in HashiCorp Vault.\nExample:\nuan_vault_key: root_password   Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_shadow } This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-254/upgrade/upgrades/",
	"title": "Upgrades",
	"tags": [],
	"description": "",
	"content": "Upgrades Performing an upgrade of UAN from one version to the next follows the same general process as a fresh install. Some considerations may need to be made when merging the existing CFS configuration with the latest CFS configuration provided by the release.\nThe overall workflow for completing a UAN upgrade involves:\n  Perform the UAN Installation\n  Review any Notable Changes\n  Merge UAN CFS Configuration Data\n  Create UAN images and reboot\n  "
},
{
	"uri": "/docs-uan/en-254/operations/about_uan_configuration/",
	"title": "About UAN Configuration",
	"tags": [],
	"description": "",
	"content": "About UAN Configuration This section describes the Ansible playbooks and roles that configure UANs.\nUAN configuration overview Configuration of UAN nodes is performed by the Configuration Framework Service (CFS). CFS can apply configuration to both images and nodes. When the configuration is applied to nodes, the nodes must be booted and accessible through SSH over the Node Management Network (NMN).\nThe Ansible roles involved in UAN configuration are listed in the site.yml file in the uan-config-management git repository in VCS. Most of the roles that are specific to image configuration are required for the operation as a UAN and must not be removed from site.yml.\nThe UAN-specific roles involved in post-boot UAN node configuration are:\n  uan_disk_config: this role configures the last disk found on the UAN that is smaller than 1TB, by default. That disk will be formatted with a scratch and swap partition mounted at /scratch and /swap, respectively. Each partition is 50% of the disk.\n  uan_packages: this role installs any RPM packages listed in the uan-config-management repo.\n  uan_interfaces: this role configures the UAN node networking. By default, this role does not configure a default route or the Customer Access Network (CAN or CHN) connection for the HPE Cray EX supercomputer. If CAN or CHN is enabled, the default route will be on the CAN or CHN. Otherwise, a default route must be set up in the customer interfaces definitions. Without the CAN or CHN, there will not be an external connection to the customer site network unless one is defined in the customer interfaces. See Configure Interfaces on UANs.\nNOTE: If a UAN layer is used in the Compute node CFS configuration, the uan_interfaces role will configure the default route on Compute nodes to be on the HSN, if the BICAN System Default Route is set to CHN.\n  uan_motd: this role Provides a default message of the day that can be customized by the administrator.\n  uan_ldap: this optional role configures the connection to LDAP servers. To disable this role, the administrator must set \u0026lsquo;uan_ldap_setup:no\u0026rsquo; in the \u0026lsquo;uan-config-management\u0026rsquo; VCS repository.\n  The UAN roles in site.yml are required and must not be removed, with exception of uan_ldap if the site is using some other method of user authentication. The uan_ldap may also be skipped by setting the value of uan_ldap_setup to no in a group_vars or host_vars configuration file.\nFor more information about these roles, see UAN Ansible Roles.\nUAN network configuration The uan_interfaces role configures the interfaces on the UAN nodes in three phases:\n Setup and configure the NMN.  Gather information from the System Layout Service (SLS) for the NMN. Populate /etc/resolv.conf. Configure the first OCP port on an HPE server, or the first LOM port on a Gigabyte server, as the nmn0 interface.   Set up the CAN or CHN, if wanted  Gather information from SLS for the CAN or CHN. Configure the route to the CAN or CHN gateway as the default one. Implement the CAN or CHN.  CAN: Implement bonded pair  On HPE servers, use the second port of the 25Gb OCP card and a second 25Gb card. On Gigabyte servers, use both ports of the 40Gb card.   CHN: Implement the CHN interface on the HSN     Setup customer-defined networks  See Configure Interfaces on UANs for detailed instructions.\nUAN LDAP network requirements LDAP configuration requires either a CAN or another customer-provided network that can route to the LDAP servers. Both such networks route outside of the HPE Cray EX system. If a UAN only has the nmn0 interface configured and active, the UAN cannot route outside of the system.\n"
},
{
	"uri": "/docs-uan/en-254/operations/boot_uans/",
	"title": "Boot UANs",
	"tags": [],
	"description": "",
	"content": "Boot UANs Perform this procedure to boot UANs using BOS so that they are ready for user logins.\nPerform Create UAN Boot Images before performing this procedure.\n  Create a BOS session to boot the UAN nodes.\nncn-m001# cray bos session create --template-uuid uan-sessiontemplate-PRODUCT_VERSION --operation reboot --format json | tee session.json { \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;jobId\u0026#34;: \u0026#34;boa-89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;session\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d/status\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;status\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; } ], \u0026#34;operation\u0026#34;: \u0026#34;reboot\u0026#34;, \u0026#34;templateUuid\u0026#34;: \u0026#34;uan-sessiontemplate-PRODUCT_VERSION\u0026#34; }   Retrieve the BOS session ID from the output of the previous command.\nncn-m001# export BOS_SESSION=$(jq -r \u0026#39;.links[] | select(.rel==\u0026#34;session\u0026#34;) | .href\u0026#39; session.json | cut -d \u0026#39;/\u0026#39; -f4) ncn-m001# echo $BOS_SESSION 89680d0a-3a6b-4569-a1a1-e275b71fce7d   Retrieve the Boot Orchestration Agent (BOA) Kubernetes job name for the BOS session.\nncn-m001# BOA_JOB_NAME=$(cray bos session describe $BOS_SESSION --format json | jq -r .job)   Retrieve the Kuberenetes pod name for the BOA assigned to run this session.\nncn-m001# BOA_POD=$(kubectl get pods -n services -l job-name=$BOA_JOB_NAME --no-headers -o custom-columns=\u0026#34;:metadata.name\u0026#34;)   View the logs for the BOA to track session progress.\nncn-m001# kubectl logs -f -n services $BOA_POD -c boa   List the CFS sessions started by the BOA. Skip this step if CFS was not enabled in the boot session template used to boot the UANs.\nIf CFS was enabled in the boot session template, the BOA will initiate a CFS session.\nIn the following command, pending and complete are also valid statuses to filter on.\nncn-m001# cray cfs sessions list --tags bos_session=$BOS_SESSION --status running --format json   "
},
{
	"uri": "/docs-uan/en-254/operations/build_a_new_uan_image_using_the_cos_recipe/",
	"title": "Build A New UAN Image Using A Cos Recipe",
	"tags": [],
	"description": "",
	"content": "Build a New UAN Image Using a COS Recipe Prior to UAN 2.3, a similar copy of the COS recipe was imported with the UAN install. In the UAN 2.3 release, UAN does not install a recipe, and a COS recipe must be used. Additional uan packages will now be installed via CFS and the uan_packages role.\nPerform the following before starting this procedure:\n Install the COS, Slingshot, and UAN product streams. Initialize the cray administrative CLI.  In the COS recipe for 2.2, several dependencies have been removed, this includes Slingshot, DVS, and Lustre. Those packages are now installed during CFS Image Customization. More information on this change is covered in the Create UAN Boot Images procedure.\n  Identify the COS image recipe to base the UAN image on. Select the recipe that matches the version of COS that the compute nodes will be using.\nncn-m001# cray ims recipes list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;compute\u0026#34;))\u0026#39; { \u0026#34;created\u0026#34;: \u0026#34;2021-02-17T15:19:48.549383+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;4a5d1178-80ad-4151-af1b-bbe1480958d1\u0026#34;, \u0026lt;\u0026lt;-- Note this ID \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;3c3b292364f7739da966c9cdae096964\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://ims/recipes/4a5d1178-80ad-4151-af1b-bbe1480958d1/recipe.tar.gz\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;linux_distribution\u0026#34;: \u0026#34;sles15\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cray-shasta-compute-sles15sp3.x86_64-2.2.27\u0026#34;, \u0026#34;recipe_type\u0026#34;: \u0026#34;kiwi-ng\u0026#34; }   Save the id of the IMS recipe in an environment variable.\nncn-m001# IMS_RECIPE_ID=4a5d1178-80ad-4151-af1b-bbe1480958d1   Use the IMS recipe id to build the UAN image:\nMore detail on this IMS procedure may be found in the procedure \u0026ldquo;Build an Image Using IMS REST Service\u0026rdquo; in the CSM documentation.\nncn-m001# IMS_PUBLIC_KEY=$(cray ims public-keys list --format json | jq -r \u0026#34;.[] | .id\u0026#34; | head -1) ncn-m001# IMS_ARCHIVE_NAME=$(cray ims recipes describe $IMS_RECIPE_ID --format json | jq -r .name) ncn-m001# IMS_ARCHIVE_NAME=${IMS_ARCHIVE_NAME/compute/uan} ncn-m001# cray ims jobs create --job-type create --public-key-id $IMS_PUBLIC_KEY --image-root-archive-name $IMS_ARCHIVE_NAME --artifact-id $IMS_RECIPE_ID   Perform Create UAN Boot Images to run CFS Image Customization on the resulting image.\n  "
},
{
	"uri": "/docs-uan/en-254/operations/configure_interfaces_on_uans/",
	"title": "Configure Interfaces On UANs",
	"tags": [],
	"description": "",
	"content": "Configure Interfaces on UANs Perform this procedure to set network interfaces on UANs by editing a configuration file.\nInterface configuration is performed by the uan_interfaces Ansible role. For details on the variables referred to in this procedure, see UAN Ansible Roles.\nIn the command examples in this procedure, PRODUCT_VERSION refers to the current installed version of the UAN product. Replace PRODUCT_VERSION with the UAN version number string when executing the commands.\nUser access may be configured to use either a direct connection to the UANs from the sites user network, or one of two optional user access networks implemented within the HPE Cray EX system. The two optional networks are the Customer Access Network (CAN) or Customer High Speed Network (CHN). The CAN is a VLAN on the Node Management Network (NMN), whereas the CHN is over the High Speed Network (HSN).\nBy default, a direct connection to the sites user network is assumed and the Admin must define the interface and default route using the customer_uan_interfaces and customer_uan_routes structures. When CAN or CHN are selected, the interfaces and default route are setup automatically.\nNetwork configuration settings are defined in the uan-config-management VCS repo under the group_vars/ROLE_SUBROLE/ or host_vars/XNAME/ directories, where ROLE_SUBROLE must be replaced by the role and subrole assigned for the node in HSM, and XNAME with the xname of the node. Values under group_vars/ROLE_SUBROLE/ apply to all nodes with the given role and subrole. Values under the host_vars/XNAME/ apply to the specific node with the xname and will override any values set in group_vars/ROLE_SUBROLE/. A yaml file is used by the Configuration Framwork Service (CFS). The examples in this procedure use customer_net.yml, but any filename may be used. Admins must create this yaml file and use the variables described in this procedure.\nIf the HPE Cray EX CAN or CHN is desired, set the uan_can_setup variable to yes in the yaml file. The UAN will be configured to use the CAN or CHN based on what the BICAN System Default Route is set to in SLS.\n  Obtain the password for the crayvcs user.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\  --template={{.data.vcs_password}} | base64 --decode   Log in to ncn-w001.\n  Create a copy of the Git configuration. Enter the credentials for the crayvcs user when prompted.\nncn-w001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git   Change to the uan-config-management directory.\nncn-w001# cd uan-config-management   Edit the yaml file, (customer_net.yml, for example), in either the group_vars/ROLE_SUBROLE/ or host_vars/XNAME directory and configure the values as needed.\nTo set up CAN or CHN:\n## uan_can_setup # Set uan_can_setup to \u0026#39;yes\u0026#39; if the site will # use the Shasta CAN or CHN network for user access. # By default, uan_can_setup is set to \u0026#39;no\u0026#39;. uan_can_setup: yes To allow a custom default route when CAN or CHN is selected:\n## uan_customer_default_route # Allow a custom default route when CAN or CHN is selected. uan_customer_default_route: no To define interfaces:\n## Customer defined networks ifcfg-X # customer_uan_interfaces is as list of interface names used for constructing # ifcfg-\u0026lt;customer_uan_interfaces.name\u0026gt; files. The setting dictionary is where # any desired ifcfg fields are defined. The field name will be converted to  # uppercase in the generated ifcfg-\u0026lt;name\u0026gt; file. # # NOTE: Interfaces should be defined in order of dependency. # ## Example ifcfg fields, not exhaustive: # bootproto: \u0026#39;\u0026#39; # device: \u0026#39;\u0026#39; # dhcp_hostname: \u0026#39;\u0026#39; # ethtool_opts: \u0026#39;\u0026#39; # gateway: \u0026#39;\u0026#39; # hwaddr: \u0026#39;\u0026#39; # ipaddr: \u0026#39;\u0026#39; # master: \u0026#39;\u0026#39; # mtu: \u0026#39;\u0026#39; # peerdns: \u0026#39;\u0026#39; # prefixlen: \u0026#39;\u0026#39; # slave: \u0026#39;\u0026#39; # srcaddr: \u0026#39;\u0026#39; # startmode: \u0026#39;\u0026#39; # userctl: \u0026#39;\u0026#39; # bonding_master: \u0026#39;\u0026#39; # bonding_module_opts: \u0026#39;\u0026#39; # bonding_slave0: \u0026#39;\u0026#39; # bonding_slave1: \u0026#39;\u0026#39; #  # customer_uan_interfaces: # - name: \u0026#34;net1\u0026#34; # settings: # bootproto: \u0026#34;static\u0026#34; # device: \u0026#34;net1\u0026#34; # ipaddr: \u0026#34;1.2.3.4\u0026#34; # startmode: \u0026#34;auto\u0026#34; # - name: \u0026#34;net2\u0026#34; # settings: # bootproto: \u0026#34;static\u0026#34; # device: \u0026#34;net2\u0026#34; # ipaddr: \u0026#34;5.6.7.8\u0026#34; # startmode: \u0026#34;auto\u0026#34; customer_uan_interfaces: [] To define interface static routes:\n## Customer defined networks ifroute-X # customer_uan_routes is as list of interface routes used for constructing # ifroute-\u0026lt;customer_uan_routes.name\u0026gt; files.  #  # customer_uan_routes: # - name: \u0026#34;net1\u0026#34; # routes: # - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; # - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; # - name: \u0026#34;net2\u0026#34; # routes: # - \u0026#34;default 10.103.8.20 255.255.255.255 - table 3\u0026#34; # - \u0026#34;10.103.8.128/25 10.103.8.20 255.255.255.255 net2\u0026#34; customer_uan_routes: [] To define the rules:\n## Customer defined networks ifrule-X # customer_uan_rules is as list of interface rules used for constructing # ifrule-\u0026lt;customer_uan_routes.name\u0026gt; files.  #  # customer_uan_rules: # - name: \u0026#34;net1\u0026#34; # rules: # - \u0026#34;from 10.1.0.0/16 lookup 1\u0026#34; # - name: \u0026#34;net2\u0026#34; # rules: # - \u0026#34;from 10.103.8.0/24 lookup 3\u0026#34; customer_uan_rules: [] To define the global static routes:\n## Customer defined networks global routes # customer_uan_global_routes is as list of global routes used for constructing # the \u0026#34;routes\u0026#34; file.  #  # customer_uan_global_routes: # - routes:  # - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; # - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; customer_uan_global_routes: []   Add the change from the working directory to the staging area.\nncn-w001# git add -A   Commit the file to the master branch.\nncn-w001# git commit -am \u0026#39;Added UAN interfaces\u0026#39;   Push the commit.\nncn-w001# git push   Obtain the commit ID for the commit pushed in the previous step.\nncn-m001# git rev-parse --verify HEAD   Update any CFS configurations used by the UANs with the commit ID from the previous step.\na. Download the JSON of the current UAN CFS configuration to a file.\nThis file will be named uan-config-PRODUCT_VERSION.json. Replace PRODUCT_VERSION with the current installed UAN version.\nncn-m001# cray cfs configurations describe uan-config-PRODUCT_VERSION \\  --format=json \u0026gt;uan-config-PRODUCT_VERSION.json b. Remove the unneeded lines from the JSON file.\nThe lines to remove are: - the `lastUpdated` line - the last `name` line These must be removed before uploading the modified JSON file back into CFS to update the UAN configuration. ```bash ncn-m001# cat uan-config-PRODUCT_VERSION.json { \u0026quot;lastUpdated\u0026quot;: \u0026quot;2021-03-27T02:32:10Z\u0026quot;, \u0026quot;layers\u0026quot;: [ { \u0026quot;cloneUrl\u0026quot;: \u0026quot;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026quot;, \u0026quot;commit\u0026quot;: \u0026quot;aa5ce7d5975950ec02493d59efb89f6fc69d67f1\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;uan-integration-PRODUCT_VERSION\u0026quot;, \u0026quot;playbook\u0026quot;: \u0026quot;site.yml\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;uan-config-2.0.1-full\u0026quot; } ```  c. Replace the commit value in the JSON file with the commit ID obtained in the previous Step.\nThe name value after the commit line may also be updated to match the new UAN product version, if desired. This is not necessary as CFS does not use this value for the configuration name. ```bash { \u0026quot;layers\u0026quot;: [ { \u0026quot;cloneUrl\u0026quot;: \u0026quot;https://api-gw-service-nmn.local/vcs/cray/uan-configmanagement.git\u0026quot;, \u0026quot;commit\u0026quot;: \u0026quot;aa5ce7d5975950ec02493d59efb89f6fc69d67f1\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;uan-integration-PRODUCT_VERSION\u0026quot;, \u0026quot;playbook\u0026quot;: \u0026quot;site.yml\u0026quot; } ] } ```  d. Create a new UAN CFS configuration with the updated JSON file.\nThe following example uses uan-config-PRODUCT_VERSION for the name of the new CFS configuration, to match the JSON file name.\n```bash ncn-m001# cray cfs configurations update uan-config-PRODUCT_VERSION \\ --file uan-config-PRODUCT_VERSION.json ```  e. Tell CFS to apply the new configuration to UANs by repeating the following command for each UAN. Replace UAN_XNAME in the command below with the name of a different UAN each time the command is run.\n```bash ncn-m001# cray cfs components update --desired-config uan-config-PRODUCT_VERSION \\ --enabled true --format json UAN_XNAME ```    Reboot the UAN with the Boot Orchestration Service (BOS).\nThe new interfaces will be available when the UAN is rebooted. Replace the UAN_SESSION_TEMPLATE value with the BOS session template name for the UANs.\nncn-w001# cray bos v1 session create \\  --template-uuid UAN_SESSION_TEMPLATE --operation reboot   Verify that the desired networking configuration is correct on each UAN.\n  "
},
{
	"uri": "/docs-uan/en-254/operations/configure_pluggable_authentication_modules_pam_on_uans/",
	"title": "Configure Pluggable Authentication Modules (pam) On UANs",
	"tags": [],
	"description": "",
	"content": "Configure Pluggable Authentication Modules (PAM) on UANs Perform this procedure to configure PAM on UANs. This enables dynamic authentication support for system services.\nIntialize and configure the Cray command line interface (CLI) tool on the system. See \u0026ldquo;Configure the Cray Command Line Interface (CLI)\u0026rdquo; in the CSM documentation for more information.\n  Verify that the Gitea Version Control Service (VCS) is running.\nncn-m001# kubectl get pods --all-namespaces | grep vcs services gitea-vcs-f57c54c4f-j8k4t 2/2 Running 1 11d services gitea-vcs-postgres-0 2/2 Running 0 11d   Retrieve the initial Gitea login credentials for the crayvcs username.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\  --template={{.data.vcs_password}} | base64 --decode These credentials can be modified in the vcs_user role prior to installation or can be modified after logging in.\n  Use an external web browser to verify the Ansible plays are available on the system.\nThe URL will take on the following format:\nhttps://api.SYSTEM-NAME.DOMAIN-NAME/vcs\n  Clone the system Version Control Service (VCS) repository to a directory on the system.\nncn-w001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git   Change to the uan-config-management directory.\nncn-w001# cd uan-config-management   Make a new directory for the PAM configuration.\na. Create a group_vars/all directory if making changes to all UANs.\n ```bash ncn-w001# mkdir -p group_vars/all ncn-w001# cd group_vars/all ```  b. Create a host_vars/XNAME directory if the change is node specific.\n ```bash ncn-w001# mkdir -p host_vars/XNAME ncn-w001# cd host_vars/XNAME ```    Configure PAM.\nThe default path is /etc/pam.d/, so only the module file name is required.\n# vi pam.yml --- uan_pam_modules: - name: pam_module_file_name lines: - \u0026#34;add this line to pam module file_name\u0026#34; - \u0026#34;add another line to pam module file_name\u0026#34; - name: another_pam_module_file_name lines: - \u0026#34;add this line to another_pam_module_file_name\u0026#34; The following is an example of adding the line \u0026quot;account required pam\\_access.so\u0026quot; to the /etc/pam.d/common-account PAM file. The \\t is used to place a tab between account required and pam\\_access.so to match the formatting of the common-account file contents. The quotes are required in the strings used in the lines filed.\n--- uan_pam_modules: - name: common-account lines: - \u0026#34;account required\\tpam_access.so\u0026#34;   Add the change from the working directory to the staging area.\n  All UANs:\nncn-w001# git add group_vars/all/pam.yml   Node specific:\nncn-w001# git add host_vars/XNAME/pam.yml     Commit the file to the master branch.\nncn-w001# git commit -am \u0026#39;Added PAM configuration\u0026#39;   Push the commit.\nncn-w001# git push If prompted, use the Gitea login credentials.\n  Reboot the UAN(s) with the Boot Orchestration Service (BOS).\nncn-w001# cray bos session create \\  --template-uuid UAN_SESSION_TEMPLATE --operation reboot   "
},
{
	"uri": "/docs-uan/en-254/installation_prereqs/configure_the_bios_of_an_hpe_uan/",
	"title": "Configure The Bios Of An HPE UAN",
	"tags": [],
	"description": "",
	"content": "Configure the BIOS of an HPE UAN Perform this procedure to configure the network interface and boot settings required by HPE UANs.\nBefore the UAN product can be installed on HPE UANs, specific network interface and boot settings must be configured in the BIOS.\nPerform Configure the BMC for UANs with iLO before performing this procedure.\n  Force a UAN to reboot into the BIOS.\nIn the following command, UAN_BMC_XNAME is the xname of the BMC of the UAN to configure. Replace USER and PASSWORD with the BMC username and password, respectively.\nncn-m001# ipmitool -U USER -P PASSWORD -H UAN_BMC_XNAME -I lanplus \\ chassis bootdev pxe options=efiboot,persistent   Monitor the console of the UAN using either ConMan or the following command:\nncn-m001# ipmitool -U USER -P PASSWORD -H UAN_BMC_XNAME -I \\ lanplus sol activate Refer to the section \u0026ldquo;About the ConMan Containerized Service\u0026rdquo; in the CSM documentation for more information about ConMan.\n  Press the ESC and 9 keys to access the BIOS System Utilities when the option appears.\n  Ensure that OCP Slot 10 Port 1 is the only port with Boot Mode set to Network Boot. All other ports must have Boot Mode set to Disabled.\nThe settings must match the following example.\n-------------------- System Configuration BIOS Platform Configuration (RBSU) \u0026gt; Network Options \u0026gt; Network Boot Options \u0026gt; PCIe Slot Network Boot Slot 1 Port 1 : Marvell FastLinQ 41000 Series - [Disabled] 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC Slot 1 Port 2 : Marvell FastLinQ 41000 Series - [Disabled] 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC Slot 2 Port 1 : Network Controller [Disabled] OCP Slot 10 Port 1 : Marvell FastLinQ 41000 [Network Boot] Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC OCP Slot 10 Port 2 : Marvell FastLinQ 41000 [Disabled] Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC --------------------   Set the Link Speed to SmartAN for all ports.\n-------------------- System Utilities System Configuration \u0026gt; Main Configuration Page \u0026gt; Port Level Configuration Link Speed [SmartAN] FEC Mode [None] Boot Mode [PXE] DCBX Protocol [Dynamic] RoCE Priority [0] PXE VLAN Mode [Disabled] Link Up Delay [30] Wake On LAN Mode [Enabled] RDMA Protocol Support [iWARP + RoCE] BAR-2 Size [8M] VF BAR-2 Size [256K] ---------------------   Set the boot options to match the following example.\n---------------------- System Utilities System Configuration \u0026gt; BIOS/Platform Configuration (RBSU) \u0026gt; Boot Options Boot Mode [UEFI Mode] UEFI Optimized Boot [Enabled] Boot Order Policy [Retry Boot Order Indefinitely] UEFI Boot Settings Legacy BIOS Boot Order -----------------------   Set the UEFI Boot Order settings to match the following example.\nThe order must be:\n USB Local disks OCP Slot 10 Port 1 IPv4 OCP Slot 10 Port 1 IPv6  ----------------------- System Utilities System Configuration \u0026gt; BIOS/Platform Configuration (RBSU) \u0026gt; Boot Options \u0026gt; UEFI Boot Settings \u0026gt; UEFI Boot Order Press the \u0026#39;+\u0026#39; key to move an entry higher in the boot list and the \u0026#39;-\u0026#39; key to move an entry lower in the boot list. Use the arrow keys to navigate through the Boot Order list. Generic USB Boot SATA Drive Box 1 Bay 1 : VK000480GWTHA SATA Drive Box 1 Bay 2 : VK000480GWTHA SATA Drive Box 1 Bay 3 : VK001920GWTTC SATA Drive Box 1 Bay 4 : VK001920GWTTC OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv4) OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv6) -------------------------   Refer to this Setting the Date and Time in the HPE UEFI documentation to set the correct date and time.\nIf the time is not set correctly, then PXE network booting issues may occur.\n  "
},
{
	"uri": "/docs-uan/en-254/installation_prereqs/configure_the_bmc_for_uans_with_ilo/",
	"title": "Configure The BMC For UANs With Ilo",
	"tags": [],
	"description": "",
	"content": "Configure the BMC for UANs with iLO Perform this procedure to enable the IPMI/DCMI settings on an HPE UAN that are necessary to continue UAN product installation on an HPE Cray EX supercomputer.\nPerform the first three steps of Prepare for UAN Product Installation before performing this procedure.\n  Create the SSH tunnel necessary to access the BMC web GUI interface.\n  Find the IP or hostname for a UAN.\n  Create an SSH tunnel to the UAN BMC. Run the following command on an external system.\nIn the following example, UAN_MGMT is the UAN iLO interface host name or IP address. NCN is the host name or IP address of a non-compute node on the system. This example assumes that NCN allows port forwarding. USER will usually be root.\n$ ssh -L 8443:UAN_MGMT:443 USER@NCN   Wait for SSH to establish the connection.\n    Open https://127.0.0.1:8443 in web browser on the NCN to access the BMC web GUI.\n  Log in to the web GUI using default credentials.\n  Click Security in the menu on the left side of the screen.\n  Click Access Settings in the menu at the top of the screen.\n  Click the pencil icon next to Network in the main window area.\n  Check the box next to IPMI/DCMI over LAN.\n  Ensure that the remote management settings match the following screenshot.\n  "
},
{
	"uri": "/docs-uan/en-254/installation_prereqs/hardware_and_software_prerequisites/",
	"title": "Hardware And Software Prerequisites",
	"tags": [],
	"description": "",
	"content": "Hardware and Software Prerequisites "
},
{
	"uri": "/docs-uan/en-254/installation_prereqs/prepare_for_uan_product_installation/",
	"title": "Prepare For UAN Product Installation",
	"tags": [],
	"description": "",
	"content": "Prepare for UAN Product Installation Perform this procedure to ready the HPE Cray EX supercomputer for UAN product installation.\nInstall and configure the COS product before performing this procedure.\n  Verify that the management network switches are properly configured.\nRefer to the switch configuration procedures in the HPE Cray System Management Documentation.\n  Ensure that the management network switches have the proper firmware.\nRefer to the procedure \u0026ldquo;Update the Management Network Firmware\u0026rdquo; in the HPE Cray EX hardware documentation.\n  Ensure that the host reservations for the UAN CAN/CHN network have been properly set.\nRefer to the procedure \u0026ldquo;Add UAN CAN IP Addresses to SLS\u0026rdquo; in the HPE Cray EX hardware documentation.\n  Configure the BMC of the UAN.\nPerform Configure the BMC for UANs with iLO if the UAN is a HPE server with an iLO.\n  Configure the BIOS of the UAN.\n Perform Configure the BIOS of an HPE UAN if the UAN is a HPE server with an iLO. Perform Configure the BIOS of a Gigabyte UAN if the UAN is a Gigabyte server.    Verify that the firmware for each UAN BMC meets the specifications.\nUse the System Admin Toolkit firmware command to check the current firmware version on a UAN node.\nncn-m001# sat firmware -x BMC_XNAME   Repeat the previous six Steps for all UANs.\n  Unpackage the file.\nncn-m001# tar zxf uan-PRODUCT_VERSION.tar.gz   Navigate into the uan-PRODUCT_VERSION/ directory.\nncn-m001# cd uan-PRODUCT_VERSION/   Run the pre-install goss tests to determine if the system is ready for the UAN product installation.\nThis requires that goss is installed on the node running the tests.\nncn# ./validate-pre-install.sh ............... Total Duration: 1.304s Count: 15, Failed: 0, Skipped: 0   Ensure that the cray-console-node pods are connected to UANs so that they are monitored and their consoles are logged.\n  Obtain a list of the xnames for all UANs (remove the --subrole argument to list all Application nodes).\nncn# cray hsm state components list --role Application --subrole UAN --format json | jq -r .Components[].ID | sort x3000c0s19b0n0 x3000c0s24b0n0 x3000c0s31b0n0   Obtain a list of the console pods.\nncn# PODS=$(kubectl get pods -n services -l app.kubernetes.io/name=cray-console-node --template \u0026#39;{{range .items}}{{.metadata.name}} {{end}}\u0026#39;)   Use conman -q to scan the list of connections being monitored by conman (only UAN xnames are shown for brevity).\nncn# for pod in $PODS; do kubectl exec -n services -c cray-console-node $pod -- conman -q; done x3000c0s19b0n0 x3000c0s24b0n0 x3000c0s31b0n0 If a console connection is not present, the install may continue, but a console connection should be established before attempting to boot the UAN.\n    Next, install the UAN product by performing the procedure Install the UAN Product Stream.\n"
},
{
	"uri": "/docs-uan/en-254/advanced/sles_image/",
	"title": "Booting An Application Node With A Sles Image (techNICal Preview)",
	"tags": [],
	"description": "",
	"content": "Booting an Application Node with a SLES Image (Technical Preview) A SLES image is available for use with Application type nodes. This image is currently considered a \u0026ldquo;Technical Preview\u0026rdquo; as the initial support for booting with SLES Images without COS. This guide documents the procedure to boot and configure the new image as it currently differs from the standard COS-based image process in some ways.\nThe image is built with the same packer/qemu pipeline as Non-Compute-Node Images. Similarties may be noticed including the kernel and package versions.\nLimitations As this is currently a \u0026ldquo;Technical Preview\u0026rdquo; of supporting SLES Images on Application Nodes, there are several limitations:\n S3 presigned URLs with an expiration limit for the rootfs must be created. BSS parameters must be set with cray bss bootparameters replace ... BOS Sessions and Templates are not supported. CFS Configurations that operate on COS and NCN images are not yet supported. CFS Node Personalization must be started manually.  Overview The following steps outline the process of configuring and booting an Application Node with the SLES Image.\n  Determine the image to use.\n  Configure the image with IMS/CFS (optional).\n  Update BSS with the necessary parameters.\n  Reboot the node.\n  Run CFS Node Personalization (optional).\n  Procedure Perform the following steps to configure and boot a SLES image on an Application type node.\n  Log in to the master node ncn-m001. All commands in this procedure are run from the master node.\n  Verify the UAN release contains a SLES image.\nncn-m001# UAN_RELEASE=2.5 ncn-m001# sat showrev --filter \u0026#39;product_name = uan\u0026#39; | grep $UAN_RELEASE   Select an Image to boot or customize.\nncn-m001# APP_IMAGE_NAME=cray-application-sles15sp3.x86_64-0.1.0 ncn-m001# APP_IMAGE_ID=$(cray ims images list --format json | jq --arg APP_IMAGE_NAME \u0026#34;$APP_IMAGE_NAME\u0026#34; -r \u0026#39;sort_by(.created) | .[] | select(.name == $APP_IMAGE_NAME ) | .id\u0026#39; | head -1) ncn-m001# cray ims images describe $APP_IMAGE_ID --format json { \u0026#34;created\u0026#34;: \u0026#34;2022-08-24T20:07:27.263737+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;13964414-bbad-40e9-9e31-a3683010febb\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/13964414-bbad-40e9-9e31-a3683010febb/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;cray-application-sles15sp3.x86_64-0.1.0\u0026#34; }   Customize the image using SAT Bootprep. This will add a root password to the image as one is not included. If CFS is not going to be used on this node, this step is optional. Support for additional product layers will be added in subsequent releases.\nncn-m001# cat bootprep-sles-uan.yml configurations: - name: sles-uan-configuration layers: - name: uan playbook: site.yml product: name: uan version: 2.5.3 branch: integration images: - name: sles-uan-image ims: is_recipe: false name: cray-application-sles15sp3.x86_64-0.1.0 configuration: sles-uan-configuration configuration_group_names: - Application - Application_UAN ncn-m001# sat bootprep run ./bootprep-sles-uan.yml ncn-m001# APP_IMAGE_NAME=sles-uan-image ncn-m001# APP_IMAGE_ID=$(cray ims images list --format json | jq --arg APP_IMAGE_NAME \u0026#34;$APP_IMAGE_NAME\u0026#34; -r \u0026#39;sort_by(.created) | .[] | select(.name == $APP_IMAGE_NAME ) | .id\u0026#39; | head -1) ncn-m001# cray ims images describe $APP_IMAGE_ID --format json { \u0026#34;created\u0026#34;: \u0026#34;2022-08-25T20:07:27.263737+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;13964414-bbad-40e9-9e31-a36830101234\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/13964414-bbad-40e9-9e31-a36830101234/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;sles-uan-image\u0026#34; }   Create a presigned URL for the rootfs. This is needed for the node to boot in this release, in the future, this will be integrated into BSS and will not need to be performed. This URL will be valid for 1 hour and will need to be recreated if the node reboots after the URL expires. To set a longer expiration, adjust the \u0026ldquo;aws s3 presign\u0026rdquo; command accordingly.\nncn-m001# export AWS_ACCESS_KEY_ID=`kubectl get secrets -o yaml ims-s3-credentials -ojsonpath='{.data.access_key}' | base64 -d` ncn-m001# export AWS_SECRET_ACCESS_KEY=`kubectl get secrets -o yaml ims-s3-credentials -ojsonpath='{.data.secret_key}' | base64 -d` ncn-m001# alias aws=\u0026quot;aws --endpoint-url http://rgw-vip\u0026quot; ncn-m001# ROOTFS_URL=$(aws s3 presign --expires-in 3600 s3://boot-images/$APP_IMAGE_ID/rootfs)   Select an Application UAN to boot with the image.\nncn-m001# cray hsm state components list --role Application --subrole UAN --format json | jq -r \u0026#39;.Components | .[] | .ID\u0026#39; x3000c0s13b0n0 x3000c0s15b0n0 ncn-m001# NODE=x3000c0s13b0n0   Select a MAC address to use as the NMN interface.\nncn-m001:~ # cray hsm inventory ethernetInterfaces list --component-id $NODE --format json | jq -r \u0026#39;.[] | \u0026#34;\\(.Description) \\t \\(.MACAddress)\u0026#34;\u0026#39; Ethernet Interface Lan1 b4:2e:99:fd:45:c8 Ethernet Interface Lan2 b4:2e:99:fd:45:c9 ncn-m001:~ # MAC=b4:2e:99:fd:45:c8   Update BSS with the kernel, initrd, and desired parameters.\nncn-m001:~ # PARAMS=\u0026#34;ifname=nmn0:$MAC ip=nmn0:dhcp spire_join_token=\\${SPIRE_JOIN_TOKEN} biosdevname=1 pcie_ports=native transparent_hugepage=never console=tty0 console=ttyS0,115200 iommu=pt metal.no-wipe=1 initrd=initrd root=live:$ROOTFS_URL rd.live.ram=0 rd.writable.fsimg=0 rd.skipfsck rd.live.squashimg=filesystem.squashfs rd.live.overlay.thin=1 rd.live.overlay.overlayfs=1 rd.luks=0 rd.luks.crypttab=0 rd.lvm.conf=0 rd.lvm=1 rd.auto=1 rd.md=1 rd.dm=0 rd.neednet=0 rd.peerdns=1 rd.md.waitclean=1 rd.multipath=0 rd.md.conf=1 rd.bootif=0 hostname=$NODE rd.net.dhcp.retry=3 append nosplash quiet log_buf_len=1 rd.retry=10 rd.shell\u0026#34; ncn-m001:~ # cray bss bootparameters replace --hosts $NODE --initrd \u0026#34;s3://boot-images/$APP_IMAGE_ID/initrd\u0026#34; --kernel \u0026#34;s3://boot-images/$APP_IMAGE_ID/kernel\u0026#34; --params \u0026#34;$PARAMS\u0026#34;   Reboot the node. Wait for the status to return off before issuing the power on command.\nncn-m001:# USERNAME=root ncn-m001:# read -r -s -p \u0026#34;$NODEBMC ${USERNAME}password: \u0026#34; IPMI_PASSWORD; echo ncn-m001:# export IPMI_PASSWORD ncn-m001:# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus -H ${NODE::-2} power off ncn-m001:# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus -H ${NODE::-2} power status ncn-m001:# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus -H ${NODE::-2} power on   Connect to the console for the node and verify it boots into multi-user mode. Find the correct pod by using conman -q to list the available connections in each pod.\nncn-m001# kubectl exec -it -n services cray-console-node-0 -- conman -j $NODE ... 2022-08-25 14:27:51 Welcome to SUSE Linux Enterprise High Performance Computing 15 SP3 (x86_64) - Kernel 5.3.18-150300.59.43-default (ttyS0). 2022-08-25 14:27:51 2022-08-25 14:27:51 x3000c0s13b0n0 login:   If the node does not complete the boot successfully, proceed to the troubleshooting section in this guide.\n  Troubleshooting Some general troublshooting tips may help in getting started using the SLES image.\nDracut failures during booting   Could not find the kernel or the initrd. Verify the BSS bootparameters for the node. Specifically, check that the IMS Image ID is correct.\nhttp://rgw-vip.nmn/boot-images/13964414-bbad-40e9-9e31-a3683010febbasdf/kernel...HTTP 0x7f0fa808 status 404 Not Found No such file or directory (http://ipxe.org/2d0c618e) http://rgw-vip.nmn/boot-images/13964414-bbad-40e9-9e31-a3683010febbasdf/initrd...HTTP 0x7f0fa808 status 404 Not Found No such file or directory (http://ipxe.org/2d0c618e)   The presigned URL was generated incorrectly.\n2022-08-22 18:49:33 [ 9.170981] dracut-initqueue[1427]: curl: (22) The requested URL returned error: 404 Not Found 2022-08-22 18:49:33 [ 9.191138] dracut-initqueue[1421]: Warning: Downloading \u0026#39;http://rgw-vip/boot-images/c0d2d5fd-8354-4f21-a0ef-8ee2878cbde7/filesystem.squashfs?AWSAccessKeyId=I 43RBLH07R65TRO3AL02\u0026amp;Signature=YLmTttUa2KT7qzKLemOd1zIsWlo%3D\u0026amp;Expires=1661273999\u0026#39; failed! 2022-08-22 18:49:33 [ 9.222966] dracut-initqueue[1411]: Warning: failed to download live image: error 0   No carrier detected on interface nmn0. Select a different MAC address to be assigned as nmn0.\nhttp://rgw-vip.nmn/boot-images/13964414-bbad-40e9-9e31-a3683010febb/kernel... ok http://rgw-vip.nmn/boot-images/13964414-bbad-40e9-9e31-a3683010febb/initrd... ok [ 4.966173] dracut-initqueue[975]: Warning: Unable to retrieve metadata from server [ 5.379611] dracut-initqueue[1130]: Warning: Unable to retrieve metadata from server [ 10.547290] dracut-initqueue[1144]: Warning: No carrier detected on interface nmn0 [ 10.564694] dracut-initqueue[1393]: ls: cannot access \u0026#39;/tmp/leaseinfo.nmn0*\u0026#39;: No such file or directory ... [ 28.381198] dracut-initqueue[945]: Warning: dracut-initqueue timeout - starting timeout scripts [ 28.400096] dracut-initqueue[945]: Warning: Could not boot.   The root filesystem doesn\u0026rsquo;t won\u0026rsquo;t download because the URL is too long. Regenerate the URL using the aws command.\n2022-08-03 19:41:52 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0Warning: Failed to create the file 2022-08-03 19:41:52 [ 9.842822] dracut-initqueue[1428]: Warning: rootfs?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026amp;X-Amz-Credential=I43RBLH07R65T 2022-08-03 19:41:52 [ 9.862811] dracut-initqueue[1428]: Warning: RO3AL02%2F20220803%2F%2Fs3%2Faws4_request\u0026amp;X-Amz-Date=20220803T193518Z\u0026amp; 2022-08-03 19:41:52 [ 9.882714] dracut-initqueue[1428]: Warning: X-Amz-Expires=86400\u0026amp;X-Amz-SignedHeaders=host\u0026amp;X-Amz-Signature=9412c9eb0 2022-08-03 19:41:52 [ 9.902718] dracut-initqueue[1428]: Warning: 585604b3c8154376113c043fb41e3954cddc92a8d799e5176f8c140: File name 2022-08-03 19:41:52 [ 9.922710] dracut-initqueue[1428]: Warning: too long 2022-08-03 19:41:52 [ 9.938714] dracut-initqueue[1428]: 2022-08-03 19:41:52 0 2020M 0 13977 0 0 524k 0 1:05:40 --:--:-- 1:05:40 524k 2022-08-03 19:41:52 [ 9.958983] dracut-initqueue[1428]: curl: (23) Failed writing body (0 != 13977) 2022-08-03 19:41:52 [ 9.975166] dracut-initqueue[1422]: Warning: Downloading \u0026#39;http://rgw-vip/boot-images/66c37928-6887-463e-8d9f-e4eec8089374/rootfs?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026amp;X-Amz-Credential=I43RBLH07R65TRO3AL02%2F20220803%2F%2Fs3%2Faws4_request\u0026amp;X-Amz-Date=20220803T193518Z\u0026amp;X-Amz-Expires=86400\u0026amp;X-Amz-SignedHeaders=host\u0026amp;X-Amz-Signature=9412c9eb0585604b3c8154376113c043fb41e3954cddc92a8d799e5176f8c140\u0026#39; failed! 2022-08-03 19:41:52 [ 10.019096] dracut-initqueue[1412]: Warning: failed to download live image: error 0   The presigned URL has expired or was generated incorrectly.\n[ 9.787431] dracut-initqueue[1435]: curl: (22) The requested URL returned error: 403 Forbidden [ 9.807724] dracut-initqueue[1429]: Warning: Downloading \u0026#39;http://rgw-vip/boot-images/13964414-bbad-40e9-9e31-a3683010febb/rootfs?AWSAccessKeyId=I43RBLH07R65TRO3AL02\u0026amp;Signature=7%2FgOCotleoyLPGmeyG%2FFX8tpkWg%3D\u0026amp;Expires=1661523713\u0026#39; failed!   The dracut module livenet is missing from the initrd. Make sure the initrd was regenerated with /srv/cray/scripts/common/create-ims-initrd.sh if CFS was used.\n2022-08-24 14:48:53 [ 5.784023] dracut: FATAL: Don\u0026#39;t know how to handle \u0026#39;root=live:http://rgw-vip/boot-images/e88ed416-5d58-4421-9013-fa2171ac11b8/rootfs?AWSAccessKeyId=I43RBLH07R65TRO3AL02\u0026amp;Signature=bL661kZHPyEgBsLLEuJHFz3zKVs%3D\u0026amp;Expires=1661438587\u0026#39; 2022-08-24 14:48:53 [ 5.805063] dracut: Refusing to continue   Unable to log in to the node.   The node is not up. Connect to the console and determine why the node has not booted, starting with the troubleshooting tips.\nncn-m001:# ssh app01 ssh: connect to host uan01 port 22: No route to host   Unable to log in to the node with a password. No root password is defined in the image by default, one must be added via CFS or by modifying the squashfs filesystem.\nncn-m001:# ssh app01 Password: Password: Password: root@app01\u0026#39;s password: Permission denied, please try again   DHCP hostname is not set   If the node does not have a hostname assigned from DHCP, try verifying the DHCP settings and restarting wicked.\nx3000c0s13b0n0:~ # grep -R ^DHCLIENT_SET_HOSTNAME= /etc/sysconfig/network/dhcp DHCLIENT_SET_HOSTNAME=\u0026#34;yes\u0026#34; x3000c0s13b0n0:# systemctl restart wicked x3000c0s13b0n0:# hostnamectl Static hostname: x3000c0s13b0n0 Transient hostname: app01 Icon name: computer-server Chassis: server Machine ID: 9bd0aacf29d04dd4827bc464121b130b Boot ID: af753b4e6fa9419bb14d55a029d0f526 Operating System: SUSE Linux Enterprise High Performance Computing 15 SP3 CPE OS Name: cpe:/o:suse:sle_hpc:15:sp3 Kernel: Linux 5.3.18-150300.59.43-default Architecture: x86-64 x3000c0s13b0n0:# hostname app01   Spire is not running   Check the spire-agent logs for error messages.\napp01# systemctl status spire-agent   "
},
{
	"uri": "/docs-uan/en-254/installation_prereqs/configure_the_bios_of_a_gigabyte_uan/",
	"title": "Configure The Bios Of A Gigabyte UAN",
	"tags": [],
	"description": "",
	"content": "Configure the BIOS of a Gigabyte UAN Perform this procedure to configure the network interface and boot settings required by Gigabyte UANs.\nBefore the UAN product can be installed on Gigabyte UANs, specific network interface and boot settings must be configured in the BIOS.\n  Press the Delete key to enter the setup utility when prompted to do so in the console.\n  Navigate to the boot menu.\n  Set the Boot Option #1 field to Network:UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection.\n  Set all other Boot Option fields to Disabled.\n  Ensure that the boot mode is set to [UEFI].\n  Confirm that the time is set correctly. If the time is not accurate, correct it now.\nIncorrect time will cause PXE booting issues.\n  Select Save \u0026amp; Exit to save the settings.\n  Select Yes to confirm and press the Enter key.\nThe UAN will reboot.\n  Optional: Run the following IPMI commands if the BIOS settings do not persist.\nIn these example commands, the BMC of the UAN is x3000c0s27b0. Replace USERNAME and PASSWORD with username and password of the BMC of the UAN. These commands do the following:\n Power off the node Perform a reset. Set the PXE boot in the options. Power on the node  ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 power off ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 mc reset cold ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 chassis bootdev pxe \\ options=efiboot,persistent ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 power on   "
},
{
	"uri": "/docs-uan/en-254/",
	"title": "Cray Ex User Access Nodes Installation And Administration Guide",
	"tags": [],
	"description": "",
	"content": "Cray EX User Access Nodes Installation and Administration Guide This document describes the installation prequisites, installation procedures, and operational procedures for Cray EX User Access Nodes (UAN).\nTable of Contents   Installation Prerequisites\nTopics:\n Management Network Switch Configuration BMC Configuration BIOS configuration UAN BMC Firmware Software Prerequisites    Upgrades\nTopics:\n Upgrade Workflow Notable Changes From Previous Versions Merging UAN Configuration Data    UAN Software Installation\nTopics:\n Download and Prepare the UAN Software Package Run the Installation Script Installation Verification    Operational Tasks\nTopics:\n Build a New UAN Image Using the COS Recipe UAN Image Pre-boot Configuration Customizing UAN images Preparing UAN Boot Session Templates Booting UAN Nodes    Advanced Topics\nTopics:\n Customizing UANs Manually    Troubleshooting\n Debugging UAN Booting Issues Debugging UAN Configuration Issues    "
},
{
	"uri": "/docs-uan/en-254/advanced/customizing_uan_images_manually/",
	"title": "Customizing UAN Images Manually",
	"tags": [],
	"description": "",
	"content": "Customizing UAN Images Manually   Query IMS for the UAN Image you want to customize.\nncn-m001:~/ $ cray ims images list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;uan\u0026#34;))\u0026#39; { \u0026#34;created\u0026#34;: \u0026#34;2021-02-18T17:17:44.168655+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;6d46d601-c41f-444d-8b49-c9a2a55d3c21\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;371b62c9f0263e4c8c70c8602ccd5158\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/6d46d601-c41f-444d-8b49-c9a2a55d3c21/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;uan-PRODUCT_VERSION-image\u0026#34; }   Create a variable for the IMS image id in the returned data.\nncn-m001:~/ $ export IMS_IMAGE_ID=6d46d601-c41f-444d-8b49-c9a2a55d3c21   Using the IMS_IMAGE_ID, follow the instructions in the Customize an Image Root Using IMS in the CSM documentation to build the UAN Image.\n  "
},
{
	"uri": "/docs-uan/en-254/advanced/enabling_can_chn/",
	"title": "Enabling The Customer Access Network (CAN) Or The Customer High Speed Network (chn)",
	"tags": [],
	"description": "",
	"content": "Enabling the Customer Access Network (CAN) or the Customer High Speed Network (CHN) The UAN product provides a role, uan_interfaces in the Configuration Framework Service (CFS). This role is suitable for Application type nodes, and in some circumstancs, configuring the CHN, on Compute type nodes.\n  Enable CAN or CHN by setting the following in the UAN CFS repo (the filename may be modified to whatever is appropriate):\nncn-m001:~/ $ cat group_vars/Application_UAN/can.yml uan_can_setup: true   SLS will be configured with either CAN or CHN, uan_interfaces will use this setting to determine if a default route should be established over the nmn (CAN) or the hsn (CHN). To see how the site is configured, query SLS:\nncn-m001:~/ $ cray sls search networks list --name CHN --format json | jq -r \u0026#39;.[] | .Name\u0026#39; CHN   (Optional) If the compute nodes are going to use the UAN CFS role uan_interfaces to set a default route on the CHN, make sure there is an appopriate ansible setting for the compute nodes in addition to the UANs:\nncn-m001:~/ $ cat group_vars/Compute/can.yml uan_can_setup: true   Once uan_can_setup has been enabled, update the CFS configuration used for the nodes to initate a reconfiguration (see the Configuration Management section of the CSM documentation for more information)\n  The CSM documenation provides additional resources to validate the configuration of CAN and CHN for UANs and Computes. Consult the section titled \u0026ldquo;Enabling Customer High Speed Network Routing\u0026rdquo; in the CSM documentation for more information.\n  "
},
{
	"uri": "/docs-uan/en-254/install/install_the_uan_product_stream/",
	"title": "Install The UAN Product Stream",
	"tags": [],
	"description": "",
	"content": "Install the UAN Product Stream This procedure installs the User Access Nodes (UAN) product on a system so that UAN boot images can be created.\nBefore performing this procedure:\n Initialize and configure the Cray command line interface (CLI) tool. See \u0026ldquo;Configure the Cray Command Line Interface (CLI)\u0026rdquo; in the CSM documentation for more information. Perform Prepare for UAN Product Installation  Replace PRODUCT_VERSION in the example commands with the UAN product stream string (2.3.0 for example). Replace CRAY_EX_DOMAIN in the example commands with the FQDN of the HPE Cray EX.\n  Start a typescript to capture the commands and output from this installation.\nncn-m001# script -af product-uan.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39;   Run the installation script:\nncn-m001# ./install.sh   Verify that the UAN configuration was imported and added to the cray-product-catalog ConfigMap in the Kubernetes services namespace.\n  Run the following command and verify that the output contains an entry for the PRODUCT_VERSION that was installed in the previous steps:\nncn-m001# kubectl get cm cray-product-catalog -n services -o json | jq -r .data.uan PRODUCT_VERSION: configuration: clone_url: https://vcs.CRAY_EX_DOMAIN/vcs/cray/uan-config-management.git commit: 6658ea9e75f5f0f73f78941202664e9631a63726 import_branch: cray/uan/PRODUCT_VERSION import_date: 2021-07-28 03:26:00.399670 ssh_url: git@vcs.CRAY_EX_DOMAIN:cray/uan-config-management.git   Verify that the Kubernetes jobs that import the configuration content completed successfully. Skip this step if the previous substep indicates that the new UAN product version content installed successfully.\nA STATUS of Completed indicates that the Kubernetes jobs completed successfully.\nncn-m001# kubectl get pods -n services | grep uan uan-config-import-PRODUCT_VERSION-wfh4f 0/3 Completed 0 3m15s     Verify that the UAN RPM repositories have been created in Nexus:\nPRODUCT_VERSION is the UAN release number and SLE_VERSION is the SLE release version, such as 15sp4 or 15sp3.\nQuery Nexus through its REST API to display the repositories prefixed with the name uan:\nncn-m001# curl -s -k https://packages.local/service/rest/v1/repositories | jq -r \u0026#39;.[] | select(.name | startswith(\u0026#34;uan\u0026#34;)) | .name\u0026#39; uan-PRODUCT_VERSION-sle-SLE_VERSION   Finish the typescript file started at the beginning of this procedure.\n# exit   Optional: Perform Merge UAN Configuration Data if a previous version of the UAN product was already installed.\n  "
},
{
	"uri": "/docs-uan/en-254/advanced/repurposing_compute_as_uan/",
	"title": "Repurposing A Compute Node As A UAN",
	"tags": [],
	"description": "",
	"content": "Repurposing a Compute Node as a UAN This section describes how to repurpose a compute node to be used as a User Access Node (UAN). This is typically done when the processor type of the compute node is not yet available in a UAN server.\nOverview The following steps outline the process of repurposing a compute node to be used as a UAN.\n  Verify the System Default Route is set to CHN.\n  Change the role of the compute node in the Hardware State Manager from Compute to Application and set the sub-role to UAN.\n  Ensure that IPs on the CHN exist for the computes nodes in SLS.\n  Boot the repurposed compute node as a UAN.\n  Verify the repurposed compute node functions as a UAN.\n  Prerequisites There are no changes needed in hardware, network cabling, or UEFI/BIOS/BMC configuration to repurpose a compute node for use as a UAN. However, compute nodes do not have the necessary network interface cards to support user access over the Customer Access Network (CAN). Additionally, the network configuration of Mountain Cabinets do not support the CAN network. Therefore, repurposing a compute node as a UAN requires the system to be configured to use the Customer High-Speed Network (CHN) and that the compute nodes have a CHN IP address in SLS.\n The SLS Networks setting for the SystemDefaultRoute must be CHN The repurposed compute nodes must have CHN IP addresses in SLS uan_can_setup must be set to true in the uan-config-management repo  Procedure Perform the following steps to repurpose a compute node for use as a UAN.\n  Log in to the master node ncn-m001. All commands in this procedure are run from the master node.\n  Verify the system is configured to use the CHN as the System Default Route. If the SystemDefaultRoute is not CHN, the compute nodes may not be repurposed as UAN.\nncn-m001# cray sls networks describe BICAN --format json | jq -r \u0026#39;.ExtraProperties.SystemDefaultRoute\u0026#39;   Verify a CHN IP address exists in SLS for each repurposed compute node. Repeat the following command and replace \u0026lt;XNAME\u0026gt; with the xname of each repurposed compute node. The compute node must have a CHN IP address in SLS or it cannot be repurposed as a UAN. See Add Compute IP addresses to CHN SLS data section of the Cray System Management documentation for information on adding compute nodes to the CHN.\nncn-m001# cray sls networks describe CHN | q -r \u0026#39;.ExtraProperties.Subnets[] | select(.FullName == \u0026#34;CHN Bootstrap DHCP Subnet\u0026#34;) | .IPReservations[] | select(.Comment == \u0026#34;\u0026lt;XNAME\u0026gt;\u0026#34;)\u0026#39;   Verify that uan_can_setup: true is set in the uan-config-management CFS repo. See Enabling the Customer Access Network (CAN) or the Customer High Speed Network (CHN) for more information.\n  Change the role and sub-role in HSM of the compute node(s) being repurposed as UANs to Application and UAN, respectively. Repeat the following command and replace \u0026lt;XNAME\u0026gt; with the xname of each repurposed compute node.\nncn-m001# cray hsm state components role update --role Application --sub-role UAN \u0026lt;XNAME\u0026gt;   Verify the role and sub-role in HSM of the repurposed compute node(s) has been changed to \u0026lsquo;Application and 'UAN, respectively. Repeat the following command and replace \u0026lt;XNAME\u0026gt; with the xname of each repurposed compute node.\nncn-m001# cray hsm state components describe \u0026lt;XNAME\u0026gt;   Run the BOS session template used to boot the UAN nodes. See Boot UAN Nodes for more information on booting UAN nodes with BOS. Replace \u0026lt;UAN_SESSIONTEMPLATE\u0026gt; with the name of the BOS session template used to boot the UAN nodes and \u0026lt;XNAME\u0026gt; with the xname of the repurposed compute node.\nncn-m001# cray bos session create --template-uuid \u0026lt;UAN_SESSIONTEMPLATE\u0026gt; --operation reboot --limit \u0026lt;XNAME\u0026gt;   Verification as a UAN Once the repurposed compute node is booted as a UAN, the following steps will verify it is configured as a UAN. These steps may vary dependent upon how the site has configured the UAN nodes.\nBasic UAN Configuration Checks   Verify the repurposed compute node has finished the configuration phase. The output should \u0026ldquo;configured\u0026rdquo;.\nncn-m001# cray cfs components describe \u0026lt;XNAME\u0026gt; --format json | jq -r .configurationStatus   Login to the repurposed compute node from the master node ncn-m001 as the root user.\n  Verify that the hsn0 interface has the CHN IP address assigned to it in SLS.\nuan# ip a | grep hsn0   Verify the default route is via hsn0\nuan# ip r | grep default   Verify that all site UAN filesystems are mounted.\n  Common UAN Configuration Checks   If LDAP is used for user authentication, verify the LDAP service is reachable.\nuan# ping \u0026lt;ldap_service_ip\u0026gt;   If SLURM is used, test sinfo and srun commands. This example srun command should return the hostname of 4 compute nodes.\nuan# sinfo uan# srun -N4 hostname   Verify Users can Login   Login to the repurposed compute node as an authorized non-root user from any host that should have UAN access.\n  If SLURM is used, test sinfo and srun commands. This example srun command should return the hostname of 4 compute nodes.\nuan# sinfo uan# srun -N4 hostname   "
},
{
	"uri": "/docs-uan/en-254/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/docs-uan/en-254/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]