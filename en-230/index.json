[
{
	"uri": "/docs-uan/en-230/test-plan/",
	"title": "Test-plan",
	"tags": [],
	"description": "",
	"content": "Test-plan Topics:\n Test-plan  "
},
{
	"uri": "/docs-uan/en-230/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Troubleshooting Topics:\n Troubleshoot UAN Boot Issues Troubleshoot UAN CFS And Network Configuration Issues Troubleshoot UAN Disk Configuration Issues  "
},
{
	"uri": "/docs-uan/en-230/upgrade/",
	"title": "Upgrade",
	"tags": [],
	"description": "",
	"content": "Upgrade Topics:\n Merge UAN Configuration Data  "
},
{
	"uri": "/docs-uan/en-230/advanced/",
	"title": "Advanced",
	"tags": [],
	"description": "",
	"content": "Advanced Topics:\n Customizing UAN Images Manually  "
},
{
	"uri": "/docs-uan/en-230/install/",
	"title": "Install",
	"tags": [],
	"description": "",
	"content": "Install Topics:\n Install The UAN Product Stream  "
},
{
	"uri": "/docs-uan/en-230/installation_prereqs/",
	"title": "Installation Prereqs",
	"tags": [],
	"description": "",
	"content": "Installation Prereqs Topics:\n Configure The Bios Of A Gigabyte UAN Configure The Bios Of An HPE UAN Configure The BMC For UANs With Ilo Prepare For UAN Product Installation Images  "
},
{
	"uri": "/docs-uan/en-230/operations/",
	"title": "Operations",
	"tags": [],
	"description": "",
	"content": "Operations Topics:\n About UAN Configuration Boot UANs Build A New UAN Image Using The Cos Recipe Configure Interfaces On UANs Configure Pluggable Authentication Modules (pam) On UANs Create UAN Boot Images Mount A New File System On An UAN UAN Ansible Roles  "
},
{
	"uri": "/docs-uan/en-230/upgrade/merge_uan_configuration_data/",
	"title": "Merge UAN Configuration Data",
	"tags": [],
	"description": "",
	"content": "Merge UAN Configuration Data Perform this procedure to update the UAN product configuration.\nBefore performing this procedure:\n Perform Install the UAN Product Stream  Version 2.0.0 or later of the UAN product must be installed before performing this procedure.\nIn this procedure:\n PRODUCT_VERSION: refers to the full name of the UAN product version that is currently installed.    Start a typescript to capture the commands and output from the procedure.\nncn-m001# script -af product-uan.$\\(date +%Y-%m-%d\\).txt ncn-m001# export PS1=\u0026#39;\\\\u@\\\\H \\\\D\\{%Y-%m-%d\\} \\\\t \\\\w \\# \u0026#39;   Obtain the URL of UAN configuration management repository in VCS (the Gitea service).\nThis URL is reported as the value of the configuration.clone_url key in the cray-product-catalog Kubernetes ConfigMap.\n  Obtain the crayvcs password.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\  --template={{.data.vcs_password}} | base64 --decode   Clone the UAN configuration management repository. Replace the hostname reported in the URL obtained in the previous step with api-gw-service-nmm.local when cloning the repository.\nncn-m001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git . . . ncn-m001# cd uan-config-management \u0026amp;\u0026amp; git checkout cray/uan/PRODUCT_VERSION \u0026amp;\u0026amp; git pull Branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; set up to track remote branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; from \u0026#39;origin\u0026#39;. Already up to date.   Checkout the branch currently used to hold UAN configuration.\nThe following example assumes that branch is integration.\nncn-m001# git checkout integration Switched to branch \u0026#39;integration\u0026#39; Your branch is up to date with \u0026#39;origin/integration\u0026#39;.   Merge the new install branch to the current branch. Write a commit message when prompted.\nncn-m001# git merge cray/uan/PRODUCT_VERSION   Push the changes to VCS. Enter the crayvcs password when prompted.\nncn-m001# git push   Retrieve the commit ID from the merge and store it for later use.\nncn-m001# git rev-parse --verify HEAD   Update any CFS configurations used by the UANs with the commit ID from the previous step.\na. Download the JSON of the current UAN CFS configuration to a file.\nThis file will be named uan-config-2.3.0.json since it will be modified and then used for the updated UAN version.\nncn-m001# cray cfs configurations describe uan-config-2.0.0 \\  --format=json \u0026amp;\u0026gt;uan-config-2.3.0.json b. Remove the unneeded lines from the JSON file.\nThe lines to remove are: - the `lastUpdated` line - the last `name` line These must be removed before uploading the modified JSON file back into CFS to update the UAN configuration. ```bash ncn-m001# cat uan-config-2.0.1.json { \u0026quot;lastUpdated\u0026quot;: \u0026quot;2021-03-27T02:32:10Z\u0026quot;, \u0026quot;layers\u0026quot;: [ { \u0026quot;cloneUrl\u0026quot;: \u0026quot;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026quot;, \u0026quot;commit\u0026quot;: \u0026quot;aa5ce7d5975950ec02493d59efb89f6fc69d67f1\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;uan-integration-2.0.0\u0026quot;, \u0026quot;playbook\u0026quot;: \u0026quot;site.yml\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;uan-config-2.0.1-full\u0026quot; } ```  c. Replace the commit value in the JSON file with the commit ID obtained in Step 14.\nThe name value after the commit line may also be updated to match the new UAN product version, if desired. This is not necessary as CFS does not use this value for the configuration name. ```bash { \u0026quot;layers\u0026quot;: [ { \u0026quot;cloneUrl\u0026quot;: \u0026quot;https://api-gw-service-nmn.local/vcs/cray/uan-configmanagement.git\u0026quot;, \u0026quot;commit\u0026quot;: \u0026quot;aa5ce7d5975950ec02493d59efb89f6fc69d67f1\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;uan-integration-2.0.0\u0026quot;, \u0026quot;playbook\u0026quot;: \u0026quot;site.yml\u0026quot; } ] } ```  d. Create a new UAN CFS configuration with the updated JSON file.\nThe following example uses uan-config-2.0.1 for the name of the new CFS configuration, to match the JSON file name.\n```bash ncn-m001# cray cfs configurations update uan-config-2.0.1 \\ --file uan-config-2.0.1.json ```  e. Tell CFS to apply the new configuration to UANs by repeating the following command for each UAN. Replace UAN_XNAME in the command below with the name of a different UAN each time the command is run.\n```bash ncn-m001# cray cfs components update --desired-config uan-config-2.0.1 \\ --enabled true --format json UAN_XNAME ```    Finish the typescript file started at the beginning of this procedure.\nncn-m001# exit   Perform \u0026ldquo;Create UAN Boot Images\u0026rdquo; in the HPE Cray UAN Administration Guide to upgrade the boot images used by the UANs.\n  "
},
{
	"uri": "/docs-uan/en-230/",
	"title": "V2.3.0",
	"tags": [],
	"description": "",
	"content": "V2.3.0 Topics:\n Changelog Advanced Css Fonts Images Install Installation Prereqs Operations Test-plan Troubleshooting Upgrade  "
},
{
	"uri": "/docs-uan/en-230/operations/configure_pluggable_authentication_modules_pam_on_uans/",
	"title": "Configure Pluggable Authentication Modules (pam) On UANs",
	"tags": [],
	"description": "",
	"content": "Configure Pluggable Authentication Modules (PAM) on UANs Perform this procedure to configure PAM on UANs. This enables dynamic authentication support for system services.\nIntialize and configure the Cray command line interface (CLI) tool on the system. See \u0026ldquo;Configure the Cray Command Line Interface (CLI)\u0026rdquo; in the CSM documentation for more information.\n  Verify that the Gitea Version Control Service (VCS) is running.\nncn-m001# kubectl get pods --all-namespaces | grep vcs services gitea-vcs-f57c54c4f-j8k4t 2/2 Running 1 11d services gitea-vcs-postgres-0 2/2 Running 0 11d   Retrieve the initial Gitea login credentials for the crayvcs username.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\  --template={{.data.vcs_password}} | base64 --decode These credentials can be modified in the vcs_user role prior to installation or can be modified after logging in.\n  Use an external web browser to verify the Ansible plays are available on the system.\nThe URL will take on the following format:\nhttps://api.SYSTEM-NAME.DOMAIN-NAME/vcs\n  Clone the system Version Control Service (VCS) repository to a directory on the system.\nncn-w001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git   Change to the uan-config-management directory.\nncn-w001# cd uan-config-management   Make a new directory for the PAM configuration.\na. Create a group_vars/all directory if making changes to all UANs.\n ```bash ncn-w001# mkdir -p group_vars/all ncn-w001# cd group_vars/all ```  b. Create a host_vars/XNAME directory if the change is node specific.\n ```bash ncn-w001# mkdir -p host_vars/XNAME ncn-w001# cd host_vars/XNAME ```    Configure PAM.\nThe default path is /etc/pam.d/, so only the module file name is required.\n# vi pam.yml --- uan_pam_modules: - name: pam_module_file_name lines: - \u0026#34;add this line to pam module file_name\u0026#34; - \u0026#34;add another line to pam module file_name\u0026#34; - name: another_pam_module_file_name lines: - \u0026#34;add this line to another_pam_module_file_name\u0026#34; The following is an example of adding the line \u0026quot;account required pam\\_access.so\u0026quot; to the /etc/pam.d/common-account PAM file. The \\t is used to place a tab between account required and pam\\_access.so to match the formatting of the common-account file contents. The quotes are required in the strings used in the lines filed.\n--- uan_pam_modules: - name: common-account lines: - \u0026#34;account required\\tpam_access.so\u0026#34;   Add the change from the working directory to the staging area.\n  All UANs:\nncn-w001# git add group_vars/all/pam.yml   Node specific:\nncn-w001# git add host_vars/XNAME/pam.yml     Commit the file to the master branch.\nncn-w001# git commit -am \u0026#39;Added PAM configuration\u0026#39;   Push the commit.\nncn-w001# git push If prompted, use the Gitea login credentials.\n  Reboot the UAN(s) with the Boot Orchestration Service (BOS).\nncn-w001# cray bos session create \\  --template-uuid UAN_SESSION_TEMPLATE --operation reboot   "
},
{
	"uri": "/docs-uan/en-230/operations/create_uan_boot_images/",
	"title": "Create UAN Boot Images",
	"tags": [],
	"description": "",
	"content": "Create UAN Boot Images This procedure updates the configuration management git repository to match the installed version of the UAN product. That updated configuration is then used to create UAN boot images and a BOS session template.\nUAN specific configuration, and other required configurations related to UANs are covered in this topic. Consult the documentation for the individual HPE products (for example, workload managers and the HPE Cray Programming Environment) that must be configured on the UANs.\nThis is the overall workflow for preparing UAN images for booting UANs:\n Clone the UAN configuration git repository and create a branch based on the branch imported by the UAN installation. Update the configuration content and push the changes to the newly created branch. Create a Configuration Framework Service (CFS) configuration for the UANs, specifying the git configuration and the UAN image to apply the configuration to. More HPE products can also be added to the CFS configuration so that the UANs can install multiple HPE products into the UAN image at the same time. Add the Slingshot Host Software (SHS) CFS layer. SHS is a required layer in the CFS config for COS and UAN images. Configure the UAN image using CFS and generate a newly configured version of the UAN image. Create a Boot Orchestration Service (BOS) boot session template for the UANs. This template maps the configured image, the CFS configuration to be applied post-boot, and the nodes which will receive the image and configuration.  Once the UAN BOS session template is created, the UANs will be ready to be booted by a BOS session.\nReplace PRODUCT_VERSION and CRAY_EX_HOSTNAME in the example commands in this procedure with the current UAN product version installed (See Step 1) and the hostname of the HPE Cray EX system, respectively.\nUAN IMAGE PRE-BOOT CONFIGURATION\n  Obtain the artifact IDs and other information from the cray-product-catalog Kubernetes ConfigMap. Record the following information:\n the clone_url the commit the import_branch value  Upon successful installation of the UAN product, the UAN configuration is cataloged in this ConfigMap. This information is required for this procedure.\nncn-m001# kubectl get cm -n services cray-product-catalog -o json | jq -r .data.uan PRODUCT_VERSION: configuration: clone_url: https://vcs.CRAY_EX_HOSTNAME/vcs/cray/uan-config-management.git commit: 6658ea9e75f5f0f73f78941202664e9631a63726 import_branch: cray/uan/PRODUCT_VERSION import_date: 2021-02-02 19:14:18.399670 ssh_url: git@vcs.CRAY_EX_HOSTNAME:cray/uan-config-management.git   Generate the password hash for the root user. Replace PASSWORD with the root password you wish to use.\nncn-m001# openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c4) PASSWORD   Obtain the HashiCorp Vault root token.\nncn-m001# kubectl get secrets -n vault cray-vault-unseal-keys -o jsonpath=\u0026#39;{.data.vault-root}\u0026#39; | base64 -d; echo   Write the password hash obtained in Step 2 to the HashiCorp Vault.\nThe vault login command will request a token. That token value is the output of the previous step. The vault read secret/uan command verifies that the hash was stored correctly. This password hash will be written to the UAN for the root user by CFS.\nncn-m001# kubectl exec -it -n vault cray-vault-0 -- sh export VAULT_ADDR=http://cray-vault:8200 vault login vault write secret/uan root_password=\u0026#39;HASH\u0026#39; vault read secret/uan   Write any uan_ldap sensitive data, such as the ldap_default_authtok value (if used), to the HashiCorp Vault.\nThe vault login command will request a token. That token value is the output of the Step 3. The vault read secret/uan_ldap command verifies that the uan_ldap data was stored correctly. Any values stored here will be written to the UAN /etc/sssd/sssd.conf file in the [domain] section by CFS.\nThis example shows storing a value for ldap_default_authtok. If more than one variable needs to be stored, they must be written in space separated key=value pairs on the same vault write secret/uan_ldap command line.\nncn-m001# kubectl exec -it -n vault cray-vault-0 -- sh export VAULT_ADDR=http://cray-vault:8200 vault login vault write secret/uan_ldap ldap_default_authtok=\u0026#39;TOKEN\u0026#39; vault read secret/uan_ldap   Obtain the password for the crayvcs user from the Kubernetes secret for use in the next command.\nncn-m001# kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode   Clone the UAN configuration management repository. Replace CRAY_EX_HOSTNAME in the clone url with api-gw-service-nmn.local when cloning the repository.\nThe repository is in the VCS/Gitea service and the location is reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.clone_url key. The CRAY_EX_HOSTNAME from the clone_url is replaced with api-gw-service-nmn.local in the command that clones the repository.\nncn-m001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git . . . ncn-m001# cd uan-config-management \u0026amp;\u0026amp; git checkout cray/uan/PRODUCT_VERSION \u0026amp;\u0026amp; git pull Branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; set up to track remote branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; from \u0026#39;origin\u0026#39;. Already up to date.   Create a branch using the imported branch from the installation to customize the UAN image.\nThis will be reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.import_branch key under the UAN section. The format is cray/uan/PRODUCT_VERSION. In this guide, an integration branch is used for examples, but the name can be any valid git branch name.\nModifying the cray/uan/PRODUCT_VERSION branch that was created by the UAN product installation is not allowed by default.\nncn-m001# git checkout -b integration \u0026amp;\u0026amp; git merge cray/uan/PRODUCT_VERSION Switched to a new branch \u0026#39;integration\u0026#39; Already up to date.   Apply any site-specific customizations and modifications to the Ansible configuration for the UAN nodes and commit the changes.\nThe default Ansible play to configure UAN nodes is site.yml in the base of the uan-config-management repository. The roles that are executed in this play allow for custom configuration as required for the system.\nConsult the individual Ansible role README.md files in the uan-config-management repository roles directory to configure individual role variables. Roles prefixed with uan_ are specific to UAN configuration and include network interfaces, disk, LDAP, software packages, and message of the day roles.\nVariables should be defined and overridden in the Ansible inventory locations of the repository as shown in the following example and not in the Ansible plays and roles defaults. See this page from the Ansible documentation for directory layouts for inventory.\nWarning: Never place sensitive information such as passwords in the git repository.\nThe following example shows how to add a vars.yml file containing site-specific configuration values to the Application_UAN group variable location.\nThese and other Ansible files do not necessarily need to be modified for UAN image creation. See About UAN Configuration for instructions for site-specific UAN configuration, including CAN configuration.\nncn-m001# vim group_vars/Application_UAN/vars.yml ncn-m001# git add group_vars/Application_UAN/vars.yml ncn-m001# git commit -m \u0026#34;Add vars.yml customizations\u0026#34; [integration ecece54] Add vars.yml customizations 1 file changed, 1 insertion(+) create mode 100644 group_vars/Application_UAN/vars.yml   Verify that the System Layout Service (SLS) and the uan_interfaces configuration role refer to the Mountain Node Management Network by the same name. Skip this step if there are no Mountain cabinets in the HPE Cray EX system.\na. Edit the roles/uan_interfaces/tasks/main.yml file and change the line that reads\nurl: http://cray-sls/v1/search/networks?name=MNMN to read\nurl: http://cray-sls/v1/search/networks?name=NMN_MTN.\nThe following excerpt of the relevant section of the file shows the result of the change. ```bash - name: Get Mountain NMN Services Network info from SLS local_action: module: uri url: http://cray-sls/v1/search/networks?name=NMN_MTN method: GET register: sls_mnmn_svcs ignore_errors: yes ```  b. Stage and commit the network name change\n```bash ncn-m# git add roles/uan_interfaces/tasks/main.yml ncn-m# git commit -m \u0026quot;Add Mountain cabinet support\u0026quot; ```    Push the changes to the repository using the proper credentials, including the password obtained previously.\nncn-m001# git push --set-upstream origin integration Username for \u0026#39;https://api-gw-service-nmn.local\u0026#39;: crayvcs Password for \u0026#39;https://crayvcs@api-gw-service-nmn.local\u0026#39;: . . . remote: Processed 1 references in total To https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git * [new branch] integration -\u0026gt; integration Branch \u0026#39;integration\u0026#39; set up to track remote branch \u0026#39;integration\u0026#39; from \u0026#39;origin\u0026#39;.   Capture the most recent commit for reference in setting up a CFS configuration and navigate to the parent directory.\nncn-m001# git rev-parse --verify HEAD ecece54b1eb65d484444c4a5ca0b244b329f4667 ncn-m001# cd .. The configuration parameters have been stored in a branch in the UAN git repository. The next phase of the process initiates the Configuration Framework Service (CFS) to customize the image.\n  CONFIGURE UAN IMAGES\n Create a JSON input file for generating a CFS configuration for the UAN.\nGather the git repository clone URL, commit, and top-level play for each configuration layer (that is, Cray product). Add them to the CFS configuration for the UAN, if needed.\nFor the commit value for the UAN layer, use the Git commit value obtained in the previous step.\nSee the product-specific documentation for further information on configuring other HPE products, as this procedure documents only the required configuration of the UAN. More layers can be added to be configured in a single CFS session.\nThe following configuration example can be used for preboot image customization as well as post-boot node configuration.\nNote that the Slingshot Host Software CFS layer is listed first. This is required as the UAN layer will attempt to install DVS and Lustre packages that require SHS be installed first. The correct playbook for Cassini or Mellanox must also be specified. Consult the Slingshot Host Software documentation for more information.\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;shs-integration-SHS_PRODUCT_VERSION\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/slingshot-host-software-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;shs_cassini_install.yml\u0026#34; \u0026#34;commit\u0026#34;: \u0026#34;45f891adb9a2e7ca304565b21b29f29226fa3e0f\u0026#34;, }, { \u0026#34;name\u0026#34;: \u0026#34;uan-integration-PRODUCT_VERSION\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;ecece54b1eb65d484444c4a5ca0b244b329f4667\u0026#34; } # **{ ... add configuration layers for other products here, if desired ... }** ] }   Add the configuration to CFS using the JSON input file.\nIn the following example, the JSON file created in the previous step is named uan-config-PRODUCT_VERSION.json only the details for the UAN layer are shown.\nncn-m001# cray cfs configurations update uan-config-PRODUCT_VERSION --file ./uan-config-PRODUCT_VERSION.json --format json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:00:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;shs-integration-SHS_PRODUCT_VERSION\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/slingshot-host-software-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;shs_cassini_install.yml\u0026#34; \u0026#34;commit\u0026#34;: \u0026#34;45f891adb9a2e7ca304565b21b29f29226fa3e0f\u0026#34;, }, { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;ecece54b1eb65d484444c4a5ca0b244b329f4667\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;uan-integration-PRODUCT_VERSION\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } **# \u0026lt;-- Additional layers not shown, but would be inserted here** ], \u0026#34;name\u0026#34;: \u0026#34;uan-config-PRODUCT_VERSION\u0026#34; } The UAN layer must be first as it configures the network interfaces that may be required by subsequent layers. When other products are added to the CFS configuration used to boot UANs, the suggested order of the layers would be:\n Slingshot Host Software (must be specified before UAN) UAN CPE (Cray Programming Environment) Workload Manager (Either Slurm or PBS Pro) Analytics customer    Create a CFS session to perform preboot image customization of the UAN image.\nncn-m001# cray cfs sessions create --name uan-config-PRODUCT_VERSION \\  --configuration-name uan-config-PRODUCT_VERSION \\  --target-definition image \\  --target-group Application_UAN IMAGE_ID\\  --format json   Wait until the CFS configuration session for the image customization to complete. Then record the ID of the IMS image created by CFS.\nThe following command will produce output while the process is running. If the CFS session completes successfully, an IMS image ID will appear in the output.\nncn-m001# cray cfs sessions describe uan-config-PRODUCT_VERSION --format json | jq   PREPARE UAN BOOT SESSION TEMPLATES\n Retrieve the xnames of the UAN nodes from the Hardware State Manager (HSM).\nThese xnames are needed for Step 18.\nncn-m001# cray hsm state components list --role Application --subrole UAN --format json | jq -r .Components[].ID x3000c0s19b0n0 x3000c0s24b0n0 x3000c0s20b0n0 x3000c0s22b0n0   Construct a JSON BOS boot session template for the UAN.\na. Populate the template with the following information:\n- The xnames of Application nodes from Step 17 - The customized image ID from Step 16 for - The CFS configuration session name from Step 14  b. Verify that the session template matches the format and structure in the following example:\n```json { \u0026quot;boot_sets\u0026quot;: { \u0026quot;uan\u0026quot;: { \u0026quot;boot_ordinal\u0026quot;: 2, \u0026quot;kernel_parameters\u0026quot;: \u0026quot;spire_join_token=${SPIRE_JOIN_TOKEN}\u0026quot;, \u0026quot;network\u0026quot;: \u0026quot;nmn\u0026quot;, \u0026quot;node_list\u0026quot;: [ ** [ ... List of Application Nodes from cray hsm state command ...]** ], \u0026quot;path\u0026quot;: \u0026quot;s3://boot-images/IMS_IMAGE_ID/manifest.json\u0026quot;, **\u0026lt;-- result_id from Step 15** \u0026quot;rootfs_provider\u0026quot;: \u0026quot;cpss3\u0026quot;, \u0026quot;rootfs_provider_passthrough\u0026quot;: \u0026quot;dvs:api-gw-service-nmn.local:300:nmn0\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;s3\u0026quot; } }, \u0026quot;cfs\u0026quot;: { \u0026quot;configuration\u0026quot;: \u0026quot;uan-config-PRODUCT_VERSION\u0026quot; }, \u0026quot;enable_cfs\u0026quot;: true, \u0026quot;name\u0026quot;: \u0026quot;uan-sessiontemplate-PRODUCT_VERSION\u0026quot; } ```  c. Save the template with a descriptive name, such as uan-sessiontemplate-PRODUCT_VERSION.json.\n  Register the session template with BOS.\nThe following command uses the JSON session template file to save a session template in BOS. This step allows administrators to boot UANs by referring to the session template name.\nncn-m001# cray bos sessiontemplate create \\  --name uan-sessiontemplate-PRODUCT_VERSION \\  --file uan-sessiontemplate-PRODUCT_VERSION.json /sessionTemplate/uan-sessiontemplate-PRODUCT_VERSION   Perform Boot UANs to boot the UANs with the new image and BOS session template.\n  "
},
{
	"uri": "/docs-uan/en-230/operations/mount_a_new_file_system_on_an_uan/",
	"title": "Mount A New File System On A UAN",
	"tags": [],
	"description": "",
	"content": "Mount a New File System on a UAN Perform this procedure to create a mount point for a new file system on a UAN.\n  Perform Steps 1-9 of Create UAN Boot Images.\n  Create a directory for Application role nodes.\nncn-w001# mkdir -p group_vars/Application   Define the home directory information for the new file system in the filesystems.yml file.\nncn-w001# vi group_vars/Application/filesystems.yml --- filesystems: - src: 10.252.1.1:/home mount_point: /home fstype: nfs4 opts: rw,noauto state: mounted   Add the change from the working directory to the staging area.\nncn-w001# git add -A   Commit the file to the working branch.\nncn-w001# git commit -am \u0026#39;Added file system info\u0026#39;   Resume Create UAN Boot Images at Step 10.\n  "
},
{
	"uri": "/docs-uan/en-230/test-plan/test-plan/",
	"title": "Test Plan For User Access Node (UAN)",
	"tags": [],
	"description": "",
	"content": "Test Plan for User Access Node (UAN) The following is the test plan for the User Access Node (UAN). The tests are grouped in three categories:\n Unit tests Integration tests Functional tests  Unit Tests When possible, unit tests are run when the various components of UAN are built (uan-rpms, uan, and uan-product-stream). The uan repository contains the ansible code used when UANs boot and configure and much of the testing for that compoment must occur on a fully configured system. Future enhancements are underway to test the various compoments of UAN on a Virtual Shasta enviroment in GCP.\n   Summary Description Automated Notes     uan builds Verify the uan repo is able to generate artifacts yes Run make in a local checkout   uan-rpms builds Verify the uan-rpms repo is able to generate rpms yes Run make in a local checkout   uan-product-stream builds Verify the uan-product-stream repo is able to generate a release yes Tag a new release in uan-product-stream or run make in a local checkout    Integration Tests The following integration tests verify that the UAN software interacts correctly with the rest of the products. The result of the integration tests will be a fully built and customized UAN image that boots and configures correctly. Depending on the configuration of the test system, extra integrations tests may be performed (HSN booting, WLM configuration, GPU configuration, etc).\n   Summary Description Automated Notes     Install the UAN product Run the install procedure Yes Run install.sh from Install_the_UAN_Product_Stream.md   Build a COS recipe for UAN Verify the COS recipe can be built as the base for a UAN image no Build_a_New_UAN_Image_Using_the_COS_Recipe.md   Run UAN CFS Verify the CFS layers run correctly (SHS, UAN, WLM, CPE, etc) no Create_UAN_Boot_Images.md   Boot UANs Verify the UAN boots successfully no Boot_UANS.md   Enable HSN booting Verify the UAN is able to HSN boot no Consult COS documentation   Enable GPU Verify the UAN is able to configure GPUs no Consult GPU documentation    Functional Tests With a fully configured Shasta system, the following functional tests determines that a UAN is able to perform its intended capabilities.\n   Summary Description Automated Notes     User Authentication Verify a user is able to ssh to the UAN using LDAP authentication. no ssh user@uan   Job launch Verify a user is able to submit a basic job. no srun hostname   Verify CPE Verify the Cray Programming Environment is available no module list   Verify GPU functionality Run the test suite if GPUs are configured yes `/opt/cray/uan/tests/validate-gpu.sh \u0026lt;nvidia    "
},
{
	"uri": "/docs-uan/en-230/troubleshooting/troubleshoot_uan_boot_issues/",
	"title": "Troubleshoot UAN Boot Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAN Boot Issues The UAN boot process BOS boots UANs. BOS uses session templates to define various parameters such as:\n Which nodes to boot Which image to boot Kernel parameters Whether to perform post-boot configuration (Node Personalization) of the nodes by CFS. Which CFS configuration to use if Node Personalization is enabled.  UAN boots are performed in three phases:\n PXE booting an iPXE binary that will load the initrd of the UAN image that will boot. Booting the initrd (dracut) image which configures the UAN for booting the UAN image. This process consists of two phases.  Configuring the UAN node to use the Content Projection Service (CPS) and Data Virtualization Service (DVS). These services manage the UAN image rootfs mounting and make that image available to the UAN nodes. Mounting the rootfs   Booting the UAN image rootfs.  PXE Issues Most PXE boot failures are the result of misconfigured network switches and/or BIOS settings. The UAN must PXE boot over the Node Management Network (NMN) and the switches must be configured to allow connectivity to the NMN. The cable for the NMN must be connected to the first port of the OCP card on HPE DL325 and DL385 servers or to the first port of the built-in LAN-On-Motherboard (LOM) on Gigabyte servers. See \u0026ldquo;Prepare for UAN Product Installation\u0026rdquo; in the UAN Installation Guide for details on the switch and BIOS settings required to configure the UAN for PXE booting.\nUANs may fail to boot when the BIOS EFITIME is too far away from the time on management nodes. If there are x509 certificate problems, check that the BIOS time is correct. See \u0026ldquo;Configure the BIOS of an HPE UAN\u0026rdquo; or \u0026ldquo;Configure the BIOS of a Gigabyte UAN\u0026rdquo; in the UAN Installation Guide for examples of checking settings in the BIOS.\nInitrd (Dracut) Issues Dracut failures are often caused by the wrong interface being named nmn0, or to multiple entries for the UAN xname in DNS. The latter is a result of multiple interfaces making DHCP requests. Either condition can cause IP address mismatches in the dvs_node_map. DNS configures entries based on DHCP leases.\nWhen dracut starts, it renames the network device named by the ifmap=netX:nmn0 kernel parameter to nmn0. This interface is the only one dracut will enable DHCP on. The ip=nmn0:dhcp kernel parameter limits dracut to DHCP only nmn0. The ifmap value must be set correctly in the kernel_parameters field of the BOS session template.\nSee Create UAN Boot Images for details on how to configure the BOS session template. For UAN nodes that have more than one PCI card installed, ifmap=net2:nmn0 is the correct setting. If only one PCI card is installed, ifmap=net0:nmn0 is normally the correct setting.\nUANs require CPS and DVS to boot from images. These services are configured in dracut to retrieve the rootfs and mount it. If the image fails to download, check that DVS and CPS are both healthy, and DVS is running on all worker nodes. Run the following commands to check DVS and CPS:\nncn-m001# kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; --no-headers ncn-w001 ncn-w002 ncn-m001# for node in `kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; \\ --no-headers`; do ssh $node \u0026#34;lsmod | grep \u0026#39;^dvs \u0026#39;\u0026#34; done ncn-w001 ncn-w002 If DVS and CPS are both healthy, then both of these commands will return all the worker NCNs in the HPE Cray EX system.\nImage Boot Issues Once dracut exits, the UAN will boot the rootfs image. Failures seen in this phase tend to be failures of spire-agent, cfs-state-reporter, or both. The cfs-state-reporter tells BOA that the node is ready and allows BOA to start CFS for Node Personalization. If cfs-state-reporter does not start, check if the spire-agent has started. The cfs-state-reporter depends on the spire-agent. Running systemctl status spire-agent will show that that service is enabled and running if there are no issues with that service. Similarly, running systemctl status cfs-state-reporter will show a status of SUCCESS.\nuan# systemctl status spire-agent ● spire-agent.service - SPIRE Agent Loaded: loaded (/usr/lib/systemd/system/spire-agent.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2021-02-24 14:27:33 CST; 19h ago Main PID: 3581 (spire-agent) Tasks: 57 CGroup: /system.slice/spire-agent.service └─3581 /usr/bin/spire-agent run -expandEnv -config /root/spire/conf/spire-agent.conf uan# systemctl status cfs-state-reporter ● cfs-state-reporter.service - cfs-state-reporter reports configuration level of the system Loaded: loaded (/usr/lib/systemd/system/cfs-state-reporter.service; enabled; vendor preset: enabled) Active: inactive (dead) since Wed 2021-02-24 14:29:51 CST; 19h ago Main PID: 3827 (code=exited, status=0/SUCCESS) There may be errors related to failing to load kernel modules during the boot:\nFAILED Failed to start Load Kernel Modules. See \u0026#39;systemctl status systemd-modules-load.service\u0026#39; for details. Provided the UAN boots and completes post boot customizations, these messages may be ignored.\n"
},
{
	"uri": "/docs-uan/en-230/troubleshooting/troubleshoot_uan_cfs_and_network_configuration_issues/",
	"title": "Troubleshoot UAN CFS And Network Configuration Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAN CFS and Network Configuration Issues Examine the UAN CFS pod logs to help troubleshoot CFS and networking issues on UANs.\nRead About UAN Configuration before starting this procedure.\n  Obtain the name of the CFS session that failed by running the following command on a management or worker NCN:\nThis example sorts the list of CFS sessions so that the most recent one is at the bottom.\nncn# kubectl -n services get pods --sort-by=.metadata.creationTimestamp | grep ^cfs   View the Ansible log of the CFS session found in the previous step (CFS_SESSION in the following example). Use the information in log to guide troubleshooting.\nncn# kubectl -n services logs -f -c ansible-0 CFS_SESSION   Optional: Troubleshoot uan_interfaces issues by logging into the affected node (usually with the conman console) and using standard network debugging techniques.\nNMN and CAN network setup errors can also result from incorrect switch configuration and network cabling.\n  "
},
{
	"uri": "/docs-uan/en-230/troubleshooting/troubleshoot_uan_disk_configuration_issues/",
	"title": "Troubleshoot UAN Disk Configuration Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAN Disk Configuration Issues Perform this procedure to enable uan_disk_config to run successfully by erasing existing disk partitions. UAN disk configuration will fail if the disk on the node is already partitioned. Manually erase any existing partitions to fix the issue.\nThis procedure currently only addresses uan_disk_config errors due to existing disk partitions.\nRefer to About UAN Configuration for an explanation of UAN disk configuration.\nThe most common cause of failure in the uan_disk_config role is the disk having been previously configured without a /scratch and /swap partition. Existing partitions prevent the parted command from dividing the disk into those two equal partitions. The solution is to log into the node and run parted manually to remove the existing partitions on that disk.\n  Examine the CFS log and identify the failed disk device.\n  Log into the affected UAN as root.\n  Use parted to manually remove any existing partitions.\nThe following example uses /dev/sdb as the disk device. Also, as partitions are removed, the remaining partitions are renumbered. Therefore, rm 1 is issued twice to remove both partitions.\nuan# parted GNU Parted 3.2 Using /dev/sda Welcome to GNU Parted! Type \u0026#39;help\u0026#39; to view a list of commands. (parted) select /dev/sdb Using /dev/sdb (parted) print Model: ATA VK000480GWSRR (scsi) Disk /dev/sdb: 480GB Sector size (logical/physical): 512B/4096B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 240GB 240GB primary ext4 type=83 2 240GB 480GB 240GB primary ext4 type=83 (parted) rm 1 (parted) rm 1 (parted) print Model: ATA VK000480GWSRR (scsi) Disk /dev/sdb: 480GB Sector size (logical/physical): 512B/4096B Partition Table: msdos Disk Flags: (parted) quit uan01:~ #   Either reboot the affected UAN or launch a CFS session against it to rerun the uan_disk_config role.\n  "
},
{
	"uri": "/docs-uan/en-230/operations/uan_ansible_roles/",
	"title": "UAN Ansible Roles",
	"tags": [],
	"description": "",
	"content": "UAN Ansible Roles uan_disk_config The uan_disk_config role configures swap and scratch disk partitions on UAN nodes.\nRequirements There must be disk devices found on the UAN node by the device_filter module or this role will exit with failure. This condition can be ignored by setting uan_require_disk to false. See variable definitions below.\nSee the library/device_filter.py file for more information on this module.\nThe device that is found will be unmounted if mounted and a swap partition will be created on the first half of the disk, and a scratch partition on the second half. ext4 filesystems are created on each partition.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_require_disk Boolean to determine if this role continues to setup disk if no disks were found by the device filter. Set to true to exit with error when no disks are found.\nuan_require_disk: false uan_device_name_filter Regular expression of disk device name for this role to filter. Input to the device_filter module.\nuan_device_name_filter: \u0026#34;^sd[a-f]$\u0026#34; uan_device_host_filter Regular expression of host for this role to filter. Input to the device_filter module.\nuan_device_host_filter: \u0026#34; uan_device_model_filter Regular expression of device model for this role to filter. Input to the device_filter module.\nuan_device_model_filter: \u0026#34; uan_device_vendor_filter Regular expression of disk vendor for this role to filter. Input to the device_filter module.\nuan_device_vendor_filter: \u0026#34; uan_device_size_filter Regular expression of disk size for this role to filter. Input to the device_filter module.\nuan_device_size_filter: \u0026#34;\u0026lt;1TB\u0026#34; uan_swap Filesystem location to mount the swap partition.\nuan_swap: \u0026#34;/swap\u0026#34; uan_scratch Filesystem location to mount the scratch partition.\nuan_scratch: \u0026#34;/scratch\u0026#34; swap_file Name of the swapfile to create. Full path is \u0026lt;uan_swap\u0026gt;/\u0026lt;swapfile\u0026gt;.\nswap_file: \u0026#34;swapfile\u0026#34; swap_dd_command dd command to create the swapfile.\nswap_dd_command: \u0026#34;/usr/bin/dd if=/dev/zero of={{ uan_swap }}/{{ swap_file }} bs=1GB count=10\u0026#34; swap_swappiness Value to set the swapiness in sysctl.\nswap_swappiness: \u0026#34;10\u0026#34; Dependencies library/device_filter.py is required to find eligible disk devices.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_disk_config } This role is included in the UAN site.yml play.\nuan_interfaces The uan_interfaces role configures site/customer-defined network interfaces and Shasta Customer Access Network (CAN) network interfaces on UAN nodes.\nRequirements None.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_can_setup uan_can_setup configures the Customer Access Network (CAN) on UAN nodes. If this value is falsey no CAN is configured on the nodes.\nuan_can_setup: no sls_nmn_name sls_nmn_name is the Node Management Network name used by SLS.\nsls_nmn_name: \u0026#34;NMN\u0026#34; sls_nmn_svcs_name sls_nmn_svcs_name is the Node Management Services Network name used by SLS.\nsls_nmn_svcs_name: \u0026#34;NMNLB\u0026#34; sls_mnmn_svcs_name sls_mnmn_svcs_name is the Mountain Node Management Services Network name used by SLS.\nsls_mnmn_svcs_name: \u0026#34;NMN_MTN\u0026#34; sls_can_name sls_can_name is the Customer Access Network name used by SLS.\nsls_can_name: \u0026#34;CAN\u0026#34; customer_uan_interfaces customer_uan_interfaces is as list of interface names used for constructing ifcfg-\u0026lt;customer_uan_interfaces.name\u0026gt; files. Define ifcfg fields for each interface here. Field names are converted to uppercase in the generated ifcfg-\u0026lt;name\u0026gt; file(s).\nInterfaces should be defined in order of dependency.\ncustomer_uan_interfaces: [] # Example: customer_uan_interfaces: - name: \u0026#34;net1\u0026#34; settings: bootproto: \u0026#34;static\u0026#34; device: \u0026#34;net1\u0026#34; ipaddr: \u0026#34;1.2.3.4\u0026#34; startmode: \u0026#34;auto\u0026#34; - name: \u0026#34;net2\u0026#34; settings: bootproto: \u0026#34;static\u0026#34; device: \u0026#34;net2\u0026#34; ipaddr: \u0026#34;5.6.7.8\u0026#34; startmode: \u0026#34;auto\u0026#34; customer_uan_routes customer_uan_routes is as list of interface routes used for constructing ifroute-\u0026lt;customer_uan_routes.name\u0026gt; files.\ncustomer_uan_routes: [] # Example customer_uan_routes: - name: \u0026#34;net1\u0026#34; routes: - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; - name: \u0026#34;net2\u0026#34; routes: - \u0026#34;default 10.103.8.20 255.255.255.255 - table 3\u0026#34; - \u0026#34;10.103.8.128/25 10.103.8.20 255.255.255.255 net2\u0026#34; customer_uan_rules customer_uan_rules is as list of interface rules used for constructing ifrule-\u0026lt;customer_uan_routes.name\u0026gt; files.\ncustomer_uan_rules: [] # Example customer_uan_rules: - name: \u0026#34;net1\u0026#34; rules: - \u0026#34;from 10.1.0.0/16 lookup 1\u0026#34; - name: \u0026#34;net2\u0026#34; rules: - \u0026#34;from 10.103.8.0/24 lookup 3\u0026#34; customer_uan_global_routes customer_uan_global_routes is a list of global routes used for constructing the \u0026ldquo;routes\u0026rdquo; file.\ncustomer_uan_global_routes: [] # Example customer_uan_global_routes: - routes: - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; external_dns_searchlist external_dns_searchlist is a list of customer-configurable fields to be added to the /etc/resolv.conf DNS search list.\nexternal_dns_searchlist: [ \u0026#39;\u0026#39; ] # Example external_dns_searchlist: - \u0026#39;my.domain.com\u0026#39; - \u0026#39;my.other.domain.com\u0026#39; external_dns_servers external_dns_servers is a list of customer-configurable fields to be added to the /etc/resolv.conf DNS server list.\nexternal_dns_servers: [ \u0026#39;\u0026#39; ] # Example external_dns_servers: - \u0026#39;1.2.3.4\u0026#39; - \u0026#39;5.6.7.8\u0026#39; external_dns_options external_dns_options is a list of customer-configurable fields to be added to the /etc/resolv.conf DNS options list.\nexternal_dns_options: [ \u0026#39;\u0026#39; ] # Example external_dns_options: - \u0026#39;single-request\u0026#39; uan_access_control uan_access_control is a boolean variable to control whether non-root access control is enabled Default is no\nuan_access_control: no api_gateways api_gateways is a list of API gateway DNS names to block non-user access\napi_gateways: - \u0026#34;api-gw-service\u0026#34; - \u0026#34;api-gw-service.local\u0026#34; - \u0026#34;api-gw-service-nmn.local\u0026#34; - \u0026#34;kubeapi-vip\u0026#34; api_gw_ports api_gw_ports is a list of gateway ports to protect.\napi_gw_ports: \u0026#34;80,443,8081,8888\u0026#34; sls_url sls_url is the SLS URL.\nsls_url: \u0026#34;http://cray-sls\u0026#34; Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_interfaces } This role is included in the UAN site.yml play.\nuan_ldap The uan_ldap role configures LDAP and AD groups on UAN nodes.\nRequirements NSCD, pam-config, sssd.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_ldap_setup is a boolean variable to selectively skip the setup of LDAP on nodes it would otherwise be configured due to uan_ldap_config being defined. The default setting is to setup LDAP when uan_ldap_config is not empty.\nuan_ldap_setup: yes uan_ldap_config configures LDAP domains and servers. If this list is empty, no LDAP configuration will be applied to the UAN targets and all role tasks will be skipped.\nuan_ldap_config: [] # Example uan_ldap_config: - domain: \u0026#34;mydomain-ldaps\u0026#34; search_base: \u0026#34;dc=...,dc=...\u0026#34; servers: [\u0026#34;ldaps://123.123.123.1\u0026#34;, \u0026#34;ldaps://213.312.123.132\u0026#34;] chpass_uri: [\u0026#34;ldaps://123.123.123.1\u0026#34;] - domain: \u0026#34;mydomain-ldap\u0026#34; search_base: \u0026#34;dc=...,dc=...\u0026#34; servers: [\u0026#34;ldap://123.123.123.1\u0026#34;, \u0026#34;ldap://213.312.123.132\u0026#34;] uan_ad_groups configures active directory groups on UAN nodes.\nuan_ad_groups: [] # Example uan_ad_groups: - { name: admin_grp, origin: ALL } - { name: dev_users, origin: ALL } uan_pam_modules configures PAM modules on the UAN nodes in /etc/pam.d.\nuan_pam_modules: [] # Example uan_pam_modules: - name: \u0026#34;common-account\u0026#34; lines: - \u0026#34;account required\\tpam_access.so\u0026#34; Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_ldap } This role is included in the UAN site.yml play.\nuan_motd The uan_motd role appends text to the /etc/motd file.\nRequirements None.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_motd_content: [] uan_motd_content contains text to be added to the end of the /etc/motd file.\nDependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_motd, uan_motd_content: \u0026#34;MOTD CONTENT\u0026#34; } This role is included in the UAN site.yml play.\nuan_packages The uan_packages role installs additional RPMs on UANs using the Ansible zypper module.\nPackages that are required for UANs to function should be preferentially installed during image customization and/or image creation.\nInstalling RPMs during post-boot node configuration can cause high system loads on large systems.\nThis role will only run on SLES-based nodes.\nRequirements Zypper must be installed.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_additional_sles15_packages: [] uan_additional_sles15_packages contains the list of RPM packages to install.\nDependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_packages, uan_additional_sles15_packages: [\u0026#39;vim\u0026#39;] } This role is included in the UAN site.yml play.\nuan_shadow The uan_shadow role configures the root password on UAN nodes.\nRequirements The root password hash has to be installed in HashiCorp Vault at secret/uan root_password.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_vault_url uan_vault_url is the URL for the HashiCorp Vault\nuan_vault_url: \u0026#34;http://cray-vault.vault:8200\u0026#34; uan_vault_role_file uan_vault_role_file is the required Kubernetes role file for HashiCorp Vault access.\nuan_vault_role_file: /var/run/secrets/kubernetes.io/serviceaccount/namespace uan_vault_jwt_file uan_vault_jwt_file is the path to the required Kubernetes token file for HashiCorp Vault access.\nuan_vault_jwt_file: /var/run/secrets/kubernetes.io/serviceaccount/token uan_vault_path uan_vault_path is the path to use for storing data for UANs in HashiCorp Vault.\nuan_vault_path: secret/uan uan_vault_key uan_vault_key is the key used for storing the root password in HashiCorp Vault.\nuan_vault_key: root_password Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_shadow } This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-230/operations/about_uan_configuration/",
	"title": "About UAN Configuration",
	"tags": [],
	"description": "",
	"content": "About UAN Configuration This section describes the Ansible playbooks and roles that configure UANs.\nUAN configuration overview Configuration of UAN nodes is performed by the Configuration Framework Service (CFS). CFS can apply configuration to both images and nodes. When the configuration is applied to nodes, the nodes must be booted and accessible through SSH over the Node Management Network (NMN).\nThe Ansible roles involved in UAN configuration are listed in the site.yml file in the uan-config-management git repository in VCS. Most of the roles that are specific to image configuration are required for the operation as a UAN and must not be removed from site.yml.\nThe UAN-specific roles involved in post-boot UAN node configuration are:\n uan_disk_config: this role configures the last disk found on the UAN that is smaller than 1TB, by default. That disk will be formatted with a scratch and swap partition mounted at /scratch and /swap, respectively. Each partition is 50% of the disk. uan_packages: this role installs any RPM packages listed in the uan-config-management repo. uan_interfaces: this role configures the UAN node networking. By default, this role does not configure a default route or the Customer Access Network (CAN) connection for the HPE Cray EX supercomputer. If CAN is enabled, the default route will be on the CAN. Otherwise, a default route must be set up in the customer interfaces definitions. Without the CAN, there will not be an external connection to the customer site network unless one is defined in the customer interfaces. uan_motd: this role Provides a default message of the day that can be customized by the administrator. uan_ldap: this optional role configures the connection to LDAP servers. To disable this role, the administrator must set \u0026lsquo;uan_ldap_setup:no\u0026rsquo; in the \u0026lsquo;uan-config-management\u0026rsquo; VCS repository.  The UAN roles in site.yml are required and must not be removed, with exception of uan_ldap if the site is using some other method of user authentication. The uan_ldap may also be skipped by setting the value of uan_ldap_setup to no in a group_vars or host_vars configuration file.\nFor more information about these roles, see UAN Ansible Roles.\nUAN network configuration The uan_interfaces role configures the interfaces on the UAN nodes in three phases:\n Setup and configure the NMN.  Gather information from the System Layout Service (SLS) for the NMN. Populate /etc/resolv.conf. Configure the first OCP port on an HPE server, or the first LOM port on a Gigabyte server, as the nmn0 interface.   Set up the CAN, if wanted  Gather information from SLS for the CAN. Configure the route to the CAN gateway as the default one. Implement the CAN interface as a bonded pair.   On HPE servers, use the second port of the 25Gb OCP card and a second 25Gb card.\n  On Gigabyte servers, use both ports of the 40Gb card.\nSee Configure Interfaces on UANs for detailed instructions.\n     Setup customer-defined networks  UAN LDAP network requirements LDAP configuration requires either a CAN or another customer-provided network that can route to the LDAP servers. Both such networks route outside of the HPE Cray EX system. If a UAN only has the nmn0 interface configured and active, the UAN cannot route outside of the system.\n"
},
{
	"uri": "/docs-uan/en-230/operations/boot_uans/",
	"title": "Boot UANs",
	"tags": [],
	"description": "",
	"content": "Boot UANs Perform this procedure to boot UANs using BOS so that they are ready for user logins.\nPerform Create UAN Boot Images before performing this procedure.\n  Create a BOS session to boot the UAN nodes.\nncn-m001# cray bos session create --template-uuid uan-sessiontemplate-PRODUCT_VERSION --operation reboot --format json | tee session.json { \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;jobId\u0026#34;: \u0026#34;boa-89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;session\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d/status\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;status\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; } ], \u0026#34;operation\u0026#34;: \u0026#34;reboot\u0026#34;, \u0026#34;templateUuid\u0026#34;: \u0026#34;uan-sessiontemplate-PRODUCT_VERSION\u0026#34; }   Retrieve the BOS session ID from the output of the previous command.\nncn-m001# export BOS_SESSION=$(jq -r \u0026#39;.links[] | select(.rel==\u0026#34;session\u0026#34;) | .href\u0026#39; session.json | cut -d \u0026#39;/\u0026#39; -f4) ncn-m001# echo $BOS_SESSION 89680d0a-3a6b-4569-a1a1-e275b71fce7d   Retrieve the Boot Orchestration Agent (BOA) Kubernetes job name for the BOS session.\nncn-m001# BOA_JOB_NAME=$(cray bos session describe $BOS_SESSION --format json | jq -r .job)   Retrieve the Kuberenetes pod name for the BOA assigned to run this session.\nncn-m001# BOA_POD=$(kubectl get pods -n services -l job-name=$BOA_JOB_NAME --no-headers -o custom-columns=\u0026#34;:metadata.name\u0026#34;)   View the logs for the BOA to track session progress.\nncn-m001# kubectl logs -f -n services $BOA_POD -c boa   List the CFS sessions started by the BOA. Skip this step if CFS was not enabled in the boot session template used to boot the UANs.\nIf CFS was enabled in the boot session template, the BOA will initiate a CFS session.\nIn the following command, pending and complete are also valid statuses to filter on.\nncn-m001# cray cfs sessions list --tags bos_session=$BOS_SESSION --status running --format json   "
},
{
	"uri": "/docs-uan/en-230/operations/build_a_new_uan_image_using_the_cos_recipe/",
	"title": "Build A New UAN Image Using A Cos Recipe",
	"tags": [],
	"description": "",
	"content": "Build a New UAN Image Using a COS Recipe Prior to UAN 2.3, a similar copy of the COS recipe was imported with the UAN install. In the UAN 2.3 release and beyond, UAN does not install a recipe, and a COS recipe must be used. Additional uan packages will now be installed via CFS and the uan_packages role.\nPerform the following before starting this procedure:\n Install the COS, Slingshot, and UAN product streams. Initialize the cray administrative CLI.  In the COS recipe for 2.2, several dependencies have been removed, this includes Slingshot, DVS, and Lustre. Those packages are now installed during CFS Image Customization. More information on this change is covered in the Create UAN Boot Images procedure.\n Build the UAN image using IMS.\na. Identify the COS image recipe to base the UAN image on.\n ```bash ncn-m001# cray ims recipes list --format json | jq '.[] | select(.name | contains(\u0026quot;compute\u0026quot;))' { \u0026quot;created\u0026quot;: \u0026quot;2021-02-17T15:19:48.549383+00:00\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;4a5d1178-80ad-4151-af1b-bbe1480958d1\u0026quot;, \u0026lt;\u0026lt;-- Note this ID \u0026quot;link\u0026quot;: { \u0026quot;etag\u0026quot;: \u0026quot;3c3b292364f7739da966c9cdae096964\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;s3://ims/recipes/4a5d1178-80ad-4151-af1b-bbe1480958d1/recipe.tar.gz\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;s3\u0026quot; }, \u0026quot;linux_distribution\u0026quot;: \u0026quot;sles15\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;cray-shasta-compute-sles15sp3.x86_64-2.2.27\u0026quot;, \u0026quot;recipe_type\u0026quot;: \u0026quot;kiwi-ng\u0026quot; } ```  b. Save the id of the IMS recipe in an environment variable.\n ```bash ncn-m001# export IMS_RECIPE_ID=4a5d1178-80ad-4151-af1b-bbe1480958d1 ```  c. Use the saved IMS recipe id in the procedure \u0026ldquo;Build an Image Using IMS REST Service\u0026rdquo; in the CSM documentation to build the UAN image.\n  Perform Create UAN Boot Images to run CFS Image Customization on the resulting image.\n  "
},
{
	"uri": "/docs-uan/en-230/operations/configure_interfaces_on_uans/",
	"title": "Configure Interfaces On UANs",
	"tags": [],
	"description": "",
	"content": "Configure Interfaces on UANs Perform this procedure to set network interfaces on UANs by editing a configuration file.\nThe Customer Access Network (CAN) is no longer setup by default as the networking connection to the site user access network. The default is now for sites to directly connect their user network to the User Access Node (UAN) or Application nodes, and to define that network configuration in the Configuration Framework Service (CFS) host_vars/XNAME/customer_net.yml file.\nAdmins must create the host_vars/XNAME/customer_net.yml file and use the variables described in this procedure to define the interfaces and routes.\nIf the HPE Cray EX CAN is required, set uan_can_setup : yes in host_vars/XNAME/customer_net.yml for each node that will use the CAN, or in group_vars/all/customer_net.yml to enable the HPE Cray EX CAN on all UANs and Application nodes.\n  Obtain the password for the crayvcs user.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\  --template={{.data.vcs_password}} | base64 --decode   Log in to ncn-w001.\n  Create a copy of the Git configuration. Enter the credentials for the crayvcs user when prompted.\nncn-w001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git   Change to the uan-config-management directory.\nncn-w001# cd uan-config-management   Edit the host_vars/XNAME/customer_net.yml file and configure the values as needed.\nTo set up CAN:\n## uan_can_setup # Set uan_can_setup to \u0026#39;yes\u0026#39; if the site will # use the Shasta CAN network for user access to # UAN/Application nodes. uan_can_setup: no To define interfaces:\n## Customer defined networks ifcfg-X # customer_uan_interfaces is as list of interface names used for constructing # ifcfg-\u0026lt;customer_uan_interfaces.name\u0026gt; files. The setting dictionary is where # any desired ifcfg fields are defined. The field name will be converted to  # uppercase in the generated ifcfg-\u0026lt;name\u0026gt; file. # # NOTE: Interfaces should be defined in order of dependency. # ## Example ifcfg fields, not exhaustive: # bootproto: \u0026#39;\u0026#39; # device: \u0026#39;\u0026#39; # dhcp_hostname: \u0026#39;\u0026#39; # ethtool_opts: \u0026#39;\u0026#39; # gateway: \u0026#39;\u0026#39; # hwaddr: \u0026#39;\u0026#39; # ipaddr: \u0026#39;\u0026#39; # master: \u0026#39;\u0026#39; # mtu: \u0026#39;\u0026#39; # peerdns: \u0026#39;\u0026#39; # prefixlen: \u0026#39;\u0026#39; # slave: \u0026#39;\u0026#39; # srcaddr: \u0026#39;\u0026#39; # startmode: \u0026#39;\u0026#39; # userctl: \u0026#39;\u0026#39; # bonding_master: \u0026#39;\u0026#39; # bonding_module_opts: \u0026#39;\u0026#39; # bonding_slave0: \u0026#39;\u0026#39; # bonding_slave1: \u0026#39;\u0026#39; #  # customer_uan_interfaces: # - name: \u0026#34;net1\u0026#34; # settings: # bootproto: \u0026#34;static\u0026#34; # device: \u0026#34;net1\u0026#34; # ipaddr: \u0026#34;1.2.3.4\u0026#34; # startmode: \u0026#34;auto\u0026#34; # - name: \u0026#34;net2\u0026#34; # settings: # bootproto: \u0026#34;static\u0026#34; # device: \u0026#34;net2\u0026#34; # ipaddr: \u0026#34;5.6.7.8\u0026#34; # startmode: \u0026#34;auto\u0026#34; customer_uan_interfaces: [] To define interface static routes:\n## Customer defined networks ifroute-X # customer_uan_routes is as list of interface routes used for constructing # ifroute-\u0026lt;customer_uan_routes.name\u0026gt; files.  #  # customer_uan_routes: # - name: \u0026#34;net1\u0026#34; # routes: # - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; # - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; # - name: \u0026#34;net2\u0026#34; # routes: # - \u0026#34;default 10.103.8.20 255.255.255.255 - table 3\u0026#34; # - \u0026#34;10.103.8.128/25 10.103.8.20 255.255.255.255 net2\u0026#34; customer_uan_routes: [] To define the rules:\n## Customer defined networks ifrule-X # customer_uan_rules is as list of interface rules used for constructing # ifrule-\u0026lt;customer_uan_routes.name\u0026gt; files.  #  # customer_uan_rules: # - name: \u0026#34;net1\u0026#34; # rules: # - \u0026#34;from 10.1.0.0/16 lookup 1\u0026#34; # - name: \u0026#34;net2\u0026#34; # rules: # - \u0026#34;from 10.103.8.0/24 lookup 3\u0026#34; customer_uan_rules: [] To define the global static routes:\n## Customer defined networks global routes # customer_uan_global_routes is as list of global routes used for constructing # the \u0026#34;routes\u0026#34; file.  #  # customer_uan_global_routes: # - routes:  # - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; # - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; customer_uan_global_routes: []   Add the change from the working directory to the staging area.\nncn-w001# git add -A   Commit the file to the master branch.\nncn-w001# git commit -am \u0026#39;Added UAN interfaces\u0026#39;   Push the commit.\nncn-w001# git push   Reboot the UAN with the Boot Orchestration Service (BOS).\nThe new interfaces will be available when the UAN is rebooted. Replace the UAN_SESSION_TEMPLATE value with the BOS session template name for the UANs.\nncn-w001# cray bos v1 session create \\  --template-uuid UAN_SESSION_TEMPLATE --operation reboot   Verify that the desired networking configuration is correct on each UAN.\n  "
},
{
	"uri": "/docs-uan/en-230/installation_prereqs/configure_the_bios_of_an_hpe_uan/",
	"title": "Configure The Bios Of An HPE UAN",
	"tags": [],
	"description": "",
	"content": "Configure the BIOS of an HPE UAN Perform this procedure to configure the network interface and boot settings required by HPE UANs.\nBefore the UAN product can be installed on HPE UANs, specific network interface and boot settings must be configured in the BIOS.\nPerform Configure the BMC for UANs with iLO before performing this procedure.\n  Force a UAN to reboot into the BIOS.\nIn the following command, UAN_BMC_XNAME is the xname of the BMC of the UAN to configure. Replace USER and PASSWORD with the BMC username and password, respectively.\nncn-m001# ipmitool -U USER -P PASSWORD -H UAN_BMC_XNAME -I lanplus \\ chassis bootdev pxe options=efiboot,persistent   Monitor the console of the UAN using either ConMan or the following command:\nncn-m001# ipmitool -U USER -P PASSWORD -H UAN_BMC_XNAME -I \\ lanplus sol activate Refer to the section \u0026ldquo;About the ConMan Containerized Service\u0026rdquo; in the CSM documentation for more information about ConMan.\n  Press the ESC and 9 keys to access the BIOS System Utilities when the option appears.\n  Ensure that OCP Slot 1 Port 1 is the only port with Boot Mode set to Network Boot (PXE). All other ports must have Boot Mode set to Disabled.\nThe settings must match the following example.\n-------------------- System Configuration BIOS Platform Configuration (RBSU) \u0026gt; Network Options \u0026gt; Network Boot Options \u0026gt; PCIe Slot Network Boot Slot 1 Port 1 : Marvell FastLinQ 41000 Series - [Disabled] 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC Slot 1 Port 2 : Marvell FastLinQ 41000 Series - [Disabled] 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC Slot 2 Port 1 : Network Controller [Disabled] OCP Slot 10 Port 1 : Marvell FastLinQ 41000 [Network Boot] Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC OCP Slot 10 Port 2 : Marvell FastLinQ 41000 [Disabled] Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC --------------------   Set the Link Speed to SmartAN for all ports.\n-------------------- System Utilities System Configuration \u0026gt; Main Configuration Page \u0026gt; Port Level Configuration Link Speed [SmartAN] FEC Mode [None] Boot Mode [PXE] DCBX Protocol [Dynamic] RoCE Priority [0] PXE VLAN Mode [Disabled] Link Up Delay [30] Wake On LAN Mode [Enabled] RDMA Protocol Support [iWARP + RoCE] BAR-2 Size [8M] VF BAR-2 Size [256K] ---------------------   Set the boot options to match the following example.\n---------------------- System Utilities System Configuration \u0026gt; BIOS/Platform Configuration (RBSU) \u0026gt; Boot Options Boot Mode [UEFI Mode] UEFI Optimized Boot [Enabled] Boot Order Policy [Retry Boot Order Indefinitely] UEFI Boot Settings Legacy BIOS Boot Order -----------------------   Set the UEFI Boot Order settings to match the following example.\nThe order must be:\n USB Local disks OCP Slot 10 Port 1 IPv4 OCP Slot 10 Port 1 IPv6  ----------------------- System Utilities System Configuration \u0026gt; BIOS/Platform Configuration (RBSU) \u0026gt; Boot Options \u0026gt; UEFI Boot Settings \u0026gt; UEFI Boot Order Press the \u0026#39;+\u0026#39; key to move an entry higher in the boot list and the \u0026#39;-\u0026#39; key to move an entry lower in the boot list. Use the arrow keys to navigate through the Boot Order list. Generic USB Boot SATA Drive Box 1 Bay 1 : VK000480GWTHA SATA Drive Box 1 Bay 2 : VK000480GWTHA SATA Drive Box 1 Bay 3 : VK001920GWTTC SATA Drive Box 1 Bay 4 : VK001920GWTTC OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv4) OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv6) -------------------------   Ensure that the time is set correctly.\nIf the time is not set correctly, PXE booting issues may occur.\n----------------------- System Utilities System Information \u0026gt; Summary System Name HPE ProLiant DL385 Gen10 Plus ...lines omitted... Date and Time 2021-08-09T12:59:45-34:07 -------------------------   "
},
{
	"uri": "/docs-uan/en-230/installation_prereqs/configure_the_bmc_for_uans_with_ilo/",
	"title": "Configure The BMC For UANs With Ilo",
	"tags": [],
	"description": "",
	"content": "Configure the BMC for UANs with iLO Perform this procedure to enable the IPMI/DCMI settings on an HPE UAN that are necessary to continue UAN product installation on an HPE Cray EX supercomputer.\nPerform the first three steps of Prepare for UAN Product Installation before performing this procedure.\n  Create the SSH tunnel necessary to access the BMC web GUI interface.\na. Find the IP or hostname for a UAN.\nb. Create an SSH tunnel to the UAN BMC.\n In the following example, `uan01-mgmt` is the UAN and `shasta-ncn-m001` is the NCN the admin is logged into. ```bash # ssh -L 8443:uan01-mgmt:443 shasta-ncn-m001 ```  c. Wait for SSH to establish the connection.\n  Open https://127.0.0.1:8443 in web browser on the NCN to access the BMC web GUI.\n  Log in to the web GUI using default credentials.\n  Click Security in the menu on the left side of the screen.\n  Click Access Settings in the menu at the top of the screen.\n  Click the pencil icon next to Network in the main window area.\n  Check the box next to IPMI/DCMI over LAN.\n  Ensure that the remote management settings match the following screenshot.\n  "
},
{
	"uri": "/docs-uan/en-230/installation_prereqs/prepare_for_uan_product_installation/",
	"title": "Prepare For UAN Product Installation",
	"tags": [],
	"description": "",
	"content": "Prepare for UAN Product Installation Perform this procedure to ready the HPE Cray EX supercomputer for UAN product installation.\nInstall and configure the COS product before performing this procedure.\nMANAGEMENT NETWORK SWITCH CONFIGURATION\n  Ensure that the management network switches are properly configured.\n  Ensure that the management network switches have the proper firmware.\nRefer to the procedure \u0026ldquo;Update the Management Network Firmware\u0026rdquo; in the HPE Cray EX hardware documentation.\n  Ensure that the host reservations for the UAN CAN network have been properly set.\nRefer to the procedure \u0026ldquo;Add UAN CAN IP Addresses to SLS\u0026rdquo; in the HPE Cray EX hardware documentation.\n  BMC CONFIGURATION\n Configure the BMC of the UAN.\n Perform Configure the BMC for UANs with iLO if the UAN is a HPE server with an iLO.    BIOS CONFIGURATION\n Configure the BIOS of the UAN.\n Perform Configure the BIOS of an HPE UAN if the UAN is a HPE server with an iLO. Perform Configure the BIOS of a Gigabyte UAN if the UAN is a Gigabyte server.    VERIFY UAN BMC FIRMWARE VERSION\n Verify that the firmware for each UAN BMC meets the specifications.\nUse the System Admin Toolkit firmware command to check the current firmware version on a UAN node.\nncn-m001# sat firmware -x BMC_XNAME   Repeat the previous six Steps for all UANs.\n  VERIFY REQUIRED SOFTWARE FOR UAN INSTALLATION\n Unpackage the file.\nncn-m001# tar zxf uan-PRODUCT_VERSION.tar.gz   Navigate into the uan-PRODUCT_VERSION/ directory.\nncn-m001# cd uan-PRODUCT_VERSION/   Run the pre-install goss tests to determine if the system is ready for the UAN product installation.\nThis requires that goss is installed on the node running the tests.\nncn# ./validate-pre-install.sh ............... Total Duration: 1.304s Count: 15, Failed: 0, Skipped: 0   Ensure that the cray-console-node pods are connected to UANs so that they are monitored and their consoles are logged.\na. Obtain a list of the xnames for all UANs (remove the --subrole argument to list all Application nodes).\nncn# cray hsm state components list --role Application --subrole UAN --format json | jq -r .Components[].ID | sort x3000c0s19b0n0 x3000c0s24b0n0 x3000c0s31b0n0 b. Obtain a list of the console pods.\nncn# PODS=$(kubectl get pods -n services -l app.kubernetes.io/name=cray-console-node --template \u0026#39;{{range .items}}{{.metadata.name}} {{end}}\u0026#39;) c. Use conman -q to scan the list of connections being monitored by conman (only UAN xnames are shown for brevity).\nncn# for pod in $PODS; do kubectl exec -n services -c cray-console-node $pod -- conman -q; done x3000c0s19b0n0 x3000c0s24b0n0 x3000c0s31b0n0 If a console connection is not present, the install may continue, but a console connection should be established before attempting to boot the UAN.\n  Next, install the UAN product by peforming the procedure Install the UAN Product Stream.\n"
},
{
	"uri": "/docs-uan/en-230/changelog/",
	"title": "Changelog",
	"tags": [],
	"description": "",
	"content": "Changelog All notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning.\n[Unreleased] [2.3.0] - 2021-12-15  CASMUSER-2926: Add loki fixes for COS 2.2 and CSM artifacts Simplify builds of CFS and pin to COS 2.2 CFS plays CASMUSER-2920: Provide uan_disable_gpg_check support CASMUSER-2917: Fix rpm uploads to nexus and set major/minor/patch at build time CASMTRIAGE-2721: Update copyrights and license headers CASMUSER-2907: Remove online install support and doc references CASM-2594: Update reference to CSM repo to use algol60 CASMUSER-2807: Add support for creating a UAN capable image with CFS only CASMUSER-2789: Remove the UAN kiwi recipe and image CASMUSER-2790: Use the COS provided boot parameters rpm CASMUSER-2832: Remove unneeded packages CASMUSER-2778: Update references from SP2 to SP3 CASMUSER-2843: Fix formatting error in uan_ldap role CASMUSER-2780: Provide ability to get sensitive data from vault for uan_ldap role CASM-2589: UAN: Limit access to management gateway by non-root users  [2.1.9] - 2021-11-28  CASMUSER-2917: Fix the SHS repo url in the kiwi recipe CASMUSER-2917: Fix rpm uploads to nexus and set major/minor/patch at build time CASMUSER-2907: Remove references to online installs  [2.1.8] - 2021-11-16  CASMUSER-2912: Pin versions correctly for CFS plays from other products Use UAN and CSM artifacts from algol60  [2.1.7] - 2021-10-28  CASMUSER-2624: Add kdump support for HPE DL UAN nodes  [2.1.6] - 2021-10-14  CASMUSER-2868: Add troubleshooting steps for CAN and SLS configuration CASMUSER-2848: Add an example vars.yml showing how to enable the CAN CASMUSER-2845: Fix goss test and script that checks UAN discovery CASMUSER-2849: Remove ifmap from the BOS session template  [2.1.5] - 2021-09-30  CASMUSER-2814: Doc updates  [2.1.4] - 2021-09-10  CASMTRIAGE-2227: Doc updates CASMTRIAGE-2129: Clarify message on checking Nexus versions CASMUSER-2804: Update content base image to pick up newer dependent roles  [2.1.3] - 2021-08-27  STP-2762: Fix formatting and links in docs  [2.1.2] - 2021-08-11  CASMTRIAGE-1878: Use the updated default CAN alias in SLS as configured by csi CASMTRIAGE-1870: Fix kdump with packages required by COS CASMTRIAGE-1875: Set swappiness value to a string CASMTRIAGE-1872: Add steps to set the BIOS EFITIME and troubleshoot x509 cert issues. CASMTRIAGE-1854: Add suggestion on the ordering of CFS layers NETETH-1541: Change Slingshot to version to 1.4  [2.1.1] - 2021-07-30  CASMUSER-2737: uan_ldap runs only at node configuration and pick up other changes including Slingshot repo 1.3  [2.1.0] - 2021-05-14  CASMUSER-2608: Update site.yml and the uan recipe for GPU support CASMUSER-2709: Fix builds to use master or stable correctly CASMUSER-2709: Fix logic to find and replace ARTIFACT_BRANCH CASMUSER-2692: Include goss and diagnostics package CASMUSER-2613: Update UAN to SLES15SP2 Added a Changelog Switched references of dtr to arti Update versions for sles15sp2 change CASMUSER-2687: Add CDST GUI packages CASMUSER-2613: Add some GPU packages that are present in the COS image CASMUSER-2711: Modify the layout of container images to work with existing helm charts CASMUSER-2711: Update release version to get the correct UAN rpms CASMUSER-2714: Update Slingshot repo to 1.1 CASMUSER-2740: Update Slingshot repo to 1.2 CASMUSER-2742: Update Slingshot repo to 1.3 CASMUSER-2744: Update cray-product-catalog-update to latest version CASMUSER-2745: Add single-request option to uan_interfaces STP-2670: Restructure docs as Markdown CASMUSER-2748: Fix recipe format for profiles and preferences to work with new kiwi-ng CASMUSER-2737: Run uan_ldap only during node configuration  "
},
{
	"uri": "/docs-uan/en-230/installation_prereqs/configure_the_bios_of_a_gigabyte_uan/",
	"title": "Configure The Bios Of A Gigabyte UAN",
	"tags": [],
	"description": "",
	"content": "Configure the BIOS of a Gigabyte UAN Perform this procedure to configure the network interface and boot settings required by Gigabyte UANs.\nBefore the UAN product can be installed on Gigabyte UANs, specific network interface and boot settings must be configured in the BIOS.\n  Press the Delete key to enter the setup utility when prompted to do so in the console.\n  Navigate to the boot menu.\n  Set the Boot Option #1 field to Network:UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection.\n  Set all other Boot Option fields to Disabled.\n  Ensure that the boot mode is set to [UEFI].\n  Confirm that the time is set correctly. If the time is not accurate, correct it now.\nIncorrect time will cause PXE booting issues.\n  Select Save \u0026amp; Exit to save the settings.\n  Select Yes to confirm and press the Enter key.\nThe UAN will reboot.\n  Optional: Run the following IPMI commands if the BIOS settings do not persist.\nIn these example commands, the BMC of the UAN is x3000c0s27b0. Replace USERNAME and PASSWORD with username and password of the BMC of the UAN. These commands do the following:\n Power off the node Perform a reset. Set the PXE boot in the options. Power on the node  ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 power off ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 mc reset cold ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 chassis bootdev pxe \\ options=efiboot,persistent ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 power on   "
},
{
	"uri": "/docs-uan/en-230/advanced/customizing_uan_images_manually/",
	"title": "Customizing UAN Images Manually",
	"tags": [],
	"description": "",
	"content": "Customizing UAN Images Manually   Query IMS for the UAN Image you want to customize.\nncn-m001:~/ $ cray ims images list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;uan\u0026#34;))\u0026#39; { \u0026#34;created\u0026#34;: \u0026#34;2021-02-18T17:17:44.168655+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;6d46d601-c41f-444d-8b49-c9a2a55d3c21\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;371b62c9f0263e4c8c70c8602ccd5158\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/6d46d601-c41f-444d-8b49-c9a2a55d3c21/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;uan-PRODUCT_VERSION-image\u0026#34; }   Create a variable for the IMS image id in the returned data.\nncn-m001:~/ $ export IMS_IMAGE_ID=6d46d601-c41f-444d-8b49-c9a2a55d3c21   Using the IMS_IMAGE_ID, follow the instructions in the Customize an Image Root Using IMS in the CSM documentation to build the UAN Image.\n  "
},
{
	"uri": "/docs-uan/en-230/install/install_the_uan_product_stream/",
	"title": "Install The UAN Product Stream",
	"tags": [],
	"description": "",
	"content": "Install the UAN Product Stream This procedure installs the User Access Nodes (UAN) product on a system so that UAN boot images can be created.\nBefore performing this procedure:\n Initialize and configure the Cray command line interface (CLI) tool. See \u0026ldquo;Configure the Cray Command Line Interface (CLI)\u0026rdquo; in the CSM documentation for more information. Perform Prepare for UAN Product Installation  Replace PRODUCT_VERSION in the example commands with the UAN product stream string (2.3.0 for example). Replace CRAY_EX_DOMAIN in the example commands with the FQDN of the HPE Cray EX.\nDOWNLOAD AND PREPARE THE UAN SOFTWARE PACKAGE\n  Start a typescript to capture the commands and output from this installation.\nncn-m001# script -af product-uan.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39;   Run the installation script:\n ```bash ncn-m001# ./install.sh ```    VERIFY THE INSTALLATION\n Verify that the UAN configuration was imported and added to the cray-product-catalog ConfigMap in the Kubernetes services namespace.\na. Run the following command and verify that the output contains an entry for the PRODUCT_VERSION that was installed in the previous steps:\n The following command may return more than one version of the UAN product if previous versions have been installed. ```bash ncn-m001# kubectl get cm cray-product-catalog -n services -o json | jq -r .data.uan PRODUCT_VERSION: configuration: clone_url: https://vcs.CRAY_EX_DOMAIN/vcs/cray/uan-config-management.git commit: 6658ea9e75f5f0f73f78941202664e9631a63726 import_branch: cray/uan/PRODUCT_VERSION import_date: 2021-07-28 03:26:00.399670 ssh_url: git@vcs.CRAY_EX_DOMAIN:cray/uan-config-management.git ```  b. Verify that the Kubernetes jobs that import the configuration content completed successfully. Skip this step if the previous substep indicates that the new UAN product version content installed successfully.\nA STATUS of `Completed` indicates that the Kubernetes jobs completed successfully. ```bash ncn-m001# kubectl get pods -n services | grep uan uan-config-import-PRODUCT_VERSION-wfh4f 0/3 Completed 0 3m15s ```    Verify that the UAN RPM repositories have been created in Nexus:\nPRODUCT_VERSION is the UAN release number and SLE_VERSION is the SLE release version, such as 15sp2 or 15sp3.\n  Query Nexus through its REST API to display the repositories prefixed with the name uan:\nncn-m001# curl -s -k https://packages.local/service/rest/v1/repositories | jq -r \u0026#39;.[] | select(.name | startswith(\u0026#34;uan\u0026#34;)) | .name\u0026#39; uan-PRODUCT_VERSION-sle-SLE_VERSION     Finish the typescript file started at the beginning of this procedure.\n# exit   Optional: Perform Merge UAN Configuration Data if a previous version of the UAN product was already installed.\n  "
},
{
	"uri": "/docs-uan/en-230/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/docs-uan/en-230/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]