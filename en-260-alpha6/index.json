[
{
	"uri": "/docs-uan/en-260-alpha6/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Cray EX User Access Nodes Installation and Administration Guide This document describes the installation prequisites, installation procedures, and operational procedures for Cray EX User Access Nodes (UAN).\nThe latest version of this documentation is available here.\nTable of Contents Topics: Installation Prereqs Prepare for UAN Product Installation Configure the BMC for UANs with iLO Configure the BIOS of an HPE UAN Configure the BIOS of a Gigabyte UAN Install Install or Upgrade UAN Operations About UAN Configuration Build a New UAN Image Using a COS Recipe Create UAN Boot Images Mount a New File System on a UAN Configure Interfaces on UANs Configure Pluggable Authentication Modules (PAM) on UANs UAN Ansible Roles\rBoot UANs Mitigation for CVE-2023-0461 Advanced Customizing UAN Images Manually Enabling the Customer Access Network (CAN) or the Customer High Speed Network (CHN) Repurposing a Compute Node as a UAN Booting an Application Node with a SLES Image (Technical Preview) Configuring a UAN for K3s (Technical Preview) Upgrade Notable Changes Troubleshooting Troubleshoot UAN Boot Issues Troubleshoot UAN CFS and Network Configuration Issues Troubleshoot UAN Disk Configuration Issues Test Plan Test Plan for User Access Node (UAN) and Repurposed Compute Node as UAN "
},
{
	"uri": "/docs-uan/en-260-alpha6/test-plan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "test-plan Topics: Test Plan for User Access Node (UAN) and Repurposed Compute Node as UAN "
},
{
	"uri": "/docs-uan/en-260-alpha6/upgrade/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=upgrade; given_title=upgrade; menuTitle: \u0026ldquo;upgrade\u0026rdquo; date: Sun Mar 26 02:01:01 UTC 2023 draft: false weight: 60 upgrade Topics: Notable Changes "
},
{
	"uri": "/docs-uan/en-260-alpha6/upgrade/notable_changes/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Notable Changes The following guide describes changes included in a particular UAN version that may be of note during an install or upgrade.\nWhen an upgrade is being performed, please review the notable changes for all of the UAN versions up to the version being installed. If a particular version does not appear in this guide, it may have only had minor changes. For a full account of the changes involved in a release, consult the ChangeLog.md file at the root of the UAN product repository.\nUAN 2.2 UAN 2.2 was an internal release and was not made generally available. UAN 2.3.1 UAN 2.3 no longer ships a default recipe or image. To build a UAN image, administrators should select a COS recipe to build as the base of their UAN and Application Nodes. The role uan_packages will now install the rpms needed by UAN and Application Nodes The role uan_packages supports GPG checking when the CSM version is 1.2 or greater. uan_disable_gpg_check: yes must be set if CSM is earlier than 1.2 uan_disable_gpg_check: no should be set if CSM is 1.2 or greater UAN 2.4.0 UAN 2.4.0 adds support for a Bifurcated Customer Access Network (BiCAN) and the ability to specify a default route other than the CAN or CHN when they are selected. Application nodes may now choose to implement user access over either the existing Customer Access Network (CAN), the new Customer High Speed Network (CHN), or a direct connection to the customers user network. By default, a direct connection is selected as it was in previous releases. uan_can_setup, when set to yes, selects the customer access network implementation based on the setting of the BICAN System Default Route in SLS. Application nodes may now set a default route other than the CAN or CHN default route when uan_can_setup: yes. uan_customer_default_route: true will allow a customer defined default route to be set using the customer_uan_routes structure when uan_can_setup is set to yes. sat bootprep is now used in the documentation to streamline the IMS, CFS, and BOS commands to create and customize images and creating sessiontemplates. UAN 2.4.1 The UAN CFS playbook now supports a section for Compute nodes. The Compute section will run the role uan_interfaces to provide Customer High Speeed Network (CHN) routing. CHN on the Compute nodes requires: Customer High Speed Network has been enabled in CSM. See \u0026ldquo;Enabling Customer High Speed Network Routing\u0026rdquo; in the CSM Documentation UAN CFS configurd with uan_can_setup: yes Fully configured HSN SLS has IP assignments for compute nodes on hsn0 Updates to GPU roles to match COS 2.3 UAN 2.4.2 There is a known issue with the version of GPU support included in the UAN CFS repo. The result is that both AMD and Nvidia SDKs are not able to be projected at the same time. Until this is resolved in a later release, modify the site.yml in the UAN CFS repo to only include either amd or nvidia. UAN 2.4.3 A new CFS role, uan_hardening adds iptables rules that will block SSH traffic to NCNs. See the README.md in the uan_hardening role for more information. UAN 2.5.3 A technical preview of a standard SLES image for UAN/Application nodes is included. Support SLES15SP4 COS based images UAN 2.6.0 UAN CFS configurations now require a CSM and two COS layers. Roles that were duplicated from COS CFS in the UAN CFS repo have been removed. Values for COS CFS roles that were previously set in the UAN CFS group_vars directory should now be set in COS CFS group_vars UAN CFS has been restructured to work for COS and Standard SLES images uan_packages variables are now vars/uan_packages.yml and vars/uan_repos.yml and have been renamed. Admins will need to migrate to the new settings. The NMN connection now supports bonding (optional). The default is a non-bonded single interface. "
},
{
	"uri": "/docs-uan/en-260-alpha6/upgrade/upgrades/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Upgrades Performing an upgrade of UAN from one version to the next follows the same general process as a fresh install. Some considerations may need to be made when merging the existing CFS configuration with the latest CFS configuration provided by the release.\nThe overall workflow for completing a UAN upgrade involves:\nPerform the UAN Installation\nReview any Notable Changes\nMerge UAN CFS Configuration Data\nCreate UAN images and reboot\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/test-plan/test-plan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Test Plan for User Access Node (UAN) and Repurposed Compute Node as UAN The following is the test plan for the User Access Node (UAN) or a Compute Node that has been repurposed to function as a UAN. The tests are grouped in three categories:\nUnit tests Integration tests Functional tests Unit Tests When possible, unit tests are run when the various components of UAN are built (uan-rpms, uan, and uan-product-stream). The uan repository contains the ansible code used when UANs boot and configure and much of the testing for that compoment must occur on a fully configured system. Future enhancements are underway to test the various compoments of UAN on a Virtual Shasta enviroment in GCP.\nSummary Description Automated Notes uan builds Verify the uan repo is able to generate artifacts yes Run make in a local checkout uan-rpms builds Verify the uan-rpms repo is able to generate rpms yes Run make in a local checkout uan-product-stream builds Verify the uan-product-stream repo is able to generate a release yes Tag a new release in uan-product-stream or run make in a local checkout Integration Tests The following integration tests verify that the UAN software interacts correctly with the rest of the products. The result of the integration tests will be a fully built and customized UAN image that boots and configures correctly. Depending on the configuration of the test system, extra integrations tests may be performed (HSN booting, WLM configuration, GPU configuration, etc).\nSummary Description Automated Notes Install the UAN product Run the install procedure Yes Run install.sh from Install_the_UAN_Product_Stream.md Build a COS recipe for UAN Verify the COS recipe can be built as the base for a UAN image no Build_a_New_UAN_Image_Using_the_COS_Recipe.md Run UAN CFS Verify the CFS layers run correctly (SHS, UAN, WLM, CPE, etc) no Create_UAN_Boot_Images.md Boot UANs Verify the UAN boots successfully no Boot_UANS.md Enable HSN booting Verify the UAN is able to HSN boot no Consult COS documentation Enable GPU Verify the UAN is able to configure GPUs no Consult GPU documentation Functional Tests With a fully configured Shasta system, the following functional tests determines that a UAN is able to perform its intended capabilities. These tests apply to both native UANs and Compute Nodes which have been repurposed as UANs.\nSummary Description Automated Notes User Authentication Verify a user is able to ssh to the UAN using LDAP authentication. no ssh user@uan Job launch Verify a user is able to submit a basic job. no srun hostname Verify CPE Verify the Cray Programming Environment is available no module list Verify GPU functionality Run the test suite if GPUs are configured yes /opt/cray/uan/tests/validate-gpu.sh \u0026lt;nvidia|amd\u0026gt; Verify CAN or CHN Configuration Inspect the network interfaces and default routes used for CAN or CHN no For systems running CAN, there must be a can0 interface present and the default route should be over that device.For systems running CHN (including Compute Nodes repurposed as UANs), the hsn0 interface must have a CHN IP in addition to the HSN IP and the default route should be over the hsn0 device. "
},
{
	"uri": "/docs-uan/en-260-alpha6/operations/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=operations; given_title=operations; menuTitle: \u0026ldquo;operations\u0026rdquo; date: Sun Mar 26 02:01:00 UTC 2023 draft: false weight: 40 operations Topics: About UAN Configuration Build a New UAN Image Using a COS Recipe Create UAN Boot Images Mount a New File System on a UAN Configure Interfaces on UANs Configure Pluggable Authentication Modules (PAM) on UANs UAN Ansible Roles\rBoot UANs Mitigation for CVE-2023-0461 "
},
{
	"uri": "/docs-uan/en-260-alpha6/operations/uan_ansible_roles/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "UAN Ansible Roles csm.gpg_keys The csm.gpg_keys role fetches the required GPG keys needed for HPE RPM verification.\nuan_ca_cert The uan_ca_cert role installs a CA certificate on the system.\nuan_disk_config The uan_disk_config role configures swap and scratch disk partitions on UAN nodes.\nRequirements There must be disk devices found on the UAN node by the device_filter module or this role will exit with failure. This condition can be ignored by setting uan_require_disk to false. See variable definitions below.\nSee the library/device_filter.py file for more information on this module.\nThe device that is found will be unmounted if mounted and a swap partition will be created on the first half of the disk, and a scratch partition on the second half. ext4 filesystems are created on each partition.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_require_disk Boolean to determine if this role continues to setup disk if no disks were found by the device filter. Set to true to exit with error when no disks are found.\nExample:\nuan_require_disk: false uan_device_name_filter Regular expression of disk device name for this role to filter. Input to the device_filter module.\nExample:\nuan_device_name_filter: \u0026#34;^sd[a-f]$\u0026#34; uan_device_host_filter Regular expression of host for this role to filter. Input to the device_filter module.\nExample:\nuan_device_host_filter: \u0026#34; uan_device_model_filter Regular expression of device model for this role to filter. Input to the device_filter module.\nExample:\nuan_device_model_filter: \u0026#34; uan_device_vendor_filter Regular expression of disk vendor for this role to filter. Input to the device_filter module.\nExample:\nuan_device_vendor_filter: \u0026#34; uan_device_size_filter Regular expression of disk size for this role to filter. Input to the device_filter module.\nExample:\nuan_device_size_filter: \u0026#34;\u0026lt;1TB\u0026#34; uan_swap Filesystem location to mount the swap partition.\nExample:\nuan_swap: \u0026#34;/swap\u0026#34; uan_scratch Filesystem location to mount the scratch partition.\nExample:\nuan_scratch: \u0026#34;/scratch\u0026#34; swap_file Name of the swapfile to create. Full path is \u0026lt;uan_swap\u0026gt;/\u0026lt;swapfile\u0026gt;.\nExample:\nswap_file: \u0026#34;swapfile\u0026#34; swap_dd_command dd command to create the swapfile.\nExample:\nswap_dd_command: \u0026#34;/usr/bin/dd if=/dev/zero of={{ uan_swap }}/{{ swap_file }} bs=1GB count=10\u0026#34; swap_swappiness Value to set the swapiness in sysctl.\nExample:\nswap_swappiness: \u0026#34;10\u0026#34; Dependencies library/device_filter.py is required to find eligible disk devices.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_disk_config } This role is included in the UAN site.yml play.\nuan_hardening The uan_hardening role configures site/customer-defined network security of UANs, for example preventing ssh out of UAN over NMN to NCN nodes.\nRequirements None.\nRole Variables Available variables are listed below, along with default values (see\ndisable_ssh_out_nmn_to_management_ncns\nBoolean variable controlling whether or not firewall rules are applied at the UAN to prevent ssh outbound over the NMN to the NCN management nodes.\nThe default value of disable_ssh_out_nmn_to_management_ncns is yes.\ndisable_ssh_out_nmn_to_management_ncns: yes disable_ssh_out_uan_to_nmn_lb\nBoolean variable controlling whether or not firewall rules are applied at the UAN to prevent ssh outbound over the NMN to NMN LB IPs.\nThe default value of disable_ssh_out_uan_to_nmn_lb is yes.\ndisable_ssh_out_uan_to_nmn_lb: yes Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_hardening} This role is included in the UAN site.yml play.\nuan_interfaces The uan_interfaces role configures site/customer-defined network interfaces and Shasta Customer Access Network (CAN) network interfaces on UAN nodes.\nRequirements None.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_nmn_bond\nuan_nmn_bond is a boolean variable controlling the configuration of the Node Management Network (NMN). When true, the NMN network connection will be configured as a bonded pair of interfaces defined by the members of the uan_nmn_bond_slaves variable. The bonded NMN interface is named nmnb0. When false, the NMN network connection will be configured as a single interface named nmn0.\nThe default value of uan_nmn_bond is no.\nuan_nmn_bond: no uan_nmn_bond_slaves\nuan_nmn_bond_slaves is a list of the interfaces to use as the bond slave pair when uan_nmn_bond is true.\nThe interface names must be in a format which doesn\u0026rsquo;t change between reboots of the node, such as ens10f0 which is the first port of the NIC in slot 10.\nNOTE: ens10f0 is typically the first port of the OCP 25Gb card that the node PXE boots.\nIMPORTANT! The first interface in the list must be the nmn0 interface which is configured during the initial image boot, typically ens10f0. This is required because the MAC address of the nmn0 interface is the MAC associated with the IP address of the UAN. The bonded nmnb0 interface and the bond slaves will assume this MAC and the IP address of nmn0 to preserve connectivity.\nThe second interface is typically the first port of a different 25Gb NIC for resiliency.\nThe default values of uan_nmn_bond_slaves are shown here. They may need to be changed to match the actual node cabling and NIC configuration.\nuan_nmn_bond_slaves: - \u0026#34;ens10f0\u0026#34; - \u0026#34;ens1f0\u0026#34; uan_can_setup\nBoolean variable controlling the configuration of user access to UAN nodes. When true, user access is configured over either the Customer Access Network (CAN) or Customer High Speed Network (CHN), depending on which is configured on the system.\nWhen uan_can_setup is false, user access over the CAN or CHN is not configured on the UAN nodes and no default route is configured. The Admin must then specify the default route in customer_uan_routes.\nThe default value of uan_can_setup is no.\nuan_can_setup: no uan_can_bond_slaves\nuan_can_bond_slaves is a list of the interfaces to use as the bond slave pair when uan_can_setup is true, uan_nmn_bond is true, and the Customer Access Network (CAN) is configured on the system. This variable is ignored if uan_nmn_bond is false.\nThe interface names must be in a format which doesn\u0026rsquo;t change between reboots of the node, such as ens10f1 which is the second port of the NIC in slot 10.\nNOTE: ens10f1 is typically the second port of the OCP 25Gb card and is used as one of the bond slaves in the CAN bond0 interface.\nThe second interface is typically the second port of a different 25Gb NIC for resiliency.\nThe default values of uan_can_bond_slaves are shown here. They may need to be changed to match the actual node cabling and NIC configuration.\nuan_can_bond_slaves: - \u0026#34;ens10f1\u0026#34; - \u0026#34;ens1f1\u0026#34; uan_customer_default_route\nBoolean variable that allows the default route to be set by the customer_uan_routes data when uan_can_setup is true.\nBy default, no default route is setup unless uan_can_setup is true, which sets the default route to the CAN or CHN.\nuan_customer_default_route: no sls_nmn_name\nNode Management Network name used by SLS. This value should not be changed.\nsls_nmn_name: \u0026#34;NMN\u0026#34; sls_nmn_svcs_name\nNode Management Services Network name used by SLS. This value should not be changed.\nsls_nmn_svcs_name: \u0026#34;NMNLB\u0026#34; sls_mnmn_svcs_name\nMountain Node Management Services Network name used by SLS. This value should not be changed.\nsls_mnmn_svcs_name: \u0026#34;NMN_MTN\u0026#34; uan_required_dns_options\nList of DNS options. By default, single-request is set and must not be removed.\nuan_required_dns_options: - \u0026#39;single-request\u0026#39; customer_uan_interfaces\nList of interface names used for constructing ifcfg-\u0026lt;customer_uan_interfaces.name\u0026gt; files. Define ifcfg fields for each interface here. Field names are converted to uppercase in the generated ifcfg-\u0026lt;name\u0026gt; file(s).\nInterfaces should be defined in order of dependency.\ncustomer_uan_interfaces: [] # Example: customer_uan_interfaces: - name: \u0026#34;net1\u0026#34; settings: bootproto: \u0026#34;static\u0026#34; device: \u0026#34;net1\u0026#34; ipaddr: \u0026#34;1.2.3.4\u0026#34; startmode: \u0026#34;auto\u0026#34; - name: \u0026#34;net2\u0026#34; settings: bootproto: \u0026#34;static\u0026#34; device: \u0026#34;net2\u0026#34; ipaddr: \u0026#34;5.6.7.8\u0026#34; startmode: \u0026#34;auto\u0026#34; `customer_uan_routes\nList of interface routes used for constructing ifroute-\u0026lt;customer_uan_routes.name\u0026gt; files.\ncustomer_uan_routes: [] # Example customer_uan_routes: - name: \u0026#34;net1\u0026#34; routes: - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; - name: \u0026#34;net2\u0026#34; routes: - \u0026#34;default 10.103.8.20 255.255.255.255 - table 3\u0026#34; - \u0026#34;10.103.8.128/25 10.103.8.20 255.255.255.255 net2\u0026#34; customer_uan_rules\nList of interface rules used for constructing ifrule-\u0026lt;customer_uan_routes.name\u0026gt; files.\ncustomer_uan_rules: [] # Example customer_uan_rules: - name: \u0026#34;net1\u0026#34; rules: - \u0026#34;from 10.1.0.0/16 lookup 1\u0026#34; - name: \u0026#34;net2\u0026#34; rules: - \u0026#34;from 10.103.8.0/24 lookup 3\u0026#34; customer_uan_global_routes\nList of global routes used for constructing the \u0026ldquo;routes\u0026rdquo; file.\ncustomer_uan_global_routes: [] # Example customer_uan_global_routes: - routes: - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; external_dns_searchlist\nList of customer-configurable fields to be added to the /etc/resolv.conf DNS search list.\nexternal_dns_searchlist: [ \u0026#39;\u0026#39; ] # Example external_dns_searchlist: - \u0026#39;my.domain.com\u0026#39; - \u0026#39;my.other.domain.com\u0026#39; external_dns_servers\nList of customer-configurable fields to be added to the /etc/resolv.conf DNS server list.\nexternal_dns_servers: [ \u0026#39;\u0026#39; ] # Example external_dns_servers: - \u0026#39;1.2.3.4\u0026#39; - \u0026#39;5.6.7.8\u0026#39; external_dns_options\nList of customer-configurable fields to be added to the /etc/resolv.conf DNS options list.\nexternal_dns_options: [ \u0026#39;\u0026#39; ] # Example external_dns_options: - \u0026#39;single-request\u0026#39; uan_access_control\nBoolean variable to control whether non-root access control is enabled. Default is no.\nuan_access_control: no api_gateways\nList of API gateway DNS names to block non-user access\napi_gateways: - \u0026#34;api-gw-service\u0026#34; - \u0026#34;api-gw-service.local\u0026#34; - \u0026#34;api-gw-service-nmn.local\u0026#34; - \u0026#34;kubeapi-vip\u0026#34; api_gw_ports\nList of gateway ports to protect.\napi_gw_ports: \u0026#34;80,443,8081,8888\u0026#34; sls_url\nThe SLS URL.\nsls_url: \u0026#34;http://cray-sls\u0026#34; Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_interfaces } This role is included in the UAN site.yml play.\nuan_ldap The uan_ldap role configures LDAP and AD groups on UAN nodes.\nRequirements NSCD, pam-config, sssd.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_ldap_setup A boolean variable to selectively skip the setup of LDAP on nodes it would otherwise be configured due to uan_ldap_config being defined. The default setting is to setup LDAP when uan_ldap_config is not empty.\nExample:\nuan_ldap_setup: yes uan_ldap_config Configures LDAP domains and servers. If this list is empty, no LDAP configuration will be applied to the UAN targets and all role tasks will be skipped.\nExample:\nuan_ldap_config: - domain: \u0026#34;mydomain-ldaps\u0026#34; search_base: \u0026#34;dc=...,dc=...\u0026#34; servers: [\u0026#34;ldaps://123.123.123.1\u0026#34;, \u0026#34;ldaps://213.312.123.132\u0026#34;] chpass_uri: [\u0026#34;ldaps://123.123.123.1\u0026#34;] - domain: \u0026#34;mydomain-ldap\u0026#34; search_base: \u0026#34;dc=...,dc=...\u0026#34; servers: [\u0026#34;ldap://123.123.123.1\u0026#34;, \u0026#34;ldap://213.312.123.132\u0026#34;] uan_ad_groups Configures active directory groups on UAN nodes.\nExample:\nuan_ad_groups: - { name: admin_grp, origin: ALL } - { name: dev_users, origin: ALL } uan_pam_modules Configures PAM modules on the UAN nodes in /etc/pam.d.\nExample:\nuan_pam_modules: - name: \u0026#34;common-account\u0026#34; lines: - \u0026#34;account required\\tpam_access.so\u0026#34; Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_ldap } This role is included in the UAN site.yml play.\nuan_motd The uan_motd role appends text to the /etc/motd file.\nRequirements None.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_motd_content: [] uan_motd_content Contains text to be added to the end of the /etc/motd file.\nDependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_motd, uan_motd_content: \u0026#34;MOTD CONTENT\u0026#34; } This role is included in the UAN site.yml play.\nuan_packages The uan_packages role adds or removes additional repositories and RPMs on UANs using the Ansible zypper_repository and zypper module.\nRepositories and packages added to this role will be installed or removed during image customization. Installing RPMs during post-boot node configuration can cause high system loads on large systems so these tasks runs only during image customizations.\nThis role will only run on SLES-based nodes.\nRequirements Zypper must be installed.\nThe csm.gpg_keys Ansible role must be installed if uan_disable_gpg_check is false.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nThis role uses the zypper_repository module. The name, description, repo, disable_gpg_check, and priority fields are supported.\nThis role uses the zypper modules. The name and disable_gpg_check fields are supported.\nuan_disable_gpg_check\nSets the disable_gpg_check field on zypper repos and packages listed in the uan_sles15_repositories add and uan_sles15_packages_add lists. The disable_gpg_check field can be overridden for each repo or package.\nuan_sles15_repositories_add\nList of repositories to add.\nuan_sles15_packages_add\nList of RPM packages to add.\nDependencies None.\nExample Playbook - hosts: Application_UAN roles: - role: uan_packages vars: uan_sles15_packages_add: - name: \u0026#34;foo\u0026#34; disable_gpg_check: yes - name: \u0026#34;bar\u0026#34; uan_sles15_packages_remove: - baz uan_sles15_repositories_add: - name: \u0026#34;uan-2.5.0-sle-15sp4\u0026#34; description: \u0026#34;UAN SUSE Linux Enterprise 15 SP4 Packages\u0026#34; repo: \u0026#34;https://packages.local/repository/uan-2.5.0-sle-15sp4\u0026#34; disable_gpg_check: no priority: 2 This role is included in the UAN site.yml play.\nuan_shadow The uan_shadow role configures the root password on UAN nodes.\nRequirements The root password hash has to be installed in HashiCorp Vault at secret/uan root_password.\nRole Variables Available variables are listed below, along with default values (see defaults/main.yml):\nuan_vault_url The URL for the HashiCorp Vault\nExample:\nuan_vault_url: \u0026#34;http://cray-vault.vault:8200\u0026#34; uan_vault_role_file The required Kubernetes role file for HashiCorp Vault access.\nExample:\nuan_vault_role_file: /var/run/secrets/kubernetes.io/serviceaccount/namespace uan_vault_jwt_file The path to the required Kubernetes token file for HashiCorp Vault access.\nExample:\nuan_vault_jwt_file: /var/run/secrets/kubernetes.io/serviceaccount/token uan_vault_path The path to use for storing data for UANs in HashiCorp Vault.\nExample:\nuan_vault_path: secret/uan uan_vault_key The key used for storing the root password in HashiCorp Vault.\nExample:\nuan_vault_key: root_password Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_shadow } This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/install/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=install; given_title=install; menuTitle: \u0026ldquo;install\u0026rdquo; date: Sun Mar 26 02:00:59 UTC 2023 draft: false weight: 30 install Topics: Install or Upgrade UAN "
},
{
	"uri": "/docs-uan/en-260-alpha6/install/install_the_uan_product_stream/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Install or Upgrade UAN Install and Upgrade Framework The Install and Upgrade Framework (IUF) provides commands which install, upgrade and deploy products on systems managed by CSM. IUF capabilities are described in detail in the IUF section of the Cray System Management Documentation. The initial install and upgrade workflows described in the HPE Cray EX System Software Stack Installation and Upgrade Guide for CSM (S-8052) detail when and how to use IUF with a new release of UAN or any other HPE Cray EX product.\nThis document does not replicate install, upgrade or deployment procedures detailed in the IUF section of the Cray System Management Documentation. This document provides details regarding software and configuration content specific to UAN which may be needed when installing, upgrading or deploying a UAN release. The IUF section of the Cray System Management Documentation will indicate when sections of this document should be referred to for detailed information.\nIUF will perform the following tasks for a release of UAN.\nIUF deliver-product stage: Uploads UAN configuration content to VCS Uploads UAN information to the CSM product catalog Uploads UAN content to Nexus repositories Uploads the UAN Stock Kernel image to IMS IUF update-vcs-config stage: Updates the VCS integration branch with new UAN configuration content IUF update-cfs-config stage: Creates new CFS configurations with new UAN configuration content IUF prepare-images stage: Creates updated UAN images based on COS with new UAN content IUF managed-nodes-rollout stage: Boots UAN nodes with an image containing new UAN content IUF uses a variety of CSM and SAT tools when performing these tasks. The IUF section of the Cray System Management Documentation describes how to use these tools directly if it is desirable to use them instead of IUF.\nIUF Stage Details for UAN This section describes any UAN details that an administrator may need to be aware of before executing IUF stages. Entries are prefixed with Information if no administrative action is required or Action if an administrator may need to perform tasks outside of IUF.\nupdate-vcs-config Action: Before executing this stage, the administrator should ensure the IUF site variables file (see iuf -sv SITE_VARS) is updated to reflect site preferences, including the desired VCS branching configuration. The branching configuration will be used by the update-vcs-config stage when modifying UAN branches in VCS.\nupdate-cfs-config Action: Before executing this stage, any site-local UAN configuration changes should be made so the following stages execute using the desired UAN configuration values. See the About UAN Configuration section of this documentation for UAN configuration content details. Note that the Prepare for UAN Product Installation section is required for fresh install scenarios.\nUAN Content Installed The following subsections describe the majority of the UAN content installed and configured on the system by IUF. The new version of UAN (2.6.XX) and its artifacts will be displayed in the CSM product catalog alongside any previously released version of UAN and its artifacts.\nConfiguration UAN provides configuration content in the form of Ansible roles and plays. This content is uploaded to a VCS repository in a branch with a specific UAN version number (2.6.XX) to distinguish it from any previously released UAN configuration content. This content is described in detail in the About UAN Configuration section.\nFor application nodes based on COS, the COS compute image is used as the base application node image and two COS CFS layers are required. The first COS CFS layer runs the cos-application.yml Ansible playbook and ensures that the COS content is applied as part of the image customization and node personalization processes. This COS CFS layer must precede the UAN CFS layer in the UAN CFS configuration. A second COS CFS layer running the Ansible playbook, cos-application-after.yml, runs after the UAN CFS layer of the UAN CFS configuration to ensure that the application node initrd is rebuilt and that any customer defined filesystems are configured.\nInput files for sat bootprep are provided in the hpc-csm-software-recipe VCS repository and include COS components in the CFS configuration, node image, and BOS session template definitions. The compute-and-uan-bootprep.yaml input file is used for compute and application nodes.\nProcedure to Set Root Password For UAN/Application Nodes The following instructions describe how to set the root password for UAN/Application nodes.\nObtain the HashiCorp Vault root token.\nncn-m001# kubectl get secrets -n vault cray-vault-unseal-keys \\ -o jsonpath=\u0026#39;{.data.vault-root}\u0026#39; | base64 -d; echo Log into the HashiCorp Vault pod.\nncn-m001# kubectl exec -itn vault cray-vault-0 -c vault -- sh Once attached to the pod\u0026rsquo;s shell, log into vault and read the secret/uan key by executing the following commands. If the secret is empty, \u0026ldquo;No value found at secret/uan\u0026rdquo; will be displayed.\npod# export VAULT_ADDR=http://cray-vault:8200 pod# vault login pod# vault read secret/uan If no value is found for this key, complete the following steps in another shell on the NCN management node.\na. Generate the password HASH for the root user. Replace \u0026lsquo;PASSWORD\u0026rsquo; with a chosen root password.\n```bash ncn-m001# openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc A-Z-a-z-0-9 | head -c4) PASSWORD ``` b. Take the HASH value returned from the previous command and enter the following in the vault pod\u0026rsquo;s shell. Instead of HASH, use the value returned from the previous step. You must escape the HASH value with the single quote to preserve any special characters that are part of the HASH value. If you previously have exited the pod, repeat steps 1-3 above; there is no need to perform the vault read since the content is empty.\n```bash pod# vault write secret/uan root_password='HASH' ``` c. Verify the new hash value is stored.\n```bash pod# vault read secret/uan ... pod# exit ``` UAN Stock Kernel Image UAN provides a stock kernel Application Node image which may be used on Application nodes that do not require any COS compatibility as UAN does. This image is not based on the COS image which the default UAN image is and is uploaded to IMS as part of the installation process. The stock kernel Application Node image is based on SLES 15 SP4.\nRPMs UAN provides RPMs used on UAN nodes. The RPMs are uploaded to Nexus as part of the installation process.\nThe following Nexus raw repositories are created:\nuan-2.6.XX-sle-15sp4 uan-2.6.XX-sle-15sp3 The following Nexus group repositories are created and reference the aforementioned Nexus raw repos.\nuan-2.6-sle-15sp4 uan-2.6-sle-15sp3 The uan-2.6-sle-15sp4 and uan-2.6-sle-15sp3 Nexus group repositories are used when building UAN node images and are accessible on UAN nodes after boot.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/advanced/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=advanced; given_title=advanced; menuTitle: \u0026ldquo;advanced\u0026rdquo; date: Sun Mar 26 02:00:59 UTC 2023 draft: false weight: 50 advanced Topics: Customizing UAN Images Manually Enabling the Customer Access Network (CAN) or the Customer High Speed Network (CHN) Repurposing a Compute Node as a UAN Booting an Application Node with a SLES Image (Technical Preview) Configuring a UAN for K3s (Technical Preview) "
},
{
	"uri": "/docs-uan/en-260-alpha6/advanced/customizing_uan_images_manually/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=customizing uan images manually; given_title=customizing uan images manually; menuTitle: \u0026ldquo;Customizing UAN Images Manually\u0026rdquo; date: Sun Mar 26 02:00:53 UTC 2023 draft: false weight: 51 Customizing UAN Images Manually Query IMS for the UAN Image you want to customize.\nncn-m001:~/ $ cray ims images list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;uan\u0026#34;))\u0026#39; { \u0026#34;created\u0026#34;: \u0026#34;2021-02-18T17:17:44.168655+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;6d46d601-c41f-444d-8b49-c9a2a55d3c21\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;371b62c9f0263e4c8c70c8602ccd5158\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/6d46d601-c41f-444d-8b49-c9a2a55d3c21/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;uan-PRODUCT_VERSION-image\u0026#34; } Create a variable for the IMS image id in the returned data.\nncn-m001:~/ $ export IMS_IMAGE_ID=6d46d601-c41f-444d-8b49-c9a2a55d3c21 Using the IMS_IMAGE_ID, follow the instructions in the Customize an Image Root Using IMS in the CSM documentation to build the UAN Image.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/advanced/enabling_can_chn/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=enabling the customer access network (can) or the customer high speed network (chn); given_title=enabling the customer access network (can) or the customer high speed network (chn); menuTitle: \u0026ldquo;Enabling the Customer Access Network (CAN) or the Customer High Speed Network (CHN)\u0026rdquo; date: Sun Mar 26 02:00:53 UTC 2023 draft: false weight: 52 Enabling the Customer Access Network (CAN) or the Customer High Speed Network (CHN) The UAN product provides a role, uan_interfaces in the Configuration Framework Service (CFS). This role is suitable for Application type nodes, and in some circumstancs, configuring the CHN, on Compute type nodes.\nEnable CAN or CHN by setting the following in the UAN CFS repo (the filename may be modified to whatever is appropriate):\nncn-m001:~/ $ cat group_vars/Application_UAN/can.yml uan_can_setup: true SLS will be configured with either CAN or CHN, uan_interfaces will use this setting to determine if a default route should be established over the nmn (CAN) or the hsn (CHN). To see how the site is configured, query SLS:\nncn-m001:~/ $ cray sls search networks list --name CHN --format json | jq -r \u0026#39;.[] | .Name\u0026#39; CHN (Optional) If the compute nodes are going to use the UAN CFS role uan_interfaces to set a default route on the CHN, make sure there is an appopriate ansible setting for the compute nodes in addition to the UANs:\nncn-m001:~/ $ cat group_vars/Compute/can.yml uan_can_setup: true Once uan_can_setup has been enabled, update the CFS configuration used for the nodes to initate a reconfiguration (see the Configuration Management section of the CSM documentation for more information)\nThe CSM documenation provides additional resources to validate the configuration of CAN and CHN for UANs and Computes. Consult the section titled \u0026ldquo;Enabling Customer High Speed Network Routing\u0026rdquo; in the CSM documentation for more information.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/advanced/enabling_k3s/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=configuring a uan for k3s (technical preview); given_title=configuring a uan for k3s (technical preview); menuTitle: \u0026ldquo;Configuring a UAN for K3s (Technical Preview)\u0026rdquo; date: Sun Mar 26 02:00:53 UTC 2023 draft: false weight: 55 Configuring a UAN for K3s (Technical Preview) WARNING: This feature is currently a Technical Preview, as such it requires completion of the Prerequisites section. Future releases will streamline these manual configuration steps and enhance the experience of using the rootless podman containers. Therefore, some of these configuration options may change in future releases.\nUAI Experience on UANs In UAN 2.6, a new playbook has been added to create a single node, K3s cluster. This K3s environment can then run the services necessary to replicate the experience of User Access Instances (UAIs) on one or more UANs.\nUse of K3s K3s will serve as the orchestrator of services necessary to replicate the capabilities of UAIs on UAN hardware. This includes HAProxy, MetalLB, and eventually DNS services like ExternalDNS and PowerDNS. Notably, this does not orchestrate instances of sshd and podman containers through K3s. K3s and the initial set of services mimic how the \u0026ldquo;Broker UAIs\u0026rdquo; in CSM to handle the SSH ingress and redirection of users into their interactive environment.\nUse of Podman Traditional UAIs in CSM required some level of privilege in CSM for access to host volume mounts, networking, and startup activities. Podman containers offer an attractive solution for an interactive environment in which to place users. They can be rootless containers that do not rely on privilege escalation. When running on UANs, podman containers have access to a hosting environment that is already tailored to users.\nOverview The overall component flow for replicating containerized environments for End-Users on UANs is as follows:\nA user uses ssh to initiate a connection to the HAProxy load-balancer running in K3s. HAProxy, using the configured load balancing algorithms, will forward the SSH connection to an instance of sshd running on a UAN. sshd, running on a UAN via systemd, will initiate a rootless podman container as the user using the ForceCommand configuration. The user is placed in a podman container for an interactive session, or their SSH_ORIGINAL_COMMAND is run in the container. When the user disconnects, the podman process exits, and the container is removed. There are alternate configurations of podman that would allow for different workflows, for example, the main pid of the container could be long running, to facilitate easier reentry to the container on subsequent logins.\nPrerequisites The following steps should be completed prior to configuring the UAN with K3s.\nDesignate a UAN to operate as the K3s control-plane node.\nNote: In a future release, additional UANs will be able to join as extra manager or worker nodes.\nIdentify a pool of IP addresses for the services running in K3s.\nThis address pool must be routable from the UAN control-plane, and should be unused for other purposes.\nThis will allow for external LoadBalancer IP Address to be assigned to services like HAProxy. Initially, these IP addresses will serve as the SSH ingress for instances of HAProxy.\nConfigure and create the /etc/subuid and /etc/subgid files.\nTo allow for users to run rootless podman containers, these files must be present and configured with an entry for each user. These files should be uploaded to the user S3 bucket:\n$ cray artifacts list user --format json { \u0026#34;artifacts\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;subuid\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2023-02-21T23:41:43.948000+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;c543aebb9b40bcf48879885734447090\\\u0026#34;, \u0026#34;Size\u0026#34;: 145686, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34;, \u0026#34;Owner\u0026#34;: { \u0026#34;DisplayName\u0026#34;: \u0026#34;User Service User\u0026#34;, \u0026#34;ID\u0026#34;: \u0026#34;USER\u0026#34; } }, { \u0026#34;Key\u0026#34;: \u0026#34;subgid\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2023-02-21T23:41:43.948000+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;73032ede132e44d2c1bc567246901737\\\u0026#34;, \u0026#34;Size\u0026#34;: 145686, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34;, \u0026#34;Owner\u0026#34;: { \u0026#34;DisplayName\u0026#34;: \u0026#34;User Service User\u0026#34;, \u0026#34;ID\u0026#34;: \u0026#34;USER\u0026#34; } } ] } Podman Image Place a container image suitable for users within a container image registry accessible from the UAN. Configuring with Configuration Framework Service (CFS) After completing the Prerequisites section, the following are available to proceed with configuring a UAN to run K3s.\nA fully configured UAN An IPAddress start and end range to assign to MetalLB Prepared subuid and subgid files Configuration Files and Playbook Configuration for K3s and related services are controlled in the following ansible files in the UAN VCS repository:\n$ ls uan-config-management/vars/ | egrep \u0026#34;k3s|sshd|helm\u0026#34; uan_helm.yml uan_k3s.yml uan_sshd.yml The ansible playbook to install and configure K3s may be found here:\n$ ls uan-config-management | grep k3s k3s.yml Ansible Roles The following Ansible roles are provided in the uan-config-management repository in VCS. There are README.md files in each Ansible role directory in this repository that provide further details.\nuan-k3s-install: Download and stage K3s assets necessary to initialize and configure k3s on a node uan-k3s-stage: Install and configure K3s on a node uan-helm: Perform tasks to initialize an environment to install helm charts uan-haproxy: Deploy a list of HAProxy charts to the k3s cluster uan-metallb: Deploy the MetalLB chart to the k3s cluster uan-sshd: Create and enable instances of sshd with systemd Artifactory Assets UAN uploads artifacts to deploy to the UAN control-plane node in a new nexus repository:\nuan-2.6.XX-third-party The following Nexus group repository is created and reference the aforementioned UAN Nexus raw repos.\nuan-2.6-third-party This repository will contain the installer for K3s, Helm charts for HAProxy and MetalLB, etc.\nValidation Tests To validate the K3s cluster once deployed, see the Validation Checks section of this document for details.\nConfiguring K3s, MetalLB, HAProxy, and SSHD for use with Podman Each of the sections below describe how the various comonents deployed to K3s and the UANs may be configured to enable users to SSH to rootless podman containers. As there is no one configuration to fit any one use case, read and understand each section to modify the configuration as needed. Once each section has been completed, see Deploy K3s to the UAN.\nMetalLB Configure the start and end range for MetalLB IPAddressPool in vars/uan_helm.yml:\n$ grep \u0026#34;^metallb_ipaddresspool\u0026#34; vars/uan_helm.yml metallb_ipaddresspool_range_start: \u0026#34;x.x.x.x\u0026#34; metallb_ipaddresspool_range_end: \u0026#34;x.x.x.x\u0026#34; MetalLB will assign an IP address to each service running in K3s that requires and external IP address. In the case of HAProxy, each instance of HAProxy will require an IP address. Podman containers do not require their own IP address.\nNote: In a future version of CSM, this range may be integrated into the System Layout Service (SLS) so the range will be automatically determined.\nImportant: Before modifying customer-access, be sure to verify none of the IP Addresses in the new pool for UANs are being used by IMS or UAIs:\n# kubectl get services -n ims | grep ims # kubectl get services -n user | grep uai # kubectl get services -n uai | grep It may be possible to reallocate the CSM MetalLB pool customer-access from CSM to make room for a subset of IPs to use with MetalLB on UANs. To shrink the customer-access pool in CSM, edit the configmap and pick a new CIDR block for for customer-access. In this example the CIDR block was x.x.x.x/26:\n# kubectl edit -n metallb-system cm/metallb ... data: config: | address-pools: - addresses: - x.x.x.x/27 name: customer-access protocol: bgp ... This leaves a portion of IP Address unallocated that may then be used to set metallb_ipaddresspool_range_start and metallb_ipaddresspool_end.\nImportant: When calculating the range of IP Address now available from customer-access. Be sure to account for the Broadcast IP of the remaining customer-access pool.\nTo complete migrating the IP Address range out of CSM, restart the MetalLB controller pod in CSM.\n# kubectl delete pod -n metallb-system -l app.kubernetes.io/component=controller HAProxy Configuration Each SSH ingress is backed by a K3s deployment of HAProxy. By default, a single instance of HAProxy is enabled in vars/uan_helm.yml:\nuan_haproxy: - name: \u0026#34;haproxy-uai\u0026#34; namespace: \u0026#34;haproxy-uai\u0026#34; chart: \u0026#34;{{ haproxy_chart }}\u0026#34; chart_path: \u0026#34;{{ helm_install_path }}/charts/{{ haproxy_chart }}.tgz\u0026#34; args: \u0026#34;--set service.type=LoadBalancer\u0026#34; This must be further configured with additional values to populate the HAProxy configuration. For example, to load-balance SSH to three UANs, the following configuration changes should be made:\nuan_haproxy: - name: \u0026#34;haproxy-uai\u0026#34; namespace: \u0026#34;haproxy-uai\u0026#34; chart: \u0026#34;{{ haproxy_chart }}\u0026#34; chart_path: \u0026#34;{{ helm_install_path }}/charts/{{ haproxy_chart }}.tgz\u0026#34; args: \u0026#34;--set service.type=LoadBalancer\u0026#34; config: | global log stdout format raw local0 maxconn 1024 defaults log global mode tcp timeout connect 10s timeout client 36h timeout server 36h option dontlognull listen ssh bind *:22 balance leastconn mode tcp option tcp-check tcp-check expect rstring SSH-2.0-OpenSSH.* server uan01 uan01.example.domain.com:9000 check inter 10s fall 2 rise 1 server uan02 uan02.example.domain.com:9000 check inter 10s fall 2 rise 1 server uan03 uan03.example.domain.com:9000 check inter 10s fall 2 rise 1 This is an example that should be tailored to the desired configuration. See the SSHD Configuration section to create new instances of SSHD to respond to HAProxy connections outside of the standard SSHD running on port 22.\nFor more information HAProxy configurations, see HAProxy Configuration.\nTo enable additional instances of HAProxy representing alternate configurations, add a new element to the list uan_haproxy.\nSSHD Configuration The role uan_sshd runs in the playbook k3s.yml to start and configure new instances of SSHD to respond to HAProxy forwarded connections. Each new instance of SSHD is defined in vars/uan_sshd.yml as an element in the list uan_sshd_configs:\nuan_sshd_configs: - name: \u0026#34;uai\u0026#34; config_path: \u0026#34;/etc/ssh/uan\u0026#34; port: \u0026#34;9000\u0026#34; This will create a systemd unit file /usr/lib/systemd/system/sshd_uai.service and will mark the service as enabled. A SSH config file will also be created at /etc/ssh/uan/sshd_uai_config to start sshd listening on port 9000.\nThis default configuration will simply place users into their standard shell. To create a rootless podman container upon logging in, specify alternate configuration:\n- name: \u0026#34;uai\u0026#34; config_path: \u0026#34;/etc/ssh/uan\u0026#34; port: \u0026#34;9000\u0026#34; config: | Match User * AcceptEnv DISPLAY X11Forwarding yes AllowTcpForwarding yes PermitTTY yes ForceCommand podman --root /scratch/containers/$USER run -it -h uai --cgroup-manager=cgroupfs --userns=keep-id --network=host -e DISPLAY=$DISPLAY registry.local/cray/uai:latest Note: In the example above, the image registry.local/cray/uai:latest was provided as an example, this should be modified to reference an available container image.\nDeploy K3s to the UAN Once the VCS repository has been updated with the appropriate values, generate a new image and reboot the UAN.\nAlternatively, update the active CFS configuration on a single running UAN to include the k3s.yml playbook and uncomment out the following line from k3s.yml:\ntasks: - name: Application node personalization play include_role: name: \u0026#34;{{ item }}\u0026#34; with_items: #- uan_k3s_stage # Uncomment to stage K3s assets without Image Customization - uan_k3s_install This will download the necessary assets without requiring an image rebuild.\nAfter the node has been booted and configured, proceed with the Validation Checks section to verify the components have been configured correctly.\nValidation Checks K3s Validation To verify the k3s.yml playbook suceeded, peform the following sanity checks.\nVerify kubectl from the UAN.\nuan01:~ # export KUBECONFIG=~/.kube/k3s.yml uan01:~ # kubectl get nodes NAME STATUS ROLES AGE VERSION uan01 Ready control-plane,master 3h58m v1.26.0+k3s1 Verify HAProxy and MetalLB are installed with helm\nuan01:~ # export KUBECONFIG=~/.kube/k3s.yml uan01:~ # helm ls -A NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION haproxy-uai\thaproxy-uai 1 2023-03-01 10:55:10.916137137 -0600 CST\tdeployed\thaproxy-1.17.3\t2.6.6 metallb metallb-system\t1 2023-03-01 10:40:15.548380973 -0600 CST\tdeployed\tmetallb-0.13.7\tv0.13.7 Check pod status of HAProxy and MetalLB\nuan01:~ # kubectl get pods -A | egrep \u0026#34;haproxy|metallb\u0026#34; metallb-system metallb-controller-5b89f7554c-mzjvt 1/1 Running 0 4h1m metallb-system metallb-speaker-ltnkx 1/1 Running 0 4h1m haproxy-uai haproxy-uai-7kg6p 1/1 Running 0 3h46m Verify MetalLB has assigned an external IP address to HAProxy:\nExamine the CPS cm-pm pod logs using the following command:\nuan01:~ # kubectl get services -A -l app.kubernetes.io/name=haproxy NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE haproxy-uai haproxy-uai LoadBalancer x.x.x.x x.x.x.x 22:30886/TCP 3h47m Verify the new instance of SSHD is running:\nuan01:~ # systemctl status sshd_uai  sshd_uai.service - OpenSSH Daemon Generated for uai Loaded: loaded (/usr/lib/systemd/system/sshd_uai.service; disabled; vendor preset: disabled) Active: active (running) since Wed 2023-03-01 12:43:31 CST; 2h 4min ago Finally, use SSH to log in through the HAProxy load balancer:\n$ ssh x.x.x.x Trying to pull registry.local/cray/uai:1.0... Getting image source signatures Copying blob 07a88a2f44f8 done Copying blob 99a1d1c8ca98 done Copying blob a028e278fdc1 done Copying blob 5caad07f5e12 done Copying blob 157b0fca679c done Copying config de48e6a913 done Writing manifest to image destination Storing signatures sh-4.4$ "
},
{
	"uri": "/docs-uan/en-260-alpha6/advanced/repurposing_compute_as_uan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=repurposing a compute node as a uan; given_title=repurposing a compute node as a uan; menuTitle: \u0026ldquo;Repurposing a Compute Node as a UAN\u0026rdquo; date: Sun Mar 26 02:00:53 UTC 2023 draft: false weight: 53 Repurposing a Compute Node as a UAN This section describes how to repurpose a compute node to be used as a User Access Node (UAN). This is typically done when the processor type of the compute node is not yet available in a UAN server.\nOverview The following steps outline the process of repurposing a compute node to be used as a UAN.\nVerify the System Default Route is set to CHN.\nChange the role of the compute node in the Hardware State Manager from Compute to Application and set the sub-role to UAN.\nEnsure that IPs on the CHN exist for the computes nodes in SLS.\nBoot the repurposed compute node as a UAN.\nVerify the repurposed compute node functions as a UAN.\nPrerequisites There are no changes needed in hardware, network cabling, or UEFI/BIOS/BMC configuration to repurpose a compute node for use as a UAN. However, compute nodes do not have the necessary network interface cards to support user access over the Customer Access Network (CAN). Additionally, the network configuration of Mountain Cabinets do not support the CAN network. Therefore, repurposing a compute node as a UAN requires the system to be configured to use the Customer High-Speed Network (CHN) and that the compute nodes have a CHN IP address in SLS.\nThe SLS Networks setting for the SystemDefaultRoute must be CHN The repurposed compute nodes must have CHN IP addresses in SLS uan_can_setup must be set to true in the uan-config-management repo Procedure Perform the following steps to repurpose a compute node for use as a UAN.\nLog in to the master node ncn-m001. All commands in this procedure are run from the master node.\nVerify the system is configured to use the CHN as the System Default Route. If the SystemDefaultRoute is not CHN, the compute nodes may not be repurposed as UAN.\nncn-m001# cray sls networks describe BICAN --format json | jq -r \u0026#39;.ExtraProperties.SystemDefaultRoute\u0026#39; Verify a CHN IP address exists in SLS for each repurposed compute node. Repeat the following command and replace \u0026lt;XNAME\u0026gt; with the xname of each repurposed compute node. The compute node must have a CHN IP address in SLS or it cannot be repurposed as a UAN. See Add Compute IP addresses to CHN SLS data section of the Cray System Management documentation for information on adding compute nodes to the CHN.\nncn-m001# cray sls networks describe CHN | q -r \u0026#39;.ExtraProperties.Subnets[] | select(.FullName == \u0026#34;CHN Bootstrap DHCP Subnet\u0026#34;) | .IPReservations[] | select(.Comment == \u0026#34;\u0026lt;XNAME\u0026gt;\u0026#34;)\u0026#39; Verify that uan_can_setup: true is set in the uan-config-management CFS repo. See Enabling the Customer Access Network (CAN) or the Customer High Speed Network (CHN) for more information.\nChange the role and sub-role in HSM of the compute node(s) being repurposed as UANs to Application and UAN, respectively. Repeat the following command and replace \u0026lt;XNAME\u0026gt; with the xname of each repurposed compute node.\nncn-m001# cray hsm state components role update --role Application --sub-role UAN \u0026lt;XNAME\u0026gt; Verify the role and sub-role in HSM of the repurposed compute node(s) has been changed to \u0026lsquo;Application and 'UAN, respectively. Repeat the following command and replace \u0026lt;XNAME\u0026gt; with the xname of each repurposed compute node.\nncn-m001# cray hsm state components describe \u0026lt;XNAME\u0026gt; Run the BOS session template used to boot the UAN nodes. See Boot UAN Nodes for more information on booting UAN nodes with BOS. Replace \u0026lt;UAN_SESSIONTEMPLATE\u0026gt; with the name of the BOS session template used to boot the UAN nodes and \u0026lt;XNAME\u0026gt; with the xname of the repurposed compute node.\nncn-m001# cray bos session create --template-uuid \u0026lt;UAN_SESSIONTEMPLATE\u0026gt; --operation reboot --limit \u0026lt;XNAME\u0026gt; Verification as a UAN Once the repurposed compute node is booted as a UAN, the following steps will verify it is configured as a UAN. These steps may vary dependent upon how the site has configured the UAN nodes.\nBasic UAN Configuration Checks Verify the repurposed compute node has finished the configuration phase. The output should \u0026ldquo;configured\u0026rdquo;.\nncn-m001# cray cfs components describe \u0026lt;XNAME\u0026gt; --format json | jq -r .configurationStatus Login to the repurposed compute node from the master node ncn-m001 as the root user.\nVerify that the hsn0 interface has the CHN IP address assigned to it in SLS.\nuan# ip a | grep hsn0 Verify the default route is via hsn0\nuan# ip r | grep default Verify that all site UAN filesystems are mounted.\nCommon UAN Configuration Checks If LDAP is used for user authentication, verify the LDAP service is reachable.\nuan# ping \u0026lt;ldap_service_ip\u0026gt; If SLURM is used, test sinfo and srun commands. This example srun command should return the hostname of 4 compute nodes.\nuan# sinfo uan# srun -N4 hostname Verify Users can Login Login to the repurposed compute node as an authorized non-root user from any host that should have UAN access.\nIf SLURM is used, test sinfo and srun commands. This example srun command should return the hostname of 4 compute nodes.\nuan# sinfo uan# srun -N4 hostname "
},
{
	"uri": "/docs-uan/en-260-alpha6/advanced/sles_image/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=booting an application node with a sles image (technical preview); given_title=booting an application node with a sles image (technical preview); menuTitle: \u0026ldquo;Booting an Application Node with a SLES Image (Technical Preview)\u0026rdquo; date: Sun Mar 26 02:00:53 UTC 2023 draft: false weight: 54 Booting an Application Node with a SLES Image (Technical Preview) A SLES image is available for use with Application type nodes. This image is currently considered a \u0026ldquo;Technical Preview\u0026rdquo; as the initial support for booting with SLES Images without COS. This guide documents the procedure to boot and configure the new image as it currently differs from the standard COS-based image process in some ways.\nThe image is built with the same packer/qemu pipeline as Non-Compute-Node Images. Similarties may be noticed including the kernel and package versions.\nLimitations As this is currently a \u0026ldquo;Technical Preview\u0026rdquo; of supporting SLES Images on Application Nodes, there are several limitations:\nS3 presigned URLs with an expiration limit for the rootfs must be created. BSS parameters must be set with cray bss bootparameters replace ... BOS Sessions and Templates are not supported. CFS Configurations that operate on COS and NCN images are not yet supported. CFS Node Personalization must be started manually. Overview The following steps outline the process of configuring and booting an Application Node with the SLES Image.\nDetermine the image to use.\nConfigure the image with IMS/CFS (optional).\nUpdate BSS with the necessary parameters.\nReboot the node.\nRun CFS Node Personalization (optional).\nProcedure Perform the following steps to configure and boot a SLES image on an Application type node.\nLog in to the master node ncn-m001. All commands in this procedure are run from the master node.\nVerify the UAN release contains a SLES image.\nncn-m001# UAN_RELEASE=2.5 ncn-m001# sat showrev --filter \u0026#39;product_name = uan\u0026#39; | grep $UAN_RELEASE Select an Image to boot or customize.\nncn-m001# APP_IMAGE_NAME=cray-application-sles15sp3.x86_64-0.1.0 ncn-m001# APP_IMAGE_ID=$(cray ims images list --format json | jq --arg APP_IMAGE_NAME \u0026#34;$APP_IMAGE_NAME\u0026#34; -r \u0026#39;sort_by(.created) | .[] | select(.name == $APP_IMAGE_NAME ) | .id\u0026#39; | head -1) ncn-m001# cray ims images describe $APP_IMAGE_ID --format json { \u0026#34;created\u0026#34;: \u0026#34;2022-08-24T20:07:27.263737+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;13964414-bbad-40e9-9e31-a3683010febb\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/13964414-bbad-40e9-9e31-a3683010febb/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;cray-application-sles15sp3.x86_64-0.1.0\u0026#34; } Customize the image using SAT Bootprep. This will add a root password to the image as one is not included. If CFS is not going to be used on this node, this step is optional. Support for additional product layers will be added in subsequent releases.\nncn-m001# cat bootprep-sles-uan.yml configurations: - name: sles-uan-configuration layers: - name: uan playbook: site.yml product: name: uan version: 2.5.3 branch: integration-2.5.3 images: - name: sles-uan-image ims: is_recipe: false name: cray-application-sles15sp3.x86_64-0.1.0 configuration: sles-uan-configuration configuration_group_names: - Application - Application_UAN ncn-m001# sat bootprep run ./bootprep-sles-uan.yml ncn-m001# APP_IMAGE_NAME=sles-uan-image ncn-m001# APP_IMAGE_ID=$(cray ims images list --format json | jq --arg APP_IMAGE_NAME \u0026#34;$APP_IMAGE_NAME\u0026#34; -r \u0026#39;sort_by(.created) | .[] | select(.name == $APP_IMAGE_NAME ) | .id\u0026#39; | head -1) ncn-m001# cray ims images describe $APP_IMAGE_ID --format json { \u0026#34;created\u0026#34;: \u0026#34;2022-08-25T20:07:27.263737+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;13964414-bbad-40e9-9e31-a36830101234\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/13964414-bbad-40e9-9e31-a36830101234/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;sles-uan-image\u0026#34; } Create a presigned URL for the rootfs. This is needed for the node to boot in this release, in the future, this will be integrated into BSS and will not need to be performed. This URL will be valid for 1 hour and will need to be recreated if the node reboots after the URL expires. To set a longer expiration, adjust the \u0026ldquo;aws s3 presign\u0026rdquo; command accordingly.\nncn-m001# export AWS_ACCESS_KEY_ID=`kubectl get secrets -o yaml ims-s3-credentials -ojsonpath=\u0026#39;{.data.access_key}\u0026#39; | base64 -d` ncn-m001# export AWS_SECRET_ACCESS_KEY=`kubectl get secrets -o yaml ims-s3-credentials -ojsonpath=\u0026#39;{.data.secret_key}\u0026#39; | base64 -d` ncn-m001# alias aws=\u0026#34;aws --endpoint-url http://rgw-vip\u0026#34; ncn-m001# ROOTFS_URL=$(aws s3 presign --expires-in 3600 s3://boot-images/$APP_IMAGE_ID/rootfs) Select an Application UAN to boot with the image.\nncn-m001# cray hsm state components list --role Application --subrole UAN --format json | jq -r \u0026#39;.Components | .[] | .ID\u0026#39; x3000c0s13b0n0 x3000c0s15b0n0 ncn-m001# NODE=x3000c0s13b0n0 Select a MAC address to use as the NMN interface.\nncn-m001:~ # cray hsm inventory ethernetInterfaces list --component-id $NODE --format json | jq -r \u0026#39;.[] | \u0026#34;\\(.Description) \\t \\(.MACAddress)\u0026#34;\u0026#39; Ethernet Interface Lan1 b4:2e:99:fd:45:c8 Ethernet Interface Lan2 b4:2e:99:fd:45:c9 ncn-m001:~ # MAC=b4:2e:99:fd:45:c8 Update BSS with the kernel, initrd, and desired parameters.\nncn-m001:~ # PARAMS=\u0026#34;ifname=nmn0:$MAC ip=nmn0:dhcp spire_join_token=\\${SPIRE_JOIN_TOKEN} biosdevname=1 pcie_ports=native transparent_hugepage=never console=tty0 console=ttyS0,115200 iommu=pt metal.no-wipe=1 initrd=initrd root=live:$ROOTFS_URL rd.live.ram=0 rd.writable.fsimg=0 rd.skipfsck rd.live.squashimg=filesystem.squashfs rd.live.overlay.thin=1 rd.live.overlay.overlayfs=1 rd.luks=0 rd.luks.crypttab=0 rd.lvm.conf=0 rd.lvm=1 rd.auto=1 rd.md=1 rd.dm=0 rd.neednet=0 rd.peerdns=1 rd.md.waitclean=1 rd.multipath=0 rd.md.conf=1 rd.bootif=0 hostname=$NODE rd.net.dhcp.retry=3 append nosplash quiet log_buf_len=1 rd.retry=10 rd.shell\u0026#34; ncn-m001:~ # cray bss bootparameters replace --hosts $NODE --initrd \u0026#34;s3://boot-images/$APP_IMAGE_ID/initrd\u0026#34; --kernel \u0026#34;s3://boot-images/$APP_IMAGE_ID/kernel\u0026#34; --params \u0026#34;$PARAMS\u0026#34; Reboot the node. Wait for the status to return off before issuing the power on command.\nncn-m001:# USERNAME=root ncn-m001:# read -r -s -p \u0026#34;$NODE BMC ${USERNAME} password: \u0026#34; IPMI_PASSWORD; echo ncn-m001:# export IPMI_PASSWORD ncn-m001:# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus -H ${NODE::-2} power off ncn-m001:# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus -H ${NODE::-2} power status ncn-m001:# ipmitool -U \u0026#34;${USERNAME}\u0026#34; -E -I lanplus -H ${NODE::-2} power on Connect to the console for the node and verify it boots into multi-user mode. Find the correct pod by using conman -q to list the available connections in each pod.\nncn-m001# kubectl exec -it -n services cray-console-node-0 -- conman -j $NODE ... 2022-08-25 14:27:51 Welcome to SUSE Linux Enterprise High Performance Computing 15 SP3 (x86_64) - Kernel 5.3.18-150300.59.43-default (ttyS0). 2022-08-25 14:27:51 2022-08-25 14:27:51 x3000c0s13b0n0 login: If the node does not complete the boot successfully, proceed to the troubleshooting section in this guide.\nTroubleshooting Some general troublshooting tips may help in getting started using the SLES image.\nDracut failures during booting Could not find the kernel or the initrd. Verify the BSS bootparameters for the node. Specifically, check that the IMS Image ID is correct.\nhttp://rgw-vip.nmn/boot-images/13964414-bbad-40e9-9e31-a3683010febbasdf/kernel...HTTP 0x7f0fa808 status 404 Not Found No such file or directory (http://ipxe.org/2d0c618e) http://rgw-vip.nmn/boot-images/13964414-bbad-40e9-9e31-a3683010febbasdf/initrd...HTTP 0x7f0fa808 status 404 Not Found No such file or directory (http://ipxe.org/2d0c618e) The presigned URL was generated incorrectly.\n2022-08-22 18:49:33 [ 9.170981] dracut-initqueue[1427]: curl: (22) The requested URL returned error: 404 Not Found 2022-08-22 18:49:33 [ 9.191138] dracut-initqueue[1421]: Warning: Downloading \u0026#39;http://rgw-vip/boot-images/c0d2d5fd-8354-4f21-a0ef-8ee2878cbde7/filesystem.squashfs?AWSAccessKeyId=I 43RBLH07R65TRO3AL02\u0026amp;Signature=YLmTttUa2KT7qzKLemOd1zIsWlo%3D\u0026amp;Expires=1661273999\u0026#39; failed! 2022-08-22 18:49:33 [ 9.222966] dracut-initqueue[1411]: Warning: failed to download live image: error 0 No carrier detected on interface nmn0. Select a different MAC address to be assigned as nmn0.\nhttp://rgw-vip.nmn/boot-images/13964414-bbad-40e9-9e31-a3683010febb/kernel... ok http://rgw-vip.nmn/boot-images/13964414-bbad-40e9-9e31-a3683010febb/initrd... ok [ 4.966173] dracut-initqueue[975]: Warning: Unable to retrieve metadata from server [ 5.379611] dracut-initqueue[1130]: Warning: Unable to retrieve metadata from server [ 10.547290] dracut-initqueue[1144]: Warning: No carrier detected on interface nmn0 [ 10.564694] dracut-initqueue[1393]: ls: cannot access \u0026#39;/tmp/leaseinfo.nmn0*\u0026#39;: No such file or directory ... [ 28.381198] dracut-initqueue[945]: Warning: dracut-initqueue timeout - starting timeout scripts [ 28.400096] dracut-initqueue[945]: Warning: Could not boot. The root filesystem doesn\u0026rsquo;t won\u0026rsquo;t download because the URL is too long. Regenerate the URL using the aws command.\n2022-08-03 19:41:52 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0Warning: Failed to create the file 2022-08-03 19:41:52 [ 9.842822] dracut-initqueue[1428]: Warning: rootfs?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026amp;X-Amz-Credential=I43RBLH07R65T 2022-08-03 19:41:52 [ 9.862811] dracut-initqueue[1428]: Warning: RO3AL02%2F20220803%2F%2Fs3%2Faws4_request\u0026amp;X-Amz-Date=20220803T193518Z\u0026amp; 2022-08-03 19:41:52 [ 9.882714] dracut-initqueue[1428]: Warning: X-Amz-Expires=86400\u0026amp;X-Amz-SignedHeaders=host\u0026amp;X-Amz-Signature=9412c9eb0 2022-08-03 19:41:52 [ 9.902718] dracut-initqueue[1428]: Warning: 585604b3c8154376113c043fb41e3954cddc92a8d799e5176f8c140: File name 2022-08-03 19:41:52 [ 9.922710] dracut-initqueue[1428]: Warning: too long 2022-08-03 19:41:52 [ 9.938714] dracut-initqueue[1428]: 2022-08-03 19:41:52 0 2020M 0 13977 0 0 524k 0 1:05:40 --:--:-- 1:05:40 524k 2022-08-03 19:41:52 [ 9.958983] dracut-initqueue[1428]: curl: (23) Failed writing body (0 != 13977) 2022-08-03 19:41:52 [ 9.975166] dracut-initqueue[1422]: Warning: Downloading \u0026#39;http://rgw-vip/boot-images/66c37928-6887-463e-8d9f-e4eec8089374/rootfs?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026amp;X-Amz-Credential=I43RBLH07R65TRO3AL02%2F20220803%2F%2Fs3%2Faws4_request\u0026amp;X-Amz-Date=20220803T193518Z\u0026amp;X-Amz-Expires=86400\u0026amp;X-Amz-SignedHeaders=host\u0026amp;X-Amz-Signature=9412c9eb0585604b3c8154376113c043fb41e3954cddc92a8d799e5176f8c140\u0026#39; failed! 2022-08-03 19:41:52 [ 10.019096] dracut-initqueue[1412]: Warning: failed to download live image: error 0 The presigned URL has expired or was generated incorrectly.\n[ 9.787431] dracut-initqueue[1435]: curl: (22) The requested URL returned error: 403 Forbidden [ 9.807724] dracut-initqueue[1429]: Warning: Downloading \u0026#39;http://rgw-vip/boot-images/13964414-bbad-40e9-9e31-a3683010febb/rootfs?AWSAccessKeyId=I43RBLH07R65TRO3AL02\u0026amp;Signature=7%2FgOCotleoyLPGmeyG%2FFX8tpkWg%3D\u0026amp;Expires=1661523713\u0026#39; failed! The dracut module livenet is missing from the initrd. Make sure the initrd was regenerated with /srv/cray/scripts/common/create-ims-initrd.sh if CFS was used.\n2022-08-24 14:48:53 [ 5.784023] dracut: FATAL: Don\u0026#39;t know how to handle \u0026#39;root=live:http://rgw-vip/boot-images/e88ed416-5d58-4421-9013-fa2171ac11b8/rootfs?AWSAccessKeyId=I43RBLH07R65TRO3AL02\u0026amp;Signature=bL661kZHPyEgBsLLEuJHFz3zKVs%3D\u0026amp;Expires=1661438587\u0026#39; 2022-08-24 14:48:53 [ 5.805063] dracut: Refusing to continue Unable to log in to the node. The node is not up. Connect to the console and determine why the node has not booted, starting with the troubleshooting tips.\nncn-m001:# ssh app01 ssh: connect to host uan01 port 22: No route to host Unable to log in to the node with a password. No root password is defined in the image by default, one must be added via CFS or by modifying the squashfs filesystem.\nncn-m001:# ssh app01 Password: Password: Password: root@app01\u0026#39;s password: Permission denied, please try again DHCP hostname is not set If the node does not have a hostname assigned from DHCP, try verifying the DHCP settings and restarting wicked.\nx3000c0s13b0n0:~ # grep -R ^DHCLIENT_SET_HOSTNAME= /etc/sysconfig/network/dhcp DHCLIENT_SET_HOSTNAME=\u0026#34;yes\u0026#34; x3000c0s13b0n0:# systemctl restart wicked x3000c0s13b0n0:# hostnamectl Static hostname: x3000c0s13b0n0 Transient hostname: app01 Icon name: computer-server Chassis: server Machine ID: 9bd0aacf29d04dd4827bc464121b130b Boot ID: af753b4e6fa9419bb14d55a029d0f526 Operating System: SUSE Linux Enterprise High Performance Computing 15 SP3 CPE OS Name: cpe:/o:suse:sle_hpc:15:sp3 Kernel: Linux 5.3.18-150300.59.43-default Architecture: x86-64 x3000c0s13b0n0:# hostname app01 Spire is not running Check the spire-agent logs for error messages.\napp01# systemctl status spire-agent "
},
{
	"uri": "/docs-uan/en-260-alpha6/installation_prereqs/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=installation prereqs; given_title=installation prereqs; menuTitle: \u0026ldquo;installation prereqs\u0026rdquo; date: Sun Mar 26 02:00:59 UTC 2023 draft: false weight: 20 installation prereqs Topics: Prepare for UAN Product Installation Configure the BMC for UANs with iLO Configure the BIOS of an HPE UAN Configure the BIOS of a Gigabyte UAN "
},
{
	"uri": "/docs-uan/en-260-alpha6/installation_prereqs/configure_the_bios_of_a_gigabyte_uan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=configure the bios of a gigabyte uan; given_title=configure the bios of a gigabyte uan; menuTitle: \u0026ldquo;Configure the BIOS of a Gigabyte UAN\u0026rdquo; date: Sun Mar 26 02:00:54 UTC 2023 draft: false weight: 24 Configure the BIOS of a Gigabyte UAN Perform this procedure to configure the network interface and boot settings required by Gigabyte UANs.\nBefore the UAN product can be installed on Gigabyte UANs, specific network interface and boot settings must be configured in the BIOS.\nPress the Delete key to enter the setup utility when prompted to do so in the console.\nNavigate to the boot menu.\nSet the Boot Option #1 field to Network:UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection.\nSet all other Boot Option fields to Disabled.\nEnsure that the boot mode is set to [UEFI].\nConfirm that the time is set correctly. If the time is not accurate, correct it now.\nIncorrect time will cause PXE booting issues.\nSelect Save \u0026amp; Exit to save the settings.\nSelect Yes to confirm and press the Enter key.\nThe UAN will reboot.\nOptional: Run the following IPMI commands if the BIOS settings do not persist.\nIn these example commands, the BMC of the UAN is x3000c0s27b0. Replace USERNAME and PASSWORD with username and password of the BMC of the UAN. These commands do the following:\nPower off the node Perform a reset. Set the PXE boot in the options. Power on the node ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 power off ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 mc reset cold ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 chassis bootdev pxe \\ options=efiboot,persistent ncn-m001# ipmitool -I lanplus -U *** -P *** -H x3000c0s27b0 power on "
},
{
	"uri": "/docs-uan/en-260-alpha6/installation_prereqs/configure_the_bios_of_an_hpe_uan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=configure the bios of an hpe uan; given_title=configure the bios of an hpe uan; menuTitle: \u0026ldquo;Configure the BIOS of an HPE UAN\u0026rdquo; date: Sun Mar 26 02:00:54 UTC 2023 draft: false weight: 23 Configure the BIOS of an HPE UAN Perform this procedure to configure the network interface and boot settings required by HPE UANs.\nBefore the UAN product can be installed on HPE UANs, specific network interface and boot settings must be configured in the BIOS.\nPerform Configure the BMC for UANs with iLO before performing this procedure.\nForce a UAN to reboot into the BIOS.\nIn the following command, UAN_BMC_XNAME is the xname of the BMC of the UAN to configure. Replace USER and PASSWORD with the BMC username and password, respectively.\nncn-m001# ipmitool -U USER -P PASSWORD -H UAN_BMC_XNAME -I lanplus \\ chassis bootdev pxe options=efiboot,persistent Monitor the console of the UAN using either ConMan or the following command:\nncn-m001# ipmitool -U USER -P PASSWORD -H UAN_BMC_XNAME -I \\ lanplus sol activate Refer to the section \u0026ldquo;About the ConMan Containerized Service\u0026rdquo; in the CSM documentation for more information about ConMan.\nPress the ESC and 9 keys to access the BIOS System Utilities when the option appears.\nEnsure that OCP Slot 10 Port 1 is the only port with Boot Mode set to Network Boot. All other ports must have Boot Mode set to Disabled.\nThe settings must match the following example.\n-------------------- System Configuration BIOS Platform Configuration (RBSU) \u0026gt; Network Options \u0026gt; Network Boot Options \u0026gt; PCIe Slot Network Boot Slot 1 Port 1 : Marvell FastLinQ 41000 Series - [Disabled] 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC Slot 1 Port 2 : Marvell FastLinQ 41000 Series - [Disabled] 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC Slot 2 Port 1 : Network Controller [Disabled] OCP Slot 10 Port 1 : Marvell FastLinQ 41000 [Network Boot] Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC OCP Slot 10 Port 2 : Marvell FastLinQ 41000 [Disabled] Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC -------------------- Set the Link Speed to SmartAN for all ports.\n-------------------- System Utilities System Configuration \u0026gt; Main Configuration Page \u0026gt; Port Level Configuration Link Speed [SmartAN] FEC Mode [None] Boot Mode [PXE] DCBX Protocol [Dynamic] RoCE Priority [0] PXE VLAN Mode [Disabled] Link Up Delay [30] Wake On LAN Mode [Enabled] RDMA Protocol Support [iWARP + RoCE] BAR-2 Size [8M] VF BAR-2 Size [256K] --------------------- Set the boot options to match the following example.\n---------------------- System Utilities System Configuration \u0026gt; BIOS/Platform Configuration (RBSU) \u0026gt; Boot Options Boot Mode [UEFI Mode] UEFI Optimized Boot [Enabled] Boot Order Policy [Retry Boot Order Indefinitely] UEFI Boot Settings Legacy BIOS Boot Order ----------------------- Set the UEFI Boot Order settings to match the following example.\nThe order must be:\nUSB Local disks OCP Slot 10 Port 1 IPv4 OCP Slot 10 Port 1 IPv6 ----------------------- System Utilities System Configuration \u0026gt; BIOS/Platform Configuration (RBSU) \u0026gt; Boot Options \u0026gt; UEFI Boot Settings \u0026gt; UEFI Boot Order Press the \u0026#39;+\u0026#39; key to move an entry higher in the boot list and the \u0026#39;-\u0026#39; key to move an entry lower in the boot list. Use the arrow keys to navigate through the Boot Order list. Generic USB Boot SATA Drive Box 1 Bay 1 : VK000480GWTHA SATA Drive Box 1 Bay 2 : VK000480GWTHA SATA Drive Box 1 Bay 3 : VK001920GWTTC SATA Drive Box 1 Bay 4 : VK001920GWTTC OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv4) OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv6) ------------------------- Refer to this Setting the Date and Time in the HPE UEFI documentation to set the correct date and time.\nIf the time is not set correctly, then PXE network booting issues may occur.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/installation_prereqs/configure_the_bmc_for_uans_with_ilo/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=configure the bmc for uans with ilo; given_title=configure the bmc for uans with ilo; menuTitle: \u0026ldquo;Configure the BMC for UANs with iLO\u0026rdquo; date: Sun Mar 26 02:00:54 UTC 2023 draft: false weight: 22 Configure the BMC for UANs with iLO Perform this procedure to enable the IPMI/DCMI settings on an HPE UAN that are necessary to continue UAN product installation on an HPE Cray EX supercomputer.\nPerform the first three steps of Prepare for UAN Product Installation before performing this procedure.\nCreate the SSH tunnel necessary to access the BMC web GUI interface.\nFind the IP or hostname for a UAN.\nCreate an SSH tunnel to the UAN BMC. Run the following command on an external system.\nIn the following example, UAN_MGMT is the UAN iLO interface host name or IP address. NCN is the host name or IP address of a non-compute node on the system. This example assumes that NCN allows port forwarding. USER will usually be root.\n$ ssh -L 8443:UAN_MGMT:443 USER@NCN Wait for SSH to establish the connection.\nOpen https://127.0.0.1:8443 in web browser on the NCN to access the BMC web GUI.\nLog in to the web GUI using default credentials.\nClick Security in the menu on the left side of the screen.\nClick Access Settings in the menu at the top of the screen.\nClick the pencil icon next to Network in the main window area.\nCheck the box next to IPMI/DCMI over LAN.\nEnsure that the remote management settings match the following screenshot.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/installation_prereqs/prepare_for_uan_product_installation/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=prepare for uan product installation; given_title=prepare for uan product installation; menuTitle: \u0026ldquo;Prepare for UAN Product Installation\u0026rdquo; date: Sun Mar 26 02:00:54 UTC 2023 draft: false weight: 21 Prepare for UAN Product Installation Perform this procedure to ready the HPE Cray EX supercomputer for UAN product installation.\nInstall and configure the COS product before performing this procedure.\nVerify that the management network switches are properly configured.\nRefer to the switch configuration procedures in the HPE Cray System Management Documentation.\nEnsure that the management network switches have the proper firmware.\nRefer to the procedure \u0026ldquo;Update the Management Network Firmware\u0026rdquo; in the HPE Cray EX hardware documentation.\nEnsure that the host reservations for the UAN CAN/CHN network have been properly set.\nRefer to the procedure \u0026ldquo;Add UAN CAN IP Addresses to SLS\u0026rdquo; in the HPE Cray EX hardware documentation.\nConfigure the BMC of the UAN.\nPerform Configure the BMC for UANs with iLO if the UAN is a HPE server with an iLO.\nConfigure the BIOS of the UAN.\nPerform Configure the BIOS of an HPE UAN if the UAN is a HPE server with an iLO. Perform Configure the BIOS of a Gigabyte UAN if the UAN is a Gigabyte server. Verify that the firmware for each UAN BMC meets the specifications.\nUse the System Admin Toolkit firmware command to check the current firmware version on a UAN node.\nncn-m001# sat firmware -x BMC_XNAME Repeat the previous six Steps for all UANs.\nUnpackage the file.\nncn-m001# tar zxf uan-PRODUCT_VERSION.tar.gz Navigate into the uan-PRODUCT_VERSION/ directory.\nncn-m001# cd uan-PRODUCT_VERSION/ Run the pre-install goss tests to determine if the system is ready for the UAN product installation.\nThis requires that goss is installed on the node running the tests.\nncn# ./validate-pre-install.sh ............... Total Duration: 1.304s Count: 15, Failed: 0, Skipped: 0 Ensure that the cray-console-node pods are connected to UANs so that they are monitored and their consoles are logged.\nObtain a list of the xnames for all UANs (remove the --subrole argument to list all Application nodes).\nncn# cray hsm state components list --role Application --subrole UAN --format json | jq -r .Components[].ID | sort x3000c0s19b0n0 x3000c0s24b0n0 x3000c0s31b0n0 Obtain a list of the console pods.\nncn# PODS=$(kubectl get pods -n services -l app.kubernetes.io/name=cray-console-node --template \u0026#39;{{range .items}}{{.metadata.name}} {{end}}\u0026#39;) Use conman -q to scan the list of connections being monitored by conman (only UAN xnames are shown for brevity).\nncn# for pod in $PODS; do kubectl exec -n services -c cray-console-node $pod -- conman -q; done x3000c0s19b0n0 x3000c0s24b0n0 x3000c0s31b0n0 If a console connection is not present, the install may continue, but a console connection should be established before attempting to boot the UAN.\nNext, install the UAN product by performing the procedure Install the UAN Product Stream.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/operations/about_uan_configuration/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=about uan configuration; given_title=about uan configuration; menuTitle: \u0026ldquo;About UAN Configuration\u0026rdquo; date: Sun Mar 26 02:00:55 UTC 2023 draft: false weight: 41 About UAN Configuration This section describes the Ansible playbooks and roles that configure UANs.\nUAN configuration overview Configuration of UAN nodes is performed by the Configuration Framework Service (CFS). CFS can apply configuration to both images and nodes. When the configuration is applied to nodes, the nodes must be booted and accessible through SSH over the Node Management Network (NMN).\nThe Ansible roles involved in UAN configuration are listed in the site.yml file in the uan-config-management git repository in VCS. Most of the roles that are specific to image configuration are required for the operation as a UAN and must not be removed from site.yml.\nThe UAN-specific roles involved in post-boot UAN node configuration are:\nuan_disk_config: this role configures the last disk found on the UAN that is smaller than 1TB, by default. That disk will be formatted with a scratch and swap partition mounted at /scratch and /swap, respectively. Each partition is 50% of the disk.\nuan_packages: this role installs any RPM packages listed in the uan-config-management repo.\nuan_interfaces: this role configures the UAN node networking. By default, this role does not configure a default route or the Customer Access Network (CAN or CHN) connection for the HPE Cray EX supercomputer. If CAN or CHN is enabled, the default route will be on the CAN or CHN. Otherwise, a default route must be set up in the customer interfaces definitions. Without the CAN or CHN, there will not be an external connection to the customer site network unless one is defined in the customer interfaces. See Configure Interfaces on UANs.\nNOTE: If a UAN layer is used in the Compute node CFS configuration, the uan_interfaces role will configure the default route on Compute nodes to be on the HSN, if the BICAN System Default Route is set to CHN.\nuan_motd: this role Provides a default message of the day that can be customized by the administrator.\nuan_ldap: this optional role configures the connection to LDAP servers. To disable this role, the administrator must set \u0026lsquo;uan_ldap_setup:no\u0026rsquo; in the \u0026lsquo;uan-config-management\u0026rsquo; VCS repository.\nThe UAN roles in site.yml are required and must not be removed, with exception of uan_ldap if the site is using some other method of user authentication. The uan_ldap may also be skipped by setting the value of uan_ldap_setup to no in a group_vars or host_vars configuration file.\nFor more information about these roles, see UAN Ansible Roles.\nUAN network configuration The uan_interfaces role configures the interfaces on the UAN nodes in three phases:\nSetup and configure the NMN. Gather information from the System Layout Service (SLS) for the NMN. Populate /etc/resolv.conf. Configure the first OCP port on an HPE server, or the first LOM port on a Gigabyte server, as the nmn0 interface. Set up the CAN or CHN, if wanted Gather information from SLS for the CAN or CHN. Configure the route to the CAN or CHN gateway as the default one. Implement the CAN or CHN. CAN: Implement bonded pair On HPE servers, use the second port of the 25Gb OCP card and a second 25Gb card. On Gigabyte servers, use both ports of the 40Gb card. CHN: Implement the CHN interface on the HSN Setup customer-defined networks See Configure Interfaces on UANs for detailed instructions.\nUAN LDAP network requirements LDAP configuration requires either a CAN or another customer-provided network that can route to the LDAP servers. Both such networks route outside of the HPE Cray EX system. If a UAN only has the nmn0 interface configured and active, the UAN cannot route outside of the system.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/operations/boot_uans/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=boot uans; given_title=boot uans; menuTitle: \u0026ldquo;Boot UANs\u0026rdquo; date: Sun Mar 26 02:00:55 UTC 2023 draft: false weight: 48 Boot UANs Perform this procedure to boot UANs using BOS so that they are ready for user logins.\nPerform Create UAN Boot Images before performing this procedure.\nCreate a BOS session to boot the UAN nodes. Replace uan-sessiontemplate-PRODUCT_VERSION in the following command with the ID of the session template created by the initial UAN product installation or the UAN product upgrade process.\nncn-m001# cray bos session create --template-uuid uan-sessiontemplate-PRODUCT_VERSION --operation reboot --format json | tee session.json { \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;jobId\u0026#34;: \u0026#34;boa-89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;session\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d/status\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;status\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; } ], \u0026#34;operation\u0026#34;: \u0026#34;reboot\u0026#34;, \u0026#34;templateUuid\u0026#34;: \u0026#34;uan-sessiontemplate-PRODUCT_VERSION\u0026#34; } Retrieve the BOS session ID from the output of the previous command.\nncn-m001# export BOS_SESSION=$(jq -r \u0026#39;.links[] | select(.rel==\u0026#34;session\u0026#34;) | .href\u0026#39; session.json | cut -d \u0026#39;/\u0026#39; -f4) ncn-m001# echo $BOS_SESSION 89680d0a-3a6b-4569-a1a1-e275b71fce7d Retrieve the Boot Orchestration Agent (BOA) Kubernetes job name for the BOS session.\nncn-m001# BOA_JOB_NAME=$(cray bos session describe $BOS_SESSION --format json | jq -r .job) Retrieve the Kuberenetes pod name for the BOA assigned to run this session.\nncn-m001# BOA_POD=$(kubectl get pods -n services -l job-name=$BOA_JOB_NAME --no-headers -o custom-columns=\u0026#34;:metadata.name\u0026#34;) View the logs for the BOA to track session progress.\nncn-m001# kubectl logs -f -n services $BOA_POD -c boa List the CFS sessions started by the BOA. Skip this step if CFS was not enabled in the boot session template used to boot the UANs.\nIf CFS was enabled in the boot session template, the BOA will initiate a CFS session.\nIn the following command, pending and complete are also valid statuses to filter on.\nncn-m001# cray cfs sessions list --tags bos_session=$BOS_SESSION --status running --format json "
},
{
	"uri": "/docs-uan/en-260-alpha6/operations/build_a_new_uan_image_using_the_cos_recipe/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=build a new uan image using a cos recipe; given_title=build a new uan image using a cos recipe; menuTitle: \u0026ldquo;Build a New UAN Image Using a COS Recipe\u0026rdquo; date: Sun Mar 26 02:00:55 UTC 2023 draft: false weight: 42 Build a New UAN Image Using a COS Recipe Prior to UAN 2.3, a similar copy of the COS recipe was imported with the UAN install. In the UAN 2.3 release, UAN does not install a recipe, and a COS recipe must be used. Additional uan packages will now be installed via CFS and the uan_packages role.\nPerform the following before starting this procedure:\nInstall the COS, Slingshot, and UAN product streams. Initialize the cray administrative CLI. In the COS recipe for 2.2, several dependencies have been removed, this includes Slingshot, DVS, and Lustre. Those packages are now installed during CFS Image Customization. More information on this change is covered in the Create UAN Boot Images procedure.\nIdentify the COS image recipe to base the UAN image on. Select the recipe that matches the version of COS that the compute nodes will be using.\nncn-m001# cray ims recipes list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;compute\u0026#34;))\u0026#39; { \u0026#34;created\u0026#34;: \u0026#34;2021-02-17T15:19:48.549383+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;4a5d1178-80ad-4151-af1b-bbe1480958d1\u0026#34;, \u0026lt;\u0026lt;-- Note this ID \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;3c3b292364f7739da966c9cdae096964\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://ims/recipes/4a5d1178-80ad-4151-af1b-bbe1480958d1/recipe.tar.gz\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;linux_distribution\u0026#34;: \u0026#34;sles15\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cray-shasta-compute-sles15sp3.x86_64-2.2.27\u0026#34;, \u0026#34;recipe_type\u0026#34;: \u0026#34;kiwi-ng\u0026#34; } Save the id of the IMS recipe in an environment variable.\nncn-m001# IMS_RECIPE_ID=4a5d1178-80ad-4151-af1b-bbe1480958d1 Use the IMS recipe id to build the UAN image:\nMore detail on this IMS procedure may be found in the procedure \u0026ldquo;Build an Image Using IMS REST Service\u0026rdquo; in the CSM documentation.\nncn-m001# IMS_PUBLIC_KEY=$(cray ims public-keys list --format json | jq -r \u0026#34;.[] | .id\u0026#34; | head -1) ncn-m001# IMS_ARCHIVE_NAME=$(cray ims recipes describe $IMS_RECIPE_ID --format json | jq -r .name) ncn-m001# IMS_ARCHIVE_NAME=${IMS_ARCHIVE_NAME/compute/uan} ncn-m001# cray ims jobs create --job-type create --public-key-id $IMS_PUBLIC_KEY --image-root-archive-name $IMS_ARCHIVE_NAME --artifact-id $IMS_RECIPE_ID Perform Create UAN Boot Images to run CFS Image Customization on the resulting image.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/operations/configure_interfaces_on_uans/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=configure interfaces on uans; given_title=configure interfaces on uans; menuTitle: \u0026ldquo;Configure Interfaces on UANs\u0026rdquo; date: Sun Mar 26 02:00:55 UTC 2023 draft: false weight: 45 Configure Interfaces on UANs Perform this procedure to configure network interfaces on UANs by editing a configuration file.\nInterface configuration is performed by the uan_interfaces Ansible role. For details on the variables referred to in this procedure, see UAN Ansible Roles.\nIn the command examples of this procedure, PRODUCT_VERSION refers to the current installed version of the UAN product. Replace PRODUCT_VERSION with the UAN version number string when executing the commands.\nNode Management Networking By default, the Node Management Network (NMN) is connected to a single nmn0 interface. If desired, and the system networking is configured to support it, the Node Management Network may be configured as a bonded interface, nmnb0. To configure the NMN as a bonded pair, set uan_nmn_bond to true and set the interfaces to be used in the bond in uan_nmn_bond_slaves as described in UAN Ansible Roles.\nUser Access Networking User access may be configured to use either a direct connection to the UANs from the site\u0026rsquo;s user network, or one of two optional user access networks implemented within the HPE Cray EX system. The two optional networks are the Customer Access Network (CAN) and Customer High Speed Network (CHN). The CAN is a VLAN on the Node Management Network (NMN), whereas the CHN is over the High Speed Network (HSN).\nBy default, a direct connection to the site\u0026rsquo;s user network is assumed and the Admin must define the interface(s) and default route using the customer_uan_interfaces and customer_uan_routes structures. If uan_can_setup is a true value, user access will be over CAN or CHN depending on what the system default route is set to in SLS.\nWhen CAN is set as the system default route in SLS and uan_nmn_bond is false, the bonded CAN interfaces are determined automatically. If uan_nmn_bond is true, the bonded CAN interfaces must be defined by uan_can_bond_slaves (see UAN Ansible Roles). The default route is set to the bonded CAN interface can0.\nWhen CHN is set as the system default route in SLS, the CHN IP is added to hsn0 and the default route is set to the CHN.\nThe Admin may override the CAN/CHN default route by setting uan_customer_default_route to true and defining the default route in customer_uan_routes.\nProcedure Network configuration settings are defined in the uan-config-management VCS repo under the group_vars/ROLE_SUBROLE/ or host_vars/XNAME/ directories, where ROLE_SUBROLE must be replaced by the role and subrole assigned for the node in HSM, and XNAME with the xname of the node. Values under group_vars/ROLE_SUBROLE/ apply to all nodes with the given role and subrole. Values under the host_vars/XNAME/ apply to the specific node with the xname and will override any values set in group_vars/ROLE_SUBROLE/. A yaml file is used by the Configuration Framwork Service (CFS). The examples in this procedure use customer_net.yml, but any filename may be used. Admins must create this yaml file and use the variables described in this procedure.\nIf the HPE Cray EX CAN or CHN is desired, set the uan_can_setup variable to yes in the yaml file. The UAN will be configured to use the CAN or CHN based on what the BICAN System Default Route is set to in SLS.\nObtain the password for the crayvcs user.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode Log in to ncn-w001.\nCreate a copy of the Git configuration. Enter the credentials for the crayvcs user when prompted.\nncn-w001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git Change to the uan-config-management directory.\nncn-w001# cd uan-config-management Edit the yaml file, (customer_net.yml, for example), in either the group_vars/ROLE_SUBROLE/ or host_vars/XNAME directory and configure the values as needed.\nTo enable bonded NMN interfaces:\n## uan_nmn_bond # Set uan_nmn_bond to \u0026#39;yes\u0026#39; if the site will # implement a bonded NMN connection. # By default, uan_nmn_bond is set to \u0026#39;no\u0026#39;. uan_nmn_bond: yes ## uan_nmn_bond_slaves # These are the default NMN bond slaves. They may need to be # changed based on the actual system hardware configuration. uan_nmn_bond_slaves: - \u0026#34;ens10f0\u0026#34; - \u0026#34;ens1f0\u0026#34; To set up CAN or CHN:\n## uan_can_setup # Set uan_can_setup to \u0026#39;yes\u0026#39; if the site will # use the Shasta CAN or CHN network for user access. # By default, uan_can_setup is set to \u0026#39;no\u0026#39;. uan_can_setup: yes ## uan_can_bond_slaves # This variable only applies when the system default route is CAN # and `uan_nmn_bond` is true. # These are the default CAN bond slaves. They may need to be # changed based on the actual system hardware configuration. uan_can_bond_slaves: - \u0026#34;ens10f1\u0026#34; - \u0026#34;ens1f1\u0026#34; To allow a custom default route when CAN or CHN is selected:\n## uan_customer_default_route # Allow a custom default route when CAN or CHN is selected. uan_customer_default_route: no To define interfaces:\n## Customer defined networks ifcfg-X # customer_uan_interfaces is as list of interface names used for constructing # ifcfg-\u0026lt;customer_uan_interfaces.name\u0026gt; files. The setting dictionary is where # any desired ifcfg fields are defined. The field name will be converted to # uppercase in the generated ifcfg-\u0026lt;name\u0026gt; file. # # NOTE: Interfaces should be defined in order of dependency. # ## Example ifcfg fields, not exhaustive: # bootproto: \u0026#39;\u0026#39; # device: \u0026#39;\u0026#39; # dhcp_hostname: \u0026#39;\u0026#39; # ethtool_opts: \u0026#39;\u0026#39; # gateway: \u0026#39;\u0026#39; # hwaddr: \u0026#39;\u0026#39; # ipaddr: \u0026#39;\u0026#39; # master: \u0026#39;\u0026#39; # mtu: \u0026#39;\u0026#39; # peerdns: \u0026#39;\u0026#39; # prefixlen: \u0026#39;\u0026#39; # slave: \u0026#39;\u0026#39; # srcaddr: \u0026#39;\u0026#39; # startmode: \u0026#39;\u0026#39; # userctl: \u0026#39;\u0026#39; # bonding_master: \u0026#39;\u0026#39; # bonding_module_opts: \u0026#39;\u0026#39; # bonding_slave0: \u0026#39;\u0026#39; # bonding_slave1: \u0026#39;\u0026#39; # # customer_uan_interfaces: # - name: \u0026#34;net1\u0026#34; # settings: # bootproto: \u0026#34;static\u0026#34; # device: \u0026#34;net1\u0026#34; # ipaddr: \u0026#34;1.2.3.4\u0026#34; # startmode: \u0026#34;auto\u0026#34; # - name: \u0026#34;net2\u0026#34; # settings: # bootproto: \u0026#34;static\u0026#34; # device: \u0026#34;net2\u0026#34; # ipaddr: \u0026#34;5.6.7.8\u0026#34; # startmode: \u0026#34;auto\u0026#34; customer_uan_interfaces: [] To define interface static routes:\n## Customer defined networks ifroute-X # customer_uan_routes is as list of interface routes used for constructing # ifroute-\u0026lt;customer_uan_routes.name\u0026gt; files. # # customer_uan_routes: # - name: \u0026#34;net1\u0026#34; # routes: # - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; # - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; # - name: \u0026#34;net2\u0026#34; # routes: # - \u0026#34;default 10.103.8.20 255.255.255.255 - table 3\u0026#34; # - \u0026#34;10.103.8.128/25 10.103.8.20 255.255.255.255 net2\u0026#34; customer_uan_routes: [] To define the rules:\n## Customer defined networks ifrule-X # customer_uan_rules is as list of interface rules used for constructing # ifrule-\u0026lt;customer_uan_routes.name\u0026gt; files. # # customer_uan_rules: # - name: \u0026#34;net1\u0026#34; # rules: # - \u0026#34;from 10.1.0.0/16 lookup 1\u0026#34; # - name: \u0026#34;net2\u0026#34; # rules: # - \u0026#34;from 10.103.8.0/24 lookup 3\u0026#34; customer_uan_rules: [] To define the global static routes:\n## Customer defined networks global routes # customer_uan_global_routes is as list of global routes used for constructing # the \u0026#34;routes\u0026#34; file. # # customer_uan_global_routes: # - routes: # - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; # - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; customer_uan_global_routes: [] Add the change from the working directory to the staging area.\nncn-w001# git add -A Commit the file to the master branch.\nncn-w001# git commit -am \u0026#39;Added UAN interfaces\u0026#39; Push the commit.\nncn-w001# git push Obtain the commit ID for the commit pushed in the previous step.\nncn-m001# git rev-parse --verify HEAD Update any CFS configurations used by the UANs with the commit ID from the previous step.\na. Download the JSON of the current UAN CFS configuration to a file.\nThis file will be named uan-config-PRODUCT_VERSION.json. Replace PRODUCT_VERSION with the current installed UAN version.\nncn-m001# cray cfs configurations describe uan-config-PRODUCT_VERSION \\ --format=json \u0026gt;uan-config-PRODUCT_VERSION.json b. Remove the unneeded lines from the JSON file.\nThe lines to remove are: - the `lastUpdated` line - the last `name` line These must be removed before uploading the modified JSON file back into CFS to update the UAN configuration. ```bash ncn-m001# cat uan-config-PRODUCT_VERSION.json { \u0026quot;lastUpdated\u0026quot;: \u0026quot;2021-03-27T02:32:10Z\u0026quot;, \u0026quot;layers\u0026quot;: [ { \u0026quot;cloneUrl\u0026quot;: \u0026quot;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026quot;, \u0026quot;commit\u0026quot;: \u0026quot;aa5ce7d5975950ec02493d59efb89f6fc69d67f1\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;uan-integration-PRODUCT_VERSION\u0026quot;, \u0026quot;playbook\u0026quot;: \u0026quot;site.yml\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;uan-config-2.0.1-full\u0026quot; } ``` c. Replace the commit value in the JSON file with the commit ID obtained in the previous Step.\nThe name value after the commit line may also be updated to match the new UAN product version, if desired. This is not necessary as CFS does not use this value for the configuration name. ```bash { \u0026quot;layers\u0026quot;: [ { \u0026quot;cloneUrl\u0026quot;: \u0026quot;https://api-gw-service-nmn.local/vcs/cray/uan-configmanagement.git\u0026quot;, \u0026quot;commit\u0026quot;: \u0026quot;aa5ce7d5975950ec02493d59efb89f6fc69d67f1\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;uan-integration-PRODUCT_VERSION\u0026quot;, \u0026quot;playbook\u0026quot;: \u0026quot;site.yml\u0026quot; } ] } ``` d. Create a new UAN CFS configuration with the updated JSON file.\nThe following example uses uan-config-PRODUCT_VERSION for the name of the new CFS configuration, to match the JSON file name.\n```bash ncn-m001# cray cfs configurations update uan-config-PRODUCT_VERSION \\ --file uan-config-PRODUCT_VERSION.json ``` e. Tell CFS to apply the new configuration to UANs by repeating the following command for each UAN. Replace UAN_XNAME in the command below with the name of a different UAN each time the command is run.\n```bash ncn-m001# cray cfs components update --desired-config uan-config-PRODUCT_VERSION \\ --enabled true --format json UAN_XNAME ``` Reboot the UAN with the Boot Orchestration Service (BOS).\nThe new interfaces will be available when the UAN is rebooted. Replace the UAN_SESSION_TEMPLATE value with the BOS session template name for the UANs.\nncn-w001# cray bos v1 session create \\ --template-uuid UAN_SESSION_TEMPLATE --operation reboot Verify that the desired networking configuration is correct on each UAN.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/operations/configure_pluggable_authentication_modules_pam_on_uans/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=configure pluggable authentication modules (pam) on uans; given_title=configure pluggable authentication modules (pam) on uans; menuTitle: \u0026ldquo;Configure Pluggable Authentication Modules (PAM) on UANs\u0026rdquo; date: Sun Mar 26 02:00:56 UTC 2023 draft: false weight: 46 Configure Pluggable Authentication Modules (PAM) on UANs Perform this procedure to configure PAM on UANs. This enables dynamic authentication support for system services.\nIntialize and configure the Cray command line interface (CLI) tool on the system. See \u0026ldquo;Configure the Cray Command Line Interface (CLI)\u0026rdquo; in the CSM documentation for more information.\nVerify that the Gitea Version Control Service (VCS) is running.\nncn-m001# kubectl get pods --all-namespaces | grep vcs services gitea-vcs-f57c54c4f-j8k4t 2/2 Running 1 11d services gitea-vcs-postgres-0 2/2 Running 0 11d Retrieve the initial Gitea login credentials for the crayvcs username.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode These credentials can be modified in the vcs_user role prior to installation or can be modified after logging in.\nUse an external web browser to verify the Ansible plays are available on the system.\nThe URL will take on the following format:\nhttps://api.SYSTEM-NAME.DOMAIN-NAME/vcs\nClone the system Version Control Service (VCS) repository to a directory on the system.\nncn-w001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git Change to the uan-config-management directory.\nncn-w001# cd uan-config-management Make a new directory for the PAM configuration.\na. Create a group_vars/all directory if making changes to all UANs.\n```bash ncn-w001# mkdir -p group_vars/all ncn-w001# cd group_vars/all ``` b. Create a host_vars/XNAME directory if the change is node specific.\n```bash ncn-w001# mkdir -p host_vars/XNAME ncn-w001# cd host_vars/XNAME ``` Configure PAM.\nThe default path is /etc/pam.d/, so only the module file name is required.\n# vi pam.yml --- uan_pam_modules: - name: pam_module_file_name lines: - \u0026#34;add this line to pam module file_name\u0026#34; - \u0026#34;add another line to pam module file_name\u0026#34; - name: another_pam_module_file_name lines: - \u0026#34;add this line to another_pam_module_file_name\u0026#34; The following is an example of adding the line \u0026quot;account required pam\\_access.so\u0026quot; to the /etc/pam.d/common-account PAM file. The \\t is used to place a tab between account required and pam\\_access.so to match the formatting of the common-account file contents. The quotes are required in the strings used in the lines filed.\n--- uan_pam_modules: - name: common-account lines: - \u0026#34;account required\\tpam_access.so\u0026#34; Add the change from the working directory to the staging area.\nAll UANs:\nncn-w001# git add group_vars/all/pam.yml Node specific:\nncn-w001# git add host_vars/XNAME/pam.yml Commit the file to the master branch.\nncn-w001# git commit -am \u0026#39;Added PAM configuration\u0026#39; Push the commit.\nncn-w001# git push If prompted, use the Gitea login credentials.\nReboot the UAN(s) with the Boot Orchestration Service (BOS).\nncn-w001# cray bos session create \\ --template-uuid UAN_SESSION_TEMPLATE --operation reboot "
},
{
	"uri": "/docs-uan/en-260-alpha6/operations/create_uan_boot_images/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=create uan boot images; given_title=create uan boot images; menuTitle: \u0026ldquo;Create UAN Boot Images\u0026rdquo; date: Sun Mar 26 02:00:56 UTC 2023 draft: false weight: 43 Create UAN Boot Images Perform Build a New UAN Image Using a COS Recipe before performing this procedure.\nThis procedure updates the configuration management git repository to match the installed version of the UAN product. That updated configuration is then used to create UAN boot images and a BOS session template.\nUAN specific configuration, and other required configurations related to UANs are covered in this topic. Refer to product-specific documentation for further information on configuring other HPE products (for example, workload managers and the HPE Cray Programming Environment) that may be configured on the UANs.\nThis is the overall workflow for preparing UAN images to boot UANs:\nClone the UAN configuration git repository and create a branch based on the branch imported by the UAN installation. Update the configuration content and push the changes to the newly created branch. Use Shasta Admin Toolkit (SAT) command sat bootprep, to automate the creation of IMS image, CFS configurations, and BOS session templates. Once the UAN BOS session template is created, the UANs will be ready to be booted by a BOS session.\nReplace PRODUCT_VERSION and CRAY_EX_HOSTNAME in the example commands in this procedure with the current UAN product version installed (See Step 1) and the hostname of the HPE Cray EX system, respectively.\nPREPARE CFS CONFIGURATION\nObtain the artifact IDs and other information from the cray-product-catalog Kubernetes ConfigMap. Record the following information:\nthe clone_url the import_branch value Upon successful installation of the UAN product, the UAN configuration is cataloged in this ConfigMap. This information is required for this procedure.\nPRODUCT_VERSION will be replaced by a numbered version string, such as 2.1.7 or 2.3.0.\nncn-m001# kubectl get cm -n services cray-product-catalog -o json | jq -r .data.uan PRODUCT_VERSION: configuration: clone_url: https://vcs.CRAY_EX_HOSTNAME/vcs/cray/uan-config-management.git commit: 6658ea9e75f5f0f73f78941202664e9631a63726 import_branch: cray/uan/PRODUCT_VERSION import_date: 2021-02-02 19:14:18.399670 ssh_url: git@vcs.CRAY_EX_HOSTNAME:cray/uan-config-management.git Optional Generate the password hash for the root user. Replace PASSWORD with the root password you wish to use. If an upgrade or image rebuild is being performed, the root password may have already been added to vault.\nncn-m001# openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc ./A-Za-z0-9 | head -c4) PASSWORD Optional Obtain the HashiCorp Vault root token.\nncn-m001# kubectl get secrets -n vault cray-vault-unseal-keys -o jsonpath=\u0026#39;{.data.vault-root}\u0026#39; | base64 -d; echo Optional Write the password hash obtained in Step 2 to the HashiCorp Vault.\nThe vault login command will request a token. That token value is the output of the previous step. The vault read secret/uan command verifies that the hash was stored correctly. This password hash will be written to the UAN for the root user by CFS.\nncn-m001# kubectl exec -it -n vault cray-vault-0 -c vault -- sh export VAULT_ADDR=http://cray-vault:8200 vault login vault write secret/uan root_password=\u0026#39;HASH\u0026#39; vault read secret/uan Optional Write any uan_ldap sensitive data, such as the ldap_default_authtok value, to the HashiCorp Vault.\nThe vault login command will request a token. That token value is the output of the Step 3. The vault read secret/uan_ldap command verifies that the uan_ldap data was stored correctly. Any values stored here will be written to the UAN /etc/sssd/sssd.conf file in the [domain] section by CFS.\nThis example shows storing a value for ldap_default_authtok. If more than one variable needs to be stored, they must be written in space separated key=value pairs on the same vault write secret/uan_ldap command line.\nncn-m001# kubectl exec -it -n vault cray-vault-0 -- sh export VAULT_ADDR=http://cray-vault:8200 vault login vault write secret/uan_ldap ldap_default_authtok=\u0026#39;TOKEN\u0026#39; vault read secret/uan_ldap Obtain the password for the crayvcs user from the Kubernetes secret for use in the next command.\nncn-m001# VCS_USER=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_username}} | base64 --decode) VCS_PASS=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode) Clone the UAN configuration management repository. Replace CRAY_EX_HOSTNAME in the clone url with api-gw-service-nmn.local when cloning the repository.\nThe repository is in the VCS/Gitea service and the location is reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.clone_url key. The CRAY_EX_HOSTNAME from the clone_url is replaced with api-gw-service-nmn.local in the command that clones the repository.\nncn-m001# git clone https://$VCS_USER:$VCS_PASS@api-gw-service-nmn.local/vcs/cray/uan-config-management.git . . . ncn-m001# cd uan-config-management \u0026amp;\u0026amp; git checkout cray/uan/PRODUCT_VERSION \u0026amp;\u0026amp; git pull Branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; set up to track remote branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; from \u0026#39;origin\u0026#39;. Already up to date. Create a branch using the imported branch from the installation to customize the UAN image.\nThis will be reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.import_branch key under the UAN section. The format is cray/uan/PRODUCT_VERSION. In this guide, an integration-PRODUCT_VERSION branch is used for examples to comply with IUF defaults, but the name can be any valid git branch name configured to be used by IUF.\nModifying the cray/uan/PRODUCT_VERSION branch that was created by the UAN product installation is not allowed by default.\nncn-m001# git checkout -b integration-PRODUCT_VERSION \u0026amp;\u0026amp; git merge cray/uan/PRODUCT_VERSION Switched to a new branch \u0026#39;integration-PRODUCT_VERSION\u0026#39; Already up to date. Apply any site-specific customizations and modifications to the Ansible configuration for the UAN nodes and commit the changes.\nThe default Ansible play to configure UAN nodes is site.yml in the base of the uan-config-management repository. The roles that are executed in this play allow for custom configuration as required for the system.\nConsult the individual Ansible role README.md files in the uan-config-management repository roles directory to configure individual role variables. Roles prefixed with uan_ are specific to UAN configuration and include network interfaces, disk, LDAP, software packages, and message of the day roles.\nNOTE: Admins must ensure the uan_can_setup variable is set to the correct value for the site. This variable controls how the nodes are configured for user access. When uan_can_setup is yes, user access is over the CAN or CHN, based on the BICAN System Default Route setting in SLS. When uan_can_setup is no, the Admin must configure the user access interface and default route. See Configure Interfaces on UANs\nWarning: Never place sensitive information such as passwords in the git repository.\nThe following example shows how to add a vars.yml file containing site-specific configuration values to the Application_UAN group variable location.\nThese and other Ansible files do not necessarily need to be modified for UAN image creation. See About UAN Configuration for instructions for site-specific UAN configuration, including CAN/CHN configuration.\nncn-m001# vim group_vars/Application_UAN/vars.yml ncn-m001# git add group_vars/Application_UAN/vars.yml ncn-m001# git commit -m \u0026#34;Add vars.yml customizations\u0026#34; [integration-PRODUCT_VERSION ecece54] Add vars.yml customizations 1 file changed, 1 insertion(+) create mode 100644 group_vars/Application_UAN/vars.yml Push the changes to the repository using the proper credentials, including the password obtained previously.\nncn-m001# git push --set-upstream origin integration-PRODUCT_VERSION Username for \u0026#39;https://api-gw-service-nmn.local\u0026#39;: crayvcs Password for \u0026#39;https://crayvcs@api-gw-service-nmn.local\u0026#39;: . . . remote: Processed 1 references in total To https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git * [new branch] integration-PRODUCT_VERSION -\u0026gt; integration-PRODUCT_VERSION Branch \u0026#39;integration-PRODUCT_VERSION\u0026#39; set up to track remote branch \u0026#39;integration-PRODUCT_VERSION\u0026#39; from \u0026#39;origin\u0026#39;. The configuration parameters have been stored in a branch in the UAN git repository. The next phase of the process uses sat bootprep to handle creating the CFS configurations, IMS images, and BOS sessiontemplates for UANs.\nCREATE UAN IMAGES\nWith Shasta Admin Toolkit (SAT) version 2.2.16 and later, it is recommended that administrators create an input file for use with sat bootprep. Use the following command to determine which version of SAT is installed:\nncn-m001# sat showrev --products --filter \u0026#39;product_name=\u0026#34;sat\u0026#34;\u0026#39; A sat bootprep input file will have three sections: configurations, images, and session_templates. These sections create CFS configurations, IMS images, and BOS session templates respectively. Each section may have multiple elements to create more than one CFS, IMS, or BOS artifact. The format is similar to the input files for CFS, IMS, and BOS, but SAT will automate the process with fewer steps. Follow the subsections below to create a UAN bootprep input file.\nRefer to HPE Cray EX System Software Getting Started Guide for further information on configuring other HPE products, as this procedure documents only the required configuration of the UAN.\nSAT Bootprep Configuration\nThe SAT bootprep input file should have a configuration section that specifies each layer to be included in the CFS configuration for the UAN images for image customization and node personalization. This section will result in a CFS configuration named uan-config. The versions of each layer may be gathered using sat showrev.\nNote that the Slingshot Host Software CFS layer is listed first. This is required as the UAN layer will attempt to install DVS and Lustre packages that require SHS be installed first. The correct playbook for Cassini or Mellanox must also be specified. Consult the Slingshot Host Software documentation for more information.\nBeginning with UAN version 2.6.0, CFS configuration roles which are provided by COS are now defined as two separate COS configuration layers as shown in the example below. Prior to UAN version 2.6.0, these roles were included in the UAN configuration layer. Separating these roles into COS layers allows COS updates to be independent from UAN updates.\nconfigurations: - name: uan-config layers: - name: shs-mellanox_install-integration playbook: shs_mellanox_install.yml product: name: slingshot-host-software version: 2.0.0 branch: integration # - name: shs-cassini_install-integration # playbook: shs_cassini_install.yml # product: # name: slingshot-host-software # version: 2.0.0 # branch: integration - name: cos-application-integration playbook: cos-application.yml product: name: cos version: 2.5 - name: csm-packages-integration playbook: csm_packages.yml product: name: csm version: 1.4 - name: uan playbook: site.yml product: name: uan version: 2.6.0 branch: integration-PRODUCT_VERSION ... add configuration layers for other products here, if desired ... - name: uan-rebuild-initrd playbook: rebuild-initrd.yml product: name: uan version: 2.6.0 branch: integration-PRODUCT_VERSION SAT Bootprep Image\nThe SAT bootprep input file should have a section that specifies which IMS images to create for UAN nodes. UANs are built using the COS recipe, so the section below specifies which image recipe to use based on what is provided by COS. To determine which COS recipes are available run the following command:\nncn-m001# sat showrev --products --filter \u0026#39;product_name=\u0026#34;cos\u0026#34;\u0026#39; This example will create an IMS image with the name cray-shasta-uan-sles15sp3.x86_64-2.3.25. An appropriate name should be used to correctly identify the UAN image being built. Also note that the CFS configuration uan-config is being referenced so that CFS image customization will be run using that configuration along with the specified node groups.\nimages: - name: cray-shasta-uan-sles15sp3.x86_64-2.3.25 ims: is_recipe: true name: cray-shasta-compute-sles15sp3.x86_64-2.3.25 configuration: uan-config configuration_group_names: - Application - Application_UAN SAT Bootprep Session Template\nThe final section of the SAT bootprep input file creates BOS session templates. This section references the named IMS image that sat bootprep generates, as well as a CFS configuration. The boot_sets key \u0026ldquo;uan\u0026rdquo; may be changed as needed. If there are more than one boot_sets in the session template, each key will need to be unique.\nsession_templates: - name: uan-2.4.0 image: cray-shasta-uan-sles15sp3.x86_64-2.3.25 configuration: uan-config bos_parameters: boot_sets: uan: kernel_parameters: spire_join_token=${SPIRE_JOIN_TOKEN} node_roles_groups: - Application_UAN Run SAT Bootprep\nInitiate the sat bootprep command to generate the configurations and artifacts needed to boot UANs. This command may take some time as it will initiate IMS image creation and CFS image customization.\nncn-m001# sat bootprep run uan-bootprep.yaml If changes are necessary to complete sat bootprep with the provided input file, make adjustments to the CFS layers or input file as needed and rerun the sat bootprep command. If any artifacts are going to be overwritten, SAT will prompt for confirmation before taking action. This is useful when making CFS changes as SAT will automatically configure the layers to use the latest git commits if the branches are specified correctly.\nOnce sat bootprep completes successfully, save the input file to a known location. This input file will be useful to regenerate artifacts as changes are made or different product layers are added.\nFinally, perform Boot UANs to boot the UANs with the new BOS session template.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/operations/cve-2023-0461/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=mitigation for cve-2023-0461; given_title=mitigation for cve-2023-0461; menuTitle: \u0026ldquo;Mitigation for CVE-2023-0461\u0026rdquo; date: Sun Mar 26 02:00:55 UTC 2023 draft: false weight: 49 Mitigation for CVE-2023-0461 Description There is a use-after-free vulnerability in the Linux Kernel which can be exploited to achieve local privilege escalation. To reach the vulnerability kernel configuration flag CONFIG_TLS or CONFIG_XFRM_ESPINTCP has to be configured, but the operation does not require any privilege. There is a use-after-free bug of icsk_ulp_data of a struct inet_connection_sock. When CONFIG_TLS is enabled, user can install a tls context (struct tls_context) on a connected tcp socket. The context is not cleared if this socket is disconnected and reused as a listener. If a new socket is created from the listener, the context is inherited and vulnerable. The setsockopt TCP_ULP operation does not require any privilege. We recommend upgrading past commit 2c02d41d71f90a5168391b6a5f2954112ba2307c\nCVE-2023-0461\nCurrent status from SUSE: https://www.suse.com/security/cve/CVE-2023-0461.html\nMitigation While the underlying CVE is being addressed by SUSE, UANs can mitigate this issue by unbonding the CAN (if it is being used), and unloading the TLS kernel module after blocking the kernel from being loaded again.\nThe mitigation script provided below will perform the following actions:\nSelect the first BONDING_SLAVE0 from ifcfg-bond0 if it exists Replace bond0 in ifcfg-can0 with the BONDING_SLAVE0 interface Reload interfaces Add a blacklist for the tls module Unload bonding and tls kernel modules provided mlx5_core is not present Fail if mlx5_core is detected to highlight that the mititagtion failed Important: This mitigation is intend for UANs that meet the following criteria:\nThe UANs are connected to HPE Aruba Networking switches (lcap-individual must be set for other switch types to allow for unbonded CAN connections) The UANs use HPE Slingshot HSN NICs and not Mellanox ConnectX-5 HSN NICs. Procedure Update the active CFS configuration with the following changes so that Node Personalization applies the change the UANs:\ndiff --git a/mitigate-uan-cve-2023-0461.sh b/mitigate-uan-cve-2023-0461.sh new file mode 100755 index 0000000..1ced634 --- /dev/null +++ b/mitigate-uan-cve-2023-0461.sh @@ -0,0 +1,32 @@ +#!/bin/bash + +# Select the BONDING_SLAVE0 as the unbonded interface and create a new ifcfg file +if [ -f /etc/sysconfig/network/ifcfg-bond0 ] \u0026amp;\u0026amp; grep -q BONDING_SLAVE0 /etc/sysconfig/network/ifcfg-bond0; then + ifname=$(grep BONDING_SLAVE0 /etc/sysconfig/network/ifcfg-bond0 | awk -F= \u0026#39;{print $2}\u0026#39; | tr -d \\\u0026#39;\\\u0026#34;) + sed -i -e \u0026#34;s/bond0/$ifname/g\u0026#34; /etc/sysconfig/network/ifcfg-can0 + cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/sysconfig/network/ifcfg-$ifname +STARTMODE=\u0026#39;auto\u0026#39; +BOOTPROTO=\u0026#39;static\u0026#39; +EOF + rm /etc/sysconfig/network/ifcfg-bond0 +fi + +# Reload interfaces to bring up the unbonded can +wicked ifreload all + +# Create a blacklist file and unload bonding tls +cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/modprobe.d/66-blacklist-tls.conf +blacklist tls +install tls /bin/true +EOF + +# This will fail on mellanox systems as mlx5_core depends on tls +# Failure of this script will in +if modinfo mlx5_core \u0026amp;\u0026gt; /dev/null; then + rmmod bonding tls +else + echo \u0026#34;Can\u0026#39;t rmmod tls as mlx5_core depends on it\u0026#34; + exit 1 +fi + +exit 0 diff --git a/site.yml b/site.yml index 674ab30..ad166cf 100644 --- a/site.yml +++ b/site.yml @@ -92,6 +92,8 @@ - uan_interfaces - uan_ldap - uan_hardening + - name: Bond interface mitigation + script: mitigate-uan-cve-2023-0461.sh Once UAN images are built to address the CVE, this mitigation script should be removed.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/operations/mount_a_new_file_system_on_an_uan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=mount a new file system on a uan; given_title=mount a new file system on a uan; menuTitle: \u0026ldquo;Mount a New File System on a UAN\u0026rdquo; date: Sun Mar 26 02:00:56 UTC 2023 draft: false weight: 44 Mount a New File System on a UAN Perform this procedure to create a mount point for a new file system on a UAN.\nPerform Steps 1-9 of Create UAN Boot Images.\nCreate a directory for Application role nodes.\nncn-w001# mkdir -p group_vars/Application Define the home directory information for the new file system in the filesystems.yml file.\nncn-w001# vi group_vars/Application/filesystems.yml --- filesystems: - src: 10.252.1.1:/home mount_point: /home fstype: nfs4 opts: rw,noauto state: mounted Add the change from the working directory to the staging area.\nncn-w001# git add -A Commit the file to the working branch.\nncn-w001# git commit -am \u0026#39;Added file system info\u0026#39; Resume Create UAN Boot Images at Step 10.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/troubleshooting/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=troubleshooting; given_title=troubleshooting; menuTitle: \u0026ldquo;troubleshooting\u0026rdquo; date: Sun Mar 26 02:01:01 UTC 2023 draft: false weight: 70 troubleshooting Topics: Troubleshoot UAN Boot Issues Troubleshoot UAN CFS and Network Configuration Issues Troubleshoot UAN Disk Configuration Issues "
},
{
	"uri": "/docs-uan/en-260-alpha6/troubleshooting/troubleshoot_uan_boot_issues/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=troubleshoot uan boot issues; given_title=troubleshoot uan boot issues; menuTitle: \u0026ldquo;Troubleshoot UAN Boot Issues\u0026rdquo; date: Sun Mar 26 02:00:57 UTC 2023 draft: false weight: 71 Troubleshoot UAN Boot Issues The UAN boot process BOS boots UANs. BOS uses session templates to define various parameters such as:\nWhich nodes to boot Which image to boot Kernel parameters Whether to perform post-boot configuration (Node Personalization) of the nodes by CFS. Which CFS configuration to use if Node Personalization is enabled. UAN boots are performed in three phases:\nPXE booting an iPXE binary that will load the initrd of the UAN image that will boot. Booting the initrd (dracut) image which configures the UAN for booting the UAN image. This process consists of two phases. Configuring the UAN node to use the Content Projection Service (CPS) and Data Virtualization Service (DVS). These services manage the UAN image rootfs mounting and make that image available to the UAN nodes. Mounting the rootfs Booting the UAN image rootfs. PXE Issues Most PXE boot failures are the result of misconfigured network switches and/or BIOS settings. The UAN must PXE boot over the Node Management Network (NMN) and the switches must be configured to allow connectivity to the NMN. The cable for the NMN must be connected to the first port of the OCP card on HPE DL325 and DL385 servers or to the first port of the built-in LAN-On-Motherboard (LOM) on Gigabyte servers. See \u0026ldquo;Prepare for UAN Product Installation\u0026rdquo; in the UAN Installation Guide for details on the switch and BIOS settings required to configure the UAN for PXE booting.\nUANs may fail to boot when the BIOS EFITIME is too far away from the time on management nodes. If there are x509 certificate problems, check that the BIOS time is correct. See \u0026ldquo;Configure the BIOS of an HPE UAN\u0026rdquo; or \u0026ldquo;Configure the BIOS of a Gigabyte UAN\u0026rdquo; in the UAN Installation Guide for examples of checking settings in the BIOS.\nInitrd (Dracut) Issues Dracut failures are often caused by the wrong interface being named nmn0, or to multiple entries for the UAN xname in DNS. The latter is a result of multiple interfaces making DHCP requests. Either condition can cause IP address mismatches in the dvs_node_map. DNS configures entries based on DHCP leases.\nWhen dracut starts, it renames the network device named by the ifmap=netX:nmn0 kernel parameter to nmn0. This interface is the only one dracut will enable DHCP on. The ip=nmn0:dhcp kernel parameter limits dracut to DHCP only nmn0. The ifmap value must be set correctly in the kernel_parameters field of the BOS session template.\nSee Create UAN Boot Images for details on how to configure the BOS session template. For UAN nodes that have more than one PCI card installed, ifmap=net2:nmn0 is the correct setting. If only one PCI card is installed, ifmap=net0:nmn0 is normally the correct setting.\nUANs require CPS and DVS to boot from images. These services are configured in dracut to retrieve the rootfs and mount it. If the image fails to download, check that DVS and CPS are both healthy, and DVS is running on all worker nodes. Run the following commands to check DVS and CPS:\nncn-m001# kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; --no-headers ncn-w001 ncn-w002 ncn-m001# for node in `kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; \\ --no-headers`; do ssh $node \u0026#34;lsmod | grep \u0026#39;^dvs \u0026#39;\u0026#34; done ncn-w001 ncn-w002 If DVS and CPS are both healthy, then both of these commands will return all the worker NCNs in the HPE Cray EX system.\nImage Boot Issues Once dracut exits, the UAN will boot the rootfs image. Failures seen in this phase tend to be failures of spire-agent, cfs-state-reporter, or both. The cfs-state-reporter tells BOA that the node is ready and allows BOA to start CFS for Node Personalization. If cfs-state-reporter does not start, check if the spire-agent has started. The cfs-state-reporter depends on the spire-agent. Running systemctl status spire-agent will show that that service is enabled and running if there are no issues with that service. Similarly, running systemctl status cfs-state-reporter will show a status of SUCCESS.\nuan# systemctl status spire-agent  spire-agent.service - SPIRE Agent Loaded: loaded (/usr/lib/systemd/system/spire-agent.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2021-02-24 14:27:33 CST; 19h ago Main PID: 3581 (spire-agent) Tasks: 57 CGroup: /system.slice/spire-agent.service 3581 /usr/bin/spire-agent run -expandEnv -config /root/spire/conf/spire-agent.conf uan# systemctl status cfs-state-reporter  cfs-state-reporter.service - cfs-state-reporter reports configuration level of the system Loaded: loaded (/usr/lib/systemd/system/cfs-state-reporter.service; enabled; vendor preset: enabled) Active: inactive (dead) since Wed 2021-02-24 14:29:51 CST; 19h ago Main PID: 3827 (code=exited, status=0/SUCCESS) There may be errors related to failing to load kernel modules during the boot:\nFAILED Failed to start Load Kernel Modules. See \u0026#39;systemctl status systemd-modules-load.service\u0026#39; for details. Provided the UAN boots and completes post boot customizations, these messages may be ignored.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/troubleshooting/troubleshoot_uan_cfs_and_network_configuration_issues/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=troubleshoot uan cfs and network configuration issues; given_title=troubleshoot uan cfs and network configuration issues; menuTitle: \u0026ldquo;Troubleshoot UAN CFS and Network Configuration Issues\u0026rdquo; date: Sun Mar 26 02:00:57 UTC 2023 draft: false weight: 72 Troubleshoot UAN CFS and Network Configuration Issues Examine the UAN CFS pod logs to help troubleshoot CFS and networking issues on UANs.\nRead About UAN Configuration before starting this procedure.\nObtain the name of the CFS session that failed by running the following command on a management or worker NCN:\nThis example sorts the list of CFS sessions so that the most recent one is at the bottom.\nncn# kubectl -n services get pods --sort-by=.metadata.creationTimestamp | grep ^cfs View the Ansible log of the CFS session found in the previous step (CFS_SESSION in the following example). Use the information in log to guide troubleshooting.\nncn# kubectl -n services logs -f -c ansible-0 CFS_SESSION Optional: Troubleshoot uan_interfaces issues by logging into the affected node (usually with the conman console) and using standard network debugging techniques.\nNMN and CAN/CHN network setup errors can also result from incorrect switch configuration and network cabling.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/troubleshooting/troubleshoot_uan_disk_configuration_issues/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "toc_title=troubleshoot uan disk configuration issues; given_title=troubleshoot uan disk configuration issues; menuTitle: \u0026ldquo;Troubleshoot UAN Disk Configuration Issues\u0026rdquo; date: Sun Mar 26 02:00:58 UTC 2023 draft: false weight: 73 Troubleshoot UAN Disk Configuration Issues Perform this procedure to enable uan_disk_config to run successfully by erasing existing disk partitions. UAN disk configuration will fail if the disk on the node is already partitioned. Manually erase any existing partitions to fix the issue.\nThis procedure currently only addresses uan_disk_config errors due to existing disk partitions.\nRefer to About UAN Configuration for an explanation of UAN disk configuration.\nThe most common cause of failure in the uan_disk_config role is the disk having been previously configured without a /scratch and /swap partition. Existing partitions prevent the parted command from dividing the disk into those two equal partitions. The solution is to log into the node and run parted manually to remove the existing partitions on that disk.\nExamine the CFS log and identify the failed disk device.\nLog into the affected UAN as root.\nUse parted to manually remove any existing partitions.\nThe following example uses /dev/sdb as the disk device. Also, as partitions are removed, the remaining partitions are renumbered. Therefore, rm 1 is issued twice to remove both partitions.\nuan# parted GNU Parted 3.2 Using /dev/sda Welcome to GNU Parted! Type \u0026#39;help\u0026#39; to view a list of commands. (parted) select /dev/sdb Using /dev/sdb (parted) print Model: ATA VK000480GWSRR (scsi) Disk /dev/sdb: 480GB Sector size (logical/physical): 512B/4096B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 240GB 240GB primary ext4 type=83 2 240GB 480GB 240GB primary ext4 type=83 (parted) rm 1 (parted) rm 1 (parted) print Model: ATA VK000480GWSRR (scsi) Disk /dev/sdb: 480GB Sector size (logical/physical): 512B/4096B Partition Table: msdos Disk Flags: (parted) quit uan01:~ # Either reboot the affected UAN or launch a CFS session against it to rerun the uan_disk_config role.\n"
},
{
	"uri": "/docs-uan/en-260-alpha6/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/docs-uan/en-260-alpha6/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]