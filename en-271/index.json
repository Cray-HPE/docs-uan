[
{
	"uri": "/docs-uan/en-271/installation_prereqs/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "installation prereqs Topics: Prepare for UAN Product Installation Configure the BMC for UANs with iLO Configure the BIOS of an HPE UAN "
},
{
	"uri": "/docs-uan/en-271/installation_prereqs/prepare_for_uan_product_installation/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Prepare for UAN Product Installation Perform this procedure to ready the HPE Cray Supercomputing EX system for HPE Cray Supercomputing UAN product installation.\nInstall and configure the HPE Cray Supercomputing COS product before performing this procedure.\nVerify that the management network switches are properly configured.\nSee the switch configuration procedures in the HPE Cray System Management Documentation.\nEnsure that the management network switches have the proper firmware.\nSee the procedure \u0026ldquo;Update the Management Network Firmware\u0026rdquo; in the HPE Cray EX hardware documentation.\nEnsure that the host reservations for the UAN CAN/CHN network have been properly set.\nSee the procedure \u0026ldquo;Add UAN CAN IP Addresses to SLS\u0026rdquo; in the HPE Cray Supercomputing EX hardware documentation.\nFor systems where UANs are going to host UAIs, identify a block of IP addresses for the services running in K3s. Please see Configuring a UAN for K3s (Technical Preview) for information on reserving a block of IPs on CAN/CHN for K3s MetalLB use.\nNote: The identification of IP addresses for the services running in K3s should be made at system installation time in order to avoid the possibility of IP collisions with CSM services.\nConfigure the BMC for UANs with iLO\nConfigure the BIOS of an HPE UAN\nVerify that the firmware for each UAN BMC meets the specifications.\nUse the System Admin Toolkit firmware command to check the current firmware version on a UAN node.\nncn-m001# sat firmware -x BMC_XNAME Repeat the previous six Steps for all UANs.\nUnpackage the file.\nncn-m001# tar zxf uan-PRODUCT_VERSION.tar.gz Navigate into the uan-PRODUCT_VERSION/ directory.\nncn-m001# cd uan-PRODUCT_VERSION/ Run the pre-install goss tests to determine if the system is ready for the UAN product installation.\nThis step requires that goss is installed on the node running the tests.\nncn# ./validate-pre-install.sh ............... Total Duration: 1.304s Count: 15, Failed: 0, Skipped: 0 Ensure that the cray-console-node pods are connected to UANs so that they are monitored and their consoles are logged.\nObtain a list of the xnames for all UANs (remove the --subrole argument to list all Application nodes).\nncn# cray hsm state components list --role Application --subrole UAN --format json | jq -r .Components[].ID | sort x3000c0s19b0n0 x3000c0s24b0n0 x3000c0s31b0n0 Obtain a list of the console pods.\nncn# PODS=$(kubectl get pods -n services -l app.kubernetes.io/name=cray-console-node --template \u0026#39;{{range .items}}{{.metadata.name}} {{end}}\u0026#39;) Use conman -q to scan the list of connections conman is monitoring (only UAN xnames are shown for brevity).\nncn# for pod in $PODS; do kubectl exec -n services -c cray-console-node $pod -- conman -q; done x3000c0s19b0n0 x3000c0s24b0n0 x3000c0s31b0n0 If a console connection is not present, the install may continue, but a console connection should be established before attempting to boot the UAN.\nNext, install the UAN product by performing the procedure Install the UAN Product Stream.\n"
},
{
	"uri": "/docs-uan/en-271/installation_prereqs/configure_the_bmc_for_uans_with_ilo/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Configure the BMC for UANs with iLO Perform this procedure to enable the IPMI/DCMI settings on an HPE UAN that are necessary to continue HPE Cray Supercomputing UAN product installation on an HPE Supercomputing Cray EX system.\nPerform the first three steps of Prepare for UAN Product Installation before performing this procedure.\nCreate the SSH tunnel necessary to access the BMC web GUI interface.\nFind the IP or hostname for a UAN.\nCreate an SSH tunnel to the UAN BMC. Run the following command on an external system.\nIn the following example, UAN_MGMT is the UAN iLO interface host name or IP address. NCN is the host name or IP address of a non-compute node on the system. This example assumes that NCN allows port forwarding. USER will usually be root.\n$ ssh -L 8443:UAN_MGMT:443 USER@NCN Wait for SSH to establish the connection.\nOpen https://127.0.0.1:8443 in a web browser on the NCN to access the BMC web GUI.\nLog in to the web GUI using default credentials.\nClick Security in the menu on the left side of the screen.\nClick Access Settings in the menu at the top of the screen.\nClick the pencil icon next to Network in the main window area.\nCheck the box next to IPMI/DCMI over LAN.\nEnsure that the remote management settings match the following screenshot.\n"
},
{
	"uri": "/docs-uan/en-271/installation_prereqs/configure_the_bios_of_an_hpe_uan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Configure the BIOS of an HPE UAN Perform this procedure to configure the network interface and boot settings required by HPE UANs.\nBefore the HPE Cray Supercomputing UAN product can be installed on HPE UANs, specific network interface and boot settings must be configured in the BIOS.\nPerform Configure the BMC for UANs with iLO before performing this procedure.\nForce a UAN to reboot into the BIOS.\nIn the following command, UAN_BMC_XNAME is the xname of the BMC of the UAN to configure. Replace USER and PASSWORD with the BMC user name and password, respectively.\nncn-m001# ipmitool -U USER -P PASSWORD -H UAN_BMC_XNAME -I lanplus \\ chassis bootdev pxe options=efiboot,persistent Monitor the console of the UAN using either ConMan or the following command:\nncn-m001# ipmitool -U USER -P PASSWORD -H UAN_BMC_XNAME -I \\ lanplus sol activate See also the section \u0026ldquo;About the ConMan Containerized Service\u0026rdquo; in the CSM documentation for more information about ConMan.\nPress the ESC and 9 keys to access the BIOS System Utilities when the option appears.\nEnsure that OCP Slot 10 Port 1 is the only port with Boot Mode set to Network Boot. All other ports must have Boot Mode set to Disabled.\nThe settings must match the following example.\n-------------------- System Configuration BIOS Platform Configuration (RBSU) \u0026gt; Network Options \u0026gt; Network Boot Options \u0026gt; PCIe Slot Network Boot Slot 1 Port 1 : Marvell FastLinQ 41000 Series - [Disabled] 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC Slot 1 Port 2 : Marvell FastLinQ 41000 Series - [Disabled] 2P 25GbE SFP28 QL41232HLCU-HC MD2 Adapter - NIC Slot 2 Port 1 : Network Controller [Disabled] OCP Slot 10 Port 1 : Marvell FastLinQ 41000 [Network Boot] Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC OCP Slot 10 Port 2 : Marvell FastLinQ 41000 [Disabled] Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC -------------------- Set the Link Speed to SmartAN for all ports.\n-------------------- System Utilities System Configuration \u0026gt; Main Configuration Page \u0026gt; Port Level Configuration Link Speed [SmartAN] FEC Mode [None] Boot Mode [PXE] DCBX Protocol [Dynamic] RoCE Priority [0] PXE VLAN Mode [Disabled] Link Up Delay [30] Wake On LAN Mode [Enabled] RDMA Protocol Support [iWARP + RoCE] BAR-2 Size [8M] VF BAR-2 Size [256K] --------------------- Set the boot options to match the following example.\n---------------------- System Utilities System Configuration \u0026gt; BIOS/Platform Configuration (RBSU) \u0026gt; Boot Options Boot Mode [UEFI Mode] UEFI Optimized Boot [Enabled] Boot Order Policy [Retry Boot Order Indefinitely] UEFI Boot Settings Legacy BIOS Boot Order ----------------------- Set the UEFI Boot Order settings to match the following example.\nThe order must be:\nUSB Local disks OCP Slot 10 Port 1 IPv4 OCP Slot 10 Port 1 IPv6 ----------------------- System Utilities System Configuration \u0026gt; BIOS/Platform Configuration (RBSU) \u0026gt; Boot Options \u0026gt; UEFI Boot Settings \u0026gt; UEFI Boot Order Press the \u0026#39;+\u0026#39; key to move an entry higher in the boot list and the \u0026#39;-\u0026#39; key to move an entry lower in the boot list. Use the arrow keys to navigate through the Boot Order list. Generic USB Boot SATA Drive Box 1 Bay 1 : VK000480GWTHA SATA Drive Box 1 Bay 2 : VK000480GWTHA SATA Drive Box 1 Bay 3 : VK001920GWTTC SATA Drive Box 1 Bay 4 : VK001920GWTTC OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv4) OCP Slot 10 Port 1 : Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - NIC - Marvell FastLinQ 41000 Series - 2P 25GbE SFP28 QL41232HQCU-HC OCP3 Adapter - PXE (PXE IPv6) ------------------------- See this Setting the Date and Time in the HPE UEFI documentation to set the correct date and time.\nIf the time is not set correctly, then PXE network booting issues may occur.\n"
},
{
	"uri": "/docs-uan/en-271/install/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "install Topics: Install or Upgrade UAN "
},
{
	"uri": "/docs-uan/en-271/operations/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "operations Topics: Basic UAN Configuration Build a New UAN Image Using a COS Recipe Create UAN Boot Images Mount a New File System on a UAN Configure Interfaces on UANs Configure Pluggable Authentication Modules (PAM) on UANs Boot UANs Mitigation for CVE-2023-0461 UAN gpg keys UAN ca cert UAN disk config UAN hardening UAN interfaces UAN LDAP UAN motd UAN packages UAN shadow "
},
{
	"uri": "/docs-uan/en-271/operations/basic_uan_configuration/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Basic UAN Configuration UAN configuration overview The Configuration Framework Service (CFS) performs the configuration of UAN nodes. CFS can apply configuration to both images and nodes. When the configuration is applied to nodes, the nodes must be booted and accessible through SSH over the Node Management Network (NMN).\nThe preferred method of creating CFS configurations is to use the Shasta Admin Toolkit (SAT) sat bootprep command. This command automates the manual process described in Create UAN Boot Images. That process includes creating IMS images, CFS configurations, and BOS session templates.\nCFS uses configuration layers. Configuration layers allow the sharing of Ansible roles provided by other products, and by the site. Non-root user access may be blocked during node configuration by enabling the uan-set-nologin and uan-unset-nologin configuration layers shown in the following example bootprep file. The parameterized fields are defined in a product_vars.yml file.\nIMPORTANT Do not remove or reorder the first three layers. The HPE Cray Supercomputing UAN product requires these layers and this specific order. Also, keep the required cos-application-last layer as the last or second to last layer in the configuration if uan-set-nologin and uan-unset-nologin are active.\n- name: \u0026#34;{{default.note}}uan-{{recipe.version}}{{default.suffix}}\u0026#34; layers: # The first three layers are required and must not be reordered. - name: shs-{{default.network_type}}_install-{{slingshot_host_software.working_branch}} playbook: shs_{{default.network_type}}_install.yml product: name: slingshot-host-software version: \u0026#34;{{slingshot_host_software.version}}\u0026#34; branch: \u0026#34;{{slingshot_host_software.working_branch}}\u0026#34; - name: cos-application-{{cos.working_branch}} playbook: cos-application.yml product: name: cos version: \u0026#34;{{cos.version}}\u0026#34; branch: \u0026#34;{{cos.working_branch}}\u0026#34; - name: csm-packages-{{csm.version}} playbook: csm_packages.yml product: name: csm version: \u0026#34;{{csm.version}}\u0026#34; # Optional layer to prevent non-root logins during configuration # - name: uan-set-nologin-{{uan.working_branch}} # playbook: set_nologin.yml # product: # name: uan # version: \u0026#34;{{uan.version}}\u0026#34; # branch: \u0026#34;{{uan.working_branch}}\u0026#34; - name: uan-{{uan.working_branch}} playbook: site.yml product: name: uan version: \u0026#34;{{uan.version}}\u0026#34; branch: \u0026#34;{{uan.working_branch}}\u0026#34; ### additional CFS layers here... # cos-application-last is required to be the last or second to last layer # when uan-set-nologin and uan-unset-nologin layers are active. - name: cos-application-last-{{cos.working_branch}} playbook: cos-application-after.yml product: name: cos version: \u0026#34;{{cos.version}}\u0026#34; branch: \u0026#34;{{cos.working_branch}}\u0026#34; # Optional layer to allow non-root logins after configuration # - name: uan-unset-nologin-{{uan.working_branch}} # playbook: unset_nologin.yml # product: # name: uan # version: \u0026#34;{{uan.version}}\u0026#34; # branch: \u0026#34;{{uan.working_branch}}\u0026#34; Required CFS Layers for UAN Configuration Slingshot Host Software (playbook: shs_{{default.network_type}}_install.yml) The first CFS Layer installs the Slingshot Host Software for the Slingshot network type of the system. The default.network_type is:\nmellanox for ConnectX-5 NICs used in Slingshot 10 cassini for Slingshot NICs in Slingshot 11 The name of the playbook must match the name of the HSN NICs (Mellanox or Cassini) in the UAN nodes. Additionally, the HSN NICs must be of the same type as the NCN and Compute nodes.\nCOS (playbook: cos-application.yml) The second CFS Layer runs the following roles from the cos-config-management VCS repository. Any configuration changes needed for these roles must be made in the cos-config-management group_vars or host_vars subdirectories of that repository.\nThe following Ansible roles are run during UAN image configuration:\nStandard UNIX configuration\ncos-config-map rsyslog localtime ntp limits kdump Allow trust of CSM generated keys for elective passwordless SSH during image customization\ntrust-csm-ssh-keys HPE Cray EX system configurations\noverlay-preload Install COS rpms\ncos-services-install **DVS/LNET/FS\ncray_lnet_install cray_dvs_install cray_lnet_load cray_dvs_load The following Ansible roles are run during UAN post-boot configuration:\nStandard UNIX configuration\nrsyslog HPE Cray EX system configurations\nca-cert overlay-preload GPU deploy support\ncray_gpu_deploy CMS Layer (playbook: csm_packages.yml) The third CFS Layer installs the Cray Management System packages. These packages are normally not modified.\nOptional UAN Layer (playbook: set_nologin.yml) This optional layer is recommended when the nodes will host non-root users. The layer touches the /etc/nologin file preventing non-root users from logging into the node while it is being configured. Be sure to include the optional UAN layer which calls the unset_nologin.yml playbook as indicated in this document if non-root users are to be allowed to login after the node is configured.\nUAN Layer (playbook: site.yml) The Ansible roles involved in the UAN layer of the configuration are listed in the site.yml file in the uan-config-management git repository in VCS. Most of the roles that are specific to image configuration are required for the operation as a UAN and must not be removed from site.yml.\nThe UAN-specific roles involved in post-boot UAN node configuration are:\nuan_disk_config: this role configures the last disk found on the UAN that is smaller than 1TB, by default. That disk will be formatted with a scratch and swap partition mounted at /scratch and /swap, respectively. Each partition is 50% of the disk.\nuan_packages: this role installs any RPM packages listed in the uan-config-management repo.\nuan_interfaces: this role configures the UAN node networking. By default, this role does not configure a default route or the Customer Access Network (CAN or CHN) connection for the HPE Cray EX supercomputer. If CAN or CHN is enabled, the default route will be on the CAN or CHN. Otherwise, a default route must be set up in the customer interfaces definitions. Without the CAN or CHN, there will not be an external connection to the customer site network unless one is defined in the customer interfaces. See Configure Interfaces on UANs.\nNOTE: If a UAN layer is used in the Compute node CFS configuration, the uan_interfaces role will configure the default route on Compute nodes to be on the HSN, if the BICAN System Default Route is set to CHN.\nuan_motd: this role Provides a default message of the day that the administrator can customize.\nuan_ldap: this optional role configures the connection to LDAP servers. To disable this role, the administrator must set uan_ldap_setup:no in the uan-config-management VCS repository.\nuan_hardening: This role configures site or customer-defined network security of UANs, for example, preventing SSH access from the UAN over the NMN to NCN nodes.\nThe UAN roles in site.yml are required and must not be removed, with exception of uan_ldap if the site is using some other method of user authentication. The uan_ldap may also be skipped by setting the value of uan_ldap_setup to no in a group_vars or host_vars configuration file. Configuration of this layer is made in the uan-config-management VCS repository.\nCOS (playbook: cos-application-after.yml) This CFS Layer runs the following roles from the cos-config-management VCS repository. Any configuration changes needed for these roles must be made in the group_vars or host_vars subdirectories of that repository.\nThe following Ansible roles are run during UAN image configuration:\nrebuilt-initrd The following Ansible roles are run during UAN post-boot configuration:\nconfigure_fs Optional UAN Layer (playbook: unset_nologin.yml) This UAN layer deletes the /etc/nologin file allowing non-root users to log into the UAN. If the optional UAN layer that runs set_nologin.yml was used, this layer must be used or only the root user will have access to the node.\n"
},
{
	"uri": "/docs-uan/en-271/operations/build_a_new_uan_image_using_the_cos_recipe/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Build a New UAN Image Using a COS Recipe Prior to HPE Cray Supercomputing UAN release 2.3, a similar copy of the COS image recipe was imported with the UAN install. Beginning with the 2.3 release, this product does not install an image recipe. A COS image recipe must be used. Additional UAN packages will be installed by CFS and the uan_packages role. In UAN release 2.6, this procedure is automated as part of the IUF process of installing and upgrading the UAN product. See Install or Upgrade UAN for details.\nThe following procedures are provided for cases where a new UAN image must be built after initial installation. This document describes two methods of building UAN images:\nUsing IUF to Build a New UAN Image (UAN 2.6+): use this procedure if you are using the IUF automation and the UAN software release version is 2.6 or later. Manually Build a New UAN Image from a COS Recipe (UAN 2.3+): use this procedure if you are not using the IUF automation or the UAN software release is earlier than 2.6 and later than 2.3. Using IUF to Build a New UAN Image (UAN 2.6+) The procedure for using IUF to build and prepare images is documented in the Image Preparation section of the CSM documentation. After IUF runs, the UAN CFS configuration will be created, the UAN image will be configured using that configuration, and the UAN BOS session template will be created using the new configuration and image.\nThe following information is provided for reference.\nTwo IUF stages are run to create a new UAN image:\nupdate-cfs-config: Creates a new CFS configuration defined by the information in the bootprep file. prepare-images: Applies the new CFS configuration to the image defined in the bootprep file. After IUF runs these two stages, the UAN CFS configuration will be created, the UAN image will be configured using that configuration, and a UAN BOS session template will be created using the new configuration and image.\nBefore using IUF to build a new UAN image from a COS recipe, be sure that the information in the IUF Recipe Variables file (product_vars.yaml) and bootprep file (compute-and-uan-bootprep.yaml) are correct for the wanted UAN CFS configuration and the COS image recipe.\nExample product_vars.yaml showing COS and UAN versions and working VCS branches:\ncos: version: 2.5.120 # Provides the COS image to use as a base UAN image working_branch: \u0026#34;{{ working_branch }}\u0026#34; # COS CFS branch to use (typically matches compute nodes) uan: version: 2.6.0 # Provides the UAN CFS configuration working_branch: \u0026#34;{{ working_branch }}\u0026#34; # Provides the UAN CFS branch to use Example compute-and-uan-bootprep.yaml showing UAN CFS configuration and images (COS and UAN):\nconfigurations: - name: \u0026#34;{{default.note}}uan-{{recipe.version}}{{default.suffix}}\u0026#34; layers: - name: shs-{{default.network_type}}_install-{{slingshot_host_software.working_branch}} playbook: shs_{{default.network_type}}_install.yml product: name: slingshot-host-software version: \u0026#34;{{slingshot_host_software.version}}\u0026#34; branch: \u0026#34;{{slingshot_host_software.working_branch}}\u0026#34; - name: cos-application-{{cos.working_branch}} playbook: cos-application.yml product: name: cos version: \u0026#34;{{cos.version}}\u0026#34; branch: \u0026#34;{{cos.working_branch}}\u0026#34; - name: csm-packages-{{csm.version}} playbook: csm_packages.yml product: name: csm version: \u0026#34;{{csm.version}}\u0026#34; - name: uan-{{uan.working_branch}} playbook: site.yml product: name: uan version: \u0026#34;{{uan.version}}\u0026#34; branch: \u0026#34;{{uan.working_branch}}\u0026#34; - name: csm-diags-application-{{csm_diags.version}} playbook: csm-diags-application.yml product: name: csm-diags version: \u0026#34;{{csm_diags.version}}\u0026#34; - name: sma-ldms-application-{{sma.version}} playbook: sma-ldms-application.yml product: name: sma version: \u0026#34;{{sma.version}}\u0026#34; - name: cpe-pe_deploy-{{cpe.working_branch}} playbook: pe_deploy.yml product: name: cpe version: \u0026#34;{{cpe.version}}\u0026#34; branch: \u0026#34;{{cpe.working_branch}}\u0026#34; - name: analytics-site-{{analytics.working_branch}} playbook: site.yml product: name: analytics version: \u0026#34;{{analytics.version}}\u0026#34; branch: \u0026#34;{{analytics.working_branch}}\u0026#34; - name: slurm-site-{{slurm.working_branch}} playbook: site.yml product: name: slurm version: \u0026#34;{{slurm.version}}\u0026#34; branch: \u0026#34;{{slurm.working_branch}}\u0026#34; - name: cos-application-last-{{cos.working_branch}} playbook: cos-application-after.yml product: name: cos version: \u0026#34;{{cos.version}}\u0026#34; branch: \u0026#34;{{cos.working_branch}}\u0026#34; images: # images that use base.name will inherit the note and suffix in their name - name: \u0026#34;{{default.note}}{{base.name}}{{default.suffix}}\u0026#34; ref_name: base_cos_image base: product: name: cos type: recipe version: \u0026#34;{{cos.version}}\u0026#34; - name: \u0026#34;compute-{{base.name}}\u0026#34; ref_name: compute_image base: image_ref: base_cos_image configuration: \u0026#34;{{default.note}}compute-{{recipe.version}}{{default.suffix}}\u0026#34; configuration_group_names: - Compute - name: \u0026#34;uan-{{base.name}}\u0026#34; ref_name: uan_image base: image_ref: base_cos_image configuration: \u0026#34;{{default.note}}uan-{{recipe.version}}{{default.suffix}}\u0026#34; configuration_group_names: - Application - Application_UAN After the product_vars.yaml and compute-and-uan-bootprep.yaml files are updated to reflect the wanted COS and UAN versions and VCS branches to use, IUF may be executed.\nManually Build a New UAN Image from a COS Recipe (UAN 2.3+) Perform the following before starting this procedure:\nInstall the COS, Slingshot, and UAN product streams. Initialize the cray administrative CLI. In the COS recipe for 2.2, several dependencies have been removed, including Slingshot, DVS, and Lustre. Those packages are now installed during CFS Image Customization. More information on this change is covered in the Create UAN Boot Images procedure.\nIdentify the COS image recipe to base the UAN image on. Select the recipe that matches the version of COS that the compute nodes will be using.\nncn-m001# cray ims recipes list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;compute\u0026#34;))\u0026#39; { \u0026#34;created\u0026#34;: \u0026#34;2021-02-17T15:19:48.549383+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;4a5d1178-80ad-4151-af1b-bbe1480958d1\u0026#34;, \u0026lt;\u0026lt;-- Note this ID \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;3c3b292364f7739da966c9cdae096964\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://ims/recipes/4a5d1178-80ad-4151-af1b-bbe1480958d1/recipe.tar.gz\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;linux_distribution\u0026#34;: \u0026#34;sles15\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cray-shasta-compute-sles15sp3.x86_64-2.2.27\u0026#34;, \u0026#34;recipe_type\u0026#34;: \u0026#34;kiwi-ng\u0026#34; } Save the id of the IMS recipe in an environment variable.\nncn-m001# IMS_RECIPE_ID=4a5d1178-80ad-4151-af1b-bbe1480958d1 Use the IMS recipe id to build the UAN image:\nMore detail on this IMS procedure may be found in the procedure \u0026ldquo;Build an Image Using IMS REST Service\u0026rdquo; in the CSM documentation.\nncn-m001# IMS_PUBLIC_KEY=$(cray ims public-keys list --format json | jq -r \u0026#34;.[] | .id\u0026#34; | head -1) ncn-m001# IMS_ARCHIVE_NAME=$(cray ims recipes describe $IMS_RECIPE_ID --format json | jq -r .name) ncn-m001# IMS_ARCHIVE_NAME=${IMS_ARCHIVE_NAME/compute/uan} ncn-m001# cray ims jobs create --job-type create --public-key-id $IMS_PUBLIC_KEY --image-root-archive-name $IMS_ARCHIVE_NAME --artifact-id $IMS_RECIPE_ID Perform Create UAN Boot Images to run CFS Image Customization on the resulting image.\n"
},
{
	"uri": "/docs-uan/en-271/operations/create_uan_boot_images/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Create UAN Boot Images Beginning with UAN 2.6, the procedures described here are automatically performed by IUF during installation and upgrade of the HPE Cray Supercomputing UAN product. See Install or Upgrade UAN for details. The procedures shown here are for cases when a new image is needed after the UAN product is installed or upgraded and the product release is prior to 2.6. For release 2.6 and newer, perform Build a New UAN Image Using a COS Recipe for these cases.\nOverview This procedure updates the configuration management git repository to match the installed version of the HPE Cray Supercomputing UAN product. That updated configuration is then used to create UAN boot images and a BOS session template.\nUAN specific configuration, and other required configurations related to UANs are covered in this topic. See product-specific documentation for further information on configuring other HPE products (for example, workload managers and the HPE Cray Programming Environment) that may be configured on the UANs.\nThe workflow for manually creating images to boot UANs is:\nPrepare CFS Configuration: Clone the UAN configuration git repository and create a branch based on the branch imported by the UAN installation. Update the configuration content and push the changes to the newly created branch. Create SAT Bootprep File: Enter the information that sat bootprep will use to automatically create CFS configurations, IMS images, and BOS session templates for UANs. Run sat bootprep to generate all the artifacts a BOS session requires to boot UANs. Boot UANs to boot the UANs with the new image and BOS session template. Replace PRODUCT_VERSION and CRAY_EX_HOSTNAME in the example commands in this procedure with the current UAN product version installed (See Step 1) and the hostname of the HPE Cray Supercomputing EX system, respectively.\nPrepare CFS Configuration Obtain the artifact IDs and other information from the cray-product-catalog Kubernetes ConfigMap. Record the following information:\nthe clone_url the import_branch value Upon successful installation of the UAN product, the UAN configuration is cataloged in this ConfigMap. This information is required for this procedure.\nPRODUCT_VERSION will be replaced by a numbered version string, such as 2.1.7 or 2.3.0.\nncn-m001# kubectl get cm -n services cray-product-catalog -o json | jq -r .data.uan PRODUCT_VERSION: configuration: clone_url: https://vcs.CRAY_EX_HOSTNAME/vcs/cray/uan-config-management.git commit: 6658ea9e75f5f0f73f78941202664e9631a63726 import_branch: cray/uan/PRODUCT_VERSION import_date: 2021-02-02 19:14:18.399670 ssh_url: git@vcs.CRAY_EX_HOSTNAME:cray/uan-config-management.git Obtain the password for the crayvcs user from the Kubernetes secret for use in the next command.\nncn-m001# VCS_USER=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_username}} | base64 --decode) VCS_PASS=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode) Clone the UAN configuration management repository. Replace CRAY_EX_HOSTNAME in the clone url with api-gw-service-nmn.local when cloning the repository.\nThe repository is in the VCS/Gitea service and the location is reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.clone_url key. The CRAY_EX_HOSTNAME from the clone_url is replaced with api-gw-service-nmn.local in the command that clones the repository.\nncn-m001# git clone https://$VCS_USER:$VCS_PASS@api-gw-service-nmn.local/vcs/cray/uan-config-management.git . . . ncn-m001# cd uan-config-management \u0026amp;\u0026amp; git checkout cray/uan/PRODUCT_VERSION \u0026amp;\u0026amp; git pull Branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; set up to track remote branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; from \u0026#39;origin\u0026#39;. Already up to date. Create a branch using the imported branch from the installation to customize the UAN image.\nThis branch name will be reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.import_branch key under the UAN section. The format is cray/uan/PRODUCT_VERSION. In this guide, an integration-PRODUCT_VERSION branch is used for examples to comply with IUF defaults, but the name can be any valid git branch name configured to be used by IUF.\nModifying the cray/uan/PRODUCT_VERSION branch that the UAN product installation created is not allowed by default.\nncn-m001# git checkout -b integration-PRODUCT_VERSION \u0026amp;\u0026amp; git merge cray/uan/PRODUCT_VERSION Switched to a new branch \u0026#39;integration-PRODUCT_VERSION\u0026#39; Already up to date. Apply any site-specific customizations and modifications to the Ansible configuration for the UAN nodes and commit the changes.\nThe default Ansible play to configure UAN nodes is site.yml in the base of the uan-config-management repository. The roles that are executed in this play allow for custom configuration as required for the system.\nConsult the individual Ansible role README.md files in the uan-config-management repository roles directory to configure individual role variables. Roles prefixed with uan_ are specific to UAN configuration and include network interfaces, disk, LDAP, software packages, and message of the day roles.\nNOTE: Admins must ensure the uan_can_setup variable is set to the correct value for the site. This variable controls how the nodes are configured for user access. When uan_can_setup is yes, user access is over the CAN or CHN, based on the BICAN System Default Route setting in SLS. When uan_can_setup is no, the Admin must configure the user access interface and default route. See Configure Interfaces on UANs\nWarning: Never place sensitive information such as passwords in the git repository.\nThe following example shows how to add a vars.yml file containing site-specific configuration values to the Application_UAN group variable location.\nThese and other Ansible files do not necessarily need to be modified for UAN image creation. See Basic UAN Configuration for instructions for site-specific UAN configuration, including CAN/CHN configuration.\nncn-m001# vim group_vars/Application_UAN/vars.yml ncn-m001# git add group_vars/Application_UAN/vars.yml ncn-m001# git commit -m \u0026#34;Add vars.yml customizations\u0026#34; [integration-PRODUCT_VERSION ecece54] Add vars.yml customizations 1 file changed, 1 insertion(+) create mode 100644 group_vars/Application_UAN/vars.yml Push the changes to the repository using the proper credentials, including the password obtained previously.\nncn-m001# git push --set-upstream origin integration-PRODUCT_VERSION Username for \u0026#39;https://api-gw-service-nmn.local\u0026#39;: crayvcs Password for \u0026#39;https://crayvcs@api-gw-service-nmn.local\u0026#39;: . . . remote: Processed 1 references in total To https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git * [new branch] integration-PRODUCT_VERSION -\u0026gt; integration-PRODUCT_VERSION Branch \u0026#39;integration-PRODUCT_VERSION\u0026#39; set up to track remote branch \u0026#39;integration-PRODUCT_VERSION\u0026#39; from \u0026#39;origin\u0026#39;. The configuration parameters have been stored in a branch in the UAN git repository. The next phase of the process uses sat bootprep to handle creating the CFS configurations, IMS images, and BOS session templates for UANs.\nUAN SAT Bootprep Input File Contents With Shasta Admin Toolkit (SAT) version 2.2.16 and later, HPE recommends that administrators create an input file for use with sat bootprep.\nA sat bootprep input file will have three sections:\nconfigurations: specifies each layer to be included in the CFS configuration for the UAN images for image customization and node personalization. images: specifies the IMS images to create for UAN nodes. session_templates: creates BOS session templates. This section references the named IMS image that sat bootprep generates, as well as a CFS configuration. These sections create CFS configurations, IMS images, and BOS session templates respectively. Each section may have multiple elements to create more than one CFS, IMS, or BOS artifact. The format is similar to the input files for CFS, IMS, and BOS, but SAT will automate the process with fewer steps. Follow the subsections below to create a UAN bootprep input file.\nSee also HPE Cray EX System Software Stack Installation and Upgrade Guide for CSM (S-8052) for further information on configuring other HPE products, as this procedure documents only the required configuration of the UAN.\nCreate SAT Bootprep File Verify that installed version of SAT is 2.2.16 or later.\nncn-m001# sat showrev --products --filter \u0026#39;product_name=\u0026#34;sat\u0026#34;\u0026#39; Obtain the version of each product that will be included in a CFS configuration layer.\nncn-m001# sat showrev --products --filter \u0026#39;product_name=\u0026#34;PRODUCT_NAME\u0026#34;\u0026#39; The example sat bootprep input file in this procedure includes the following products: - slingshot-host-software - cos - csm - uan\nYour site may require additional products in the input file.\nRecord or save the list of COS image recipes returned in the previous step.\nYou will select one of these recipes as the base for the UAN image in a later step.\nCreate a sat bootprep input file.\nncn-m001# touch uan-bootprep.yaml Open the empty sat bootprep input file in an editor.\nAdd the CFS configuration content.\nThe Slingshot Host Software CFS layer must be listed first. This layer is required as the UAN layer will attempt to install DVS and Lustre packages that require SHS be installed first. The correct playbook for Cassini or Mellanox must also be specified. Consult the Slingshot Host Software documentation for more information.\nBeginning with UAN version 2.6.0, CFS configuration roles which are provided by COS are now defined as two separate COS configuration layers as shown in the following example. Prior to UAN version 2.6.0, these roles were included in the UAN configuration layer. Separating these roles into COS layers allows COS updates to be independent from UAN updates.\nThe following example creates a CFS configuration named uan-config:\nExample:\nconfigurations: - name: uan-config layers: - name: shs-mellanox_install-integration playbook: shs_mellanox_install.yml product: name: slingshot-host-software version: 2.0.0 branch: integration # - name: shs-cassini_install-integration # playbook: shs_cassini_install.yml # product: # name: slingshot-host-software # version: 2.0.0 # branch: integration - name: cos-application-integration playbook: cos-application.yml product: name: cos version: 2.5 - name: csm-packages-integration playbook: csm_packages.yml product: name: csm version: 1.4 - name: uan-set-nologin playbook: set_nologin.yml product: name: uan version: 2.6.0 branch: integration-PRODUCT_VERSION - name: uan playbook: site.yml product: name: uan version: 2.6.0 branch: integration-PRODUCT_VERSION ... add configuration layers for other products here, if desired ... - name: uan-rebuild-initrd playbook: rebuild-initrd.yml product: name: uan version: 2.6.0 branch: integration-PRODUCT_VERSION - name: uan-unset-nologin playbook: unset_nologin.yml product: name: uan version: 2.6.0 branch: integration-PRODUCT_VERSION Add the content for the UAN image, using an appropriate name to correctly identify the UAN image being built.\nUAN images are built using the COS recipe, so this step specifies which image recipe to use based on what is provided by COS. The ims section references the uan-config CFS configuration so that CFS image customization will use that configuration along with the specified node groups.\nThe following example will create an IMS image with the name cray-shasta-uan-sles15sp3.x86_64-2.3.25.\nimages: - name: cray-shasta-uan-sles15sp3.x86_64-2.3.25 ims: is_recipe: true name: cray-shasta-compute-sles15sp3.x86_64-2.3.25 configuration: uan-config configuration_group_names: - Application - Application_UAN Add the content for the BOS session templates.\nYou may need to change the boot_sets key uan in the following example. If there are more than one boot_sets in the session template, each key must be unique.\nsession_templates: - name: uan-2.4.0 image: cray-shasta-uan-sles15sp3.x86_64-2.3.25 configuration: uan-config bos_parameters: boot_sets: uan: kernel_parameters: spire_join_token=${SPIRE_JOIN_TOKEN} node_roles_groups: - Application_UAN Save changes to the sat bootprep input YAML file and exit the editor. Run sat bootprep Execute the sat bootprep command to generate the configurations and artifacts needed to boot UANs.\nThis command may take some time as it will initiate IMS image creation and CFS image customization.\nncn-m001# sat bootprep run uan-bootprep.yaml Modify the CFS layers or input file if necessary to successfully complete sat bootprep.\nIf any artifacts are going to be overwritten, SAT will prompt for confirmation before overwriting them. This is useful when making CFS changes as SAT will automatically configure the layers to use the latest git commits if the branches are specified correctly.\nSave the input file to a known location after sat bootprep completes successfully.\nYou can use this input file to regenerate artifacts as changes are made or different product layers are added.\n"
},
{
	"uri": "/docs-uan/en-271/operations/mount_a_new_file_system_on_an_uan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Mount a New File System on a UAN Perform this procedure to create a mount point for a new file system on a UAN.\nPerform Steps 1-9 of Create UAN Boot Images.\nCreate a directory for Application role nodes.\nncn-w001# mkdir -p group_vars/Application Define the home directory information for the new file system in the filesystems.yml file.\nncn-w001# vi group_vars/Application/filesystems.yml --- filesystems: - src: 10.252.1.1:/home mount_point: /home fstype: nfs4 opts: rw,noauto state: mounted Add the change from the working directory to the staging area.\nncn-w001# git add -A Commit the file to the working branch.\nncn-w001# git commit -am \u0026#39;Added file system info\u0026#39; Resume Create UAN Boot Images at Step 10.\n"
},
{
	"uri": "/docs-uan/en-271/operations/configure_interfaces_on_uans/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Configure Interfaces on UANs Perform this procedure to configure network interfaces on UANs by editing a configuration file.\nThe uan_interfaces Ansible role performs interface configuration. For details on the variables used in this procedure, see uan_interfaces.\nIn the command examples of this procedure, PRODUCT_VERSION refers to the current installed version of the HPE Cray Supercomputing UAN product. Replace PRODUCT_VERSION with the product version number string when executing the commands.\nNode Management Networking By default, the Node Management Network (NMN) is connected to a single nmn0 interface. If wanted, and the system networking is configured to support it, the Node Management Network may be configured as a bonded interface, nmnb0. To configure the NMN as a bonded pair, set uan_nmn_bond to true and set the interfaces to be used in the bond in uan_nmn_bond_slaves as described in uan_interfaces.\nPrerequisites for Bonded NMN To enable NMN bonding, certain features in iPXE and the management switches connected to the UAN NMN interfaces must be enabled. These features are enabled in the following CSM and CANU versions:\nCSM-1.3.2 and newer enable the iPXE features CANU-1.7.0 and newer enables the management switch features User Access Networking User access may be configured to use either a direct connection to the UANs from the site\u0026rsquo;s user network, or one of two optional user access networks implemented within the HPE Cray Supercomputing EX system. The two optional networks are the Customer Access Network (CAN) and Customer High Speed Network (CHN). The CAN is a VLAN on the Node Management Network (NMN), whereas the CHN is over the High Speed Network (HSN).\nBy default, a direct connection to the site\u0026rsquo;s user network is assumed and the administrator must define one or more interfaces and default route using the customer_uan_interfaces and customer_uan_routes structures. If uan_can_setup is a true value, user access will be over CAN or CHN depending on what the system default route is set to in SLS.\nWhen CAN is set as the system default route in SLS and uan_nmn_bond is false, the bonded CAN interfaces are determined automatically. If uan_nmn_bond is true, the bonded CAN interfaces must be defined by uan_can_bond_slaves (see uan_interfaces). The default route is set to the bonded CAN interface can0.\nWhen CHN is set as the system default route in SLS, the CHN IP is added to hsn0 by default, but can be changed by setting uan_chn_device to the wanted interface. The default route is set to the CHN.\nThe Admin may override the CAN/CHN default route by setting uan_customer_default_route to true and defining the default route in customer_uan_routes.\nProcedure Network configuration settings are defined in the uan-config-management VCS repo under the group_vars/ROLE_SUBROLE/ or host_vars/XNAME/ directories, where ROLE_SUBROLE must be replaced by the role and subrole assigned for the node in HSM, and XNAME with the xname of the node. Values under group_vars/ROLE_SUBROLE/ apply to all nodes with the given role and subrole. Values under the host_vars/XNAME/ apply to the specific node with the xname and will override any values set in group_vars/ROLE_SUBROLE/. A yaml file is used by the Configuration Framework Service (CFS). The examples in this procedure use customer_net.yml, but any filename may be used. Admins must create this yaml file and use the variables described in this procedure.\nIf the HPE Cray EX CAN or CHN is wanted, set the uan_can_setup variable to yes in the yaml file. The UAN will be configured to use the CAN or CHN based on what the BICAN System Default Route is set to in SLS.\nObtain the password for the crayvcs user.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode Log in to ncn-w001.\nCreate a copy of the Git configuration. Enter the credentials for the crayvcs user when prompted.\nncn-w001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git Change to the uan-config-management directory.\nncn-w001# cd uan-config-management Edit the yaml file, (customer_net.yml, for example), in either the group_vars/ROLE_SUBROLE/ or host_vars/XNAME directory and configure the values as needed.\nTo enable bonded NMN interfaces:\n## uan_nmn_bond # Set uan_nmn_bond to \u0026#39;yes\u0026#39; if the site will # implement a bonded NMN connection. # By default, uan_nmn_bond is set to \u0026#39;no\u0026#39;. uan_nmn_bond: yes ## uan_nmn_bond_slaves # These are the default NMN bond slaves. They may need to be # changed based on the actual system hardware configuration. uan_nmn_bond_slaves: - \u0026#34;ens10f0\u0026#34; - \u0026#34;ens1f0\u0026#34; To set up CAN or CHN:\n## uan_can_setup # Set uan_can_setup to \u0026#39;yes\u0026#39; if the site will # use the Shasta CAN or CHN network for user access. # By default, uan_can_setup is set to \u0026#39;no\u0026#39;. uan_can_setup: yes ## uan_can_bond_slaves # This variable only applies when the system default route is CAN # and `uan_nmn_bond` is true. # These are the default CAN bond slaves. They may need to be # changed based on the actual system hardware configuration. uan_can_bond_slaves: - \u0026#34;ens10f1\u0026#34; - \u0026#34;ens1f1\u0026#34; ## uan_chn_device # This variable allows the admin to select which interface will # be used for the CHN device. # By default, \u0026#34;hsn0\u0026#34; is used for CHN. uan_chn_device: \u0026#34;hsn0\u0026#34; To allow a custom default route when CAN or CHN is selected:\n## uan_customer_default_route # Allow a custom default route when CAN or CHN is selected. uan_customer_default_route: no To define interfaces:\n## Customer defined networks ifcfg-X # customer_uan_interfaces is a list of interface names used for constructing # ifcfg-\u0026lt;customer_uan_interfaces.name\u0026gt; files. The setting dictionary is where # any desired ifcfg fields are defined. The field name will be converted to # uppercase in the generated ifcfg-\u0026lt;name\u0026gt; file. # # NOTE: Interfaces should be defined in order of dependency. # ## Example ifcfg fields, not exhaustive: # bootproto: \u0026#39;\u0026#39; # device: \u0026#39;\u0026#39; # dhcp_hostname: \u0026#39;\u0026#39; # ethtool_opts: \u0026#39;\u0026#39; # gateway: \u0026#39;\u0026#39; # hwaddr: \u0026#39;\u0026#39; # ipaddr: \u0026#39;\u0026#39; # master: \u0026#39;\u0026#39; # mtu: \u0026#39;\u0026#39; # peerdns: \u0026#39;\u0026#39; # prefixlen: \u0026#39;\u0026#39; # slave: \u0026#39;\u0026#39; # srcaddr: \u0026#39;\u0026#39; # startmode: \u0026#39;\u0026#39; # userctl: \u0026#39;\u0026#39; # bonding_master: \u0026#39;\u0026#39; # bonding_module_opts: \u0026#39;\u0026#39; # bonding_slave0: \u0026#39;\u0026#39; # bonding_slave1: \u0026#39;\u0026#39; # # customer_uan_interfaces: # - name: \u0026#34;net1\u0026#34; # settings: # bootproto: \u0026#34;static\u0026#34; # device: \u0026#34;net1\u0026#34; # ipaddr: \u0026#34;1.2.3.4\u0026#34; # startmode: \u0026#34;auto\u0026#34; # - name: \u0026#34;net2\u0026#34; # settings: # bootproto: \u0026#34;static\u0026#34; # device: \u0026#34;net2\u0026#34; # ipaddr: \u0026#34;5.6.7.8\u0026#34; # startmode: \u0026#34;auto\u0026#34; customer_uan_interfaces: [] To define interface static routes:\n## Customer defined networks ifroute-X # customer_uan_routes is a list of interface routes used for constructing # ifroute-\u0026lt;customer_uan_routes.name\u0026gt; files. # # customer_uan_routes: # - name: \u0026#34;net1\u0026#34; # routes: # - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; # - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; # - name: \u0026#34;net2\u0026#34; # routes: # - \u0026#34;default 10.103.8.20 255.255.255.255 - table 3\u0026#34; # - \u0026#34;10.103.8.128/25 10.103.8.20 255.255.255.255 net2\u0026#34; customer_uan_routes: [] To define the rules:\n## Customer defined networks ifrule-X # customer_uan_rules is a list of interface rules used for constructing # ifrule-\u0026lt;customer_uan_routes.name\u0026gt; files. # # customer_uan_rules: # - name: \u0026#34;net1\u0026#34; # rules: # - \u0026#34;from 10.1.0.0/16 lookup 1\u0026#34; # - name: \u0026#34;net2\u0026#34; # rules: # - \u0026#34;from 10.103.8.0/24 lookup 3\u0026#34; customer_uan_rules: [] To define the global static routes:\n## Customer defined networks global routes # customer_uan_global_routes is a list of global routes used for constructing # the \u0026#34;routes\u0026#34; file. # # customer_uan_global_routes: # - routes: # - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; # - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; customer_uan_global_routes: [] Add the change from the working directory to the staging area.\nncn-w001# git add -A Commit the file to the master branch.\nncn-w001# git commit -am \u0026#39;Added UAN interfaces\u0026#39; Push the commit.\nncn-w001# git push Obtain the commit ID for the commit pushed in the previous step.\nncn-m001# git rev-parse --verify HEAD Update any CFS configurations used by the UANs with the commit ID from the previous step.\na. Download the JSON of the current UAN CFS configuration to a file.\nThis file will be named uan-config-PRODUCT_VERSION.json. Replace PRODUCT_VERSION with the current installed UAN version.\nncn-m001# cray cfs configurations describe uan-config-PRODUCT_VERSION \\ --format=json \u0026gt;uan-config-PRODUCT_VERSION.json b. Remove the unneeded lines from the JSON file.\nThe unneeded lines are:\nthe lastUpdated line the last name line These lines must be removed before uploading the modified JSON file back into CFS to update the UAN configuration.\nncn-m001# cat uan-config-PRODUCT_VERSION.json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-03-27T02:32:10Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;aa5ce7d5975950ec02493d59efb89f6fc69d67f1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;uan-integration-PRODUCT_VERSION\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;uan-config-2.0.1-full\u0026#34; ] } c. Replace the commit value in the JSON file with the commit ID obtained in the previous Step.\nThe name value after the commit line may also be updated to match the new UAN product version, if wanted. This update is not necessary as CFS does not use this value for the configuration name.\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-configmanagement.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;aa5ce7d5975950ec02493d59efb89f6fc69d67f1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;uan-integration-PRODUCT_VERSION\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ] } d. Create a UAN CFS configuration with the updated JSON file.\nThe following example uses uan-config-PRODUCT_VERSION for the name of the new CFS configuration, to match the JSON file name.\nncn-m001# cray cfs configurations update uan-config-PRODUCT_VERSION \\ --file uan-config-PRODUCT_VERSION.json e. Tell CFS to apply the new configuration to UANs by repeating the following command for each UAN. Replace UAN_XNAME in the following command with the name of a different UAN each time the command is run.\nncn-m001# cray cfs components update --desired-config uan-config-PRODUCT_VERSION \\ --enabled true --format json UAN_XNAME Reboot the UAN with the Boot Orchestration Service (BOS).\nThe new interfaces will be available when the UAN is rebooted. Replace the UAN_SESSION_TEMPLATE value with the BOS session template name for the UANs.\nncn-w001# cray bos v1 session create \\ --template-uuid UAN_SESSION_TEMPLATE --operation reboot Verify that the wanted network configuration is correct on each UAN.\n"
},
{
	"uri": "/docs-uan/en-271/operations/configure_pluggable_authentication_modules_pam_on_uans/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Configure Pluggable Authentication Modules (PAM) on UANs Perform this procedure to configure PAM on UANs. PAM enables dynamic authentication support for system services.\nInitialize and configure the Cray CLI tool on the system. See \u0026ldquo;Configure the Cray Command Line Interface (CLI)\u0026rdquo; in the CSM documentation for more information.\nVerify that the Gitea Version Control Service (VCS) is running.\nncn-m001# kubectl get pods --all-namespaces | grep vcs services gitea-vcs-f57c54c4f-j8k4t 2/2 Running 1 11d services gitea-vcs-postgres-0 2/2 Running 0 11d Retrieve the initial Gitea login credentials for the crayvcs user name.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode These credentials can be modified in the vcs_user role prior to installation or can be modified after logging in.\nUse an external web browser to verify the Ansible plays are available on the system.\nThe URL will take on the following format:\nhttps://api.SYSTEM-NAME.DOMAIN-NAME/vcs\nClone the system Version Control Service (VCS) repository to a directory on the system.\nncn-w001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git Change to the uan-config-management directory.\nncn-w001# cd uan-config-management Make a new directory for the PAM configuration.\na. Create a group_vars/all directory if changing all UANs.\nncn-w001# mkdir -p group_vars/all ncn-w001# cd group_vars/all b. Create a host_vars/XNAME directory if the change is node specific.\nncn-w001# mkdir -p host_vars/XNAME ncn-w001# cd host_vars/XNAME Configure PAM.\nThe default path is /etc/pam.d/, so only the module file name is required.\n# vi pam.yml --- uan_pam_modules: - name: pam_module_file_name lines: - \u0026#34;add this line to pam module file_name\u0026#34; - \u0026#34;add another line to pam module file_name\u0026#34; - name: another_pam_module_file_name lines: - \u0026#34;add this line to another_pam_module_file_name\u0026#34; The following is an example of adding the line \u0026quot;account required pam\\_access.so\u0026quot; to the /etc/pam.d/common-account PAM file. The \\t is used to place a tab between account required and pam\\_access.so to match the formatting of the common-account file contents. The quotes are required in the strings used in the lines filed.\n--- uan_pam_modules: - name: common-account lines: - \u0026#34;account required\\tpam_access.so\u0026#34; Add the change from the working directory to the staging area.\nAll UANs:\nncn-w001# git add group_vars/all/pam.yml Node specific:\nncn-w001# git add host_vars/XNAME/pam.yml Commit the file to the master branch.\nncn-w001# git commit -am \u0026#39;Added PAM configuration\u0026#39; Push the commit.\nncn-w001# git push If prompted, use the Gitea login credentials.\nReboot the UAN(s) with the Boot Orchestration Service (BOS).\nncn-w001# cray bos session create \\ --template-uuid UAN_SESSION_TEMPLATE --operation reboot "
},
{
	"uri": "/docs-uan/en-271/operations/boot_uans/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Boot UANs Perform this procedure to boot UANs using BOS so that they are ready for user logins.\nPrerequisites IUF is not being used to automate this process Create UAN Boot Images Procedure Create a BOS session to boot the UAN nodes. Replace uan-sessiontemplate-PRODUCT_VERSION in the following command with the ID of the session template created by the initial HPE Cray Supercomputing UAN product installation or the UAN product upgrade process.\nncn-m001# cray bos session create --template-uuid uan-sessiontemplate-PRODUCT_VERSION --operation reboot --format json | tee session.json { \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;jobId\u0026#34;: \u0026#34;boa-89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;session\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d/status\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;status\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; } ], \u0026#34;operation\u0026#34;: \u0026#34;reboot\u0026#34;, \u0026#34;templateUuid\u0026#34;: \u0026#34;uan-sessiontemplate-PRODUCT_VERSION\u0026#34; } Retrieve the BOS session ID from the output of the previous command.\nncn-m001# export BOS_SESSION=$(jq -r \u0026#39;.links[] | select(.rel==\u0026#34;session\u0026#34;) | .href\u0026#39; session.json | cut -d \u0026#39;/\u0026#39; -f4) ncn-m001# echo $BOS_SESSION 89680d0a-3a6b-4569-a1a1-e275b71fce7d Retrieve the Boot Orchestration Agent (BOA) Kubernetes job name for the BOS session.\nncn-m001# BOA_JOB_NAME=$(cray bos session describe $BOS_SESSION --format json | jq -r .job) Retrieve the Kubernetes pod name for the BOA assigned to run this session.\nncn-m001# BOA_POD=$(kubectl get pods -n services -l job-name=$BOA_JOB_NAME --no-headers -o custom-columns=\u0026#34;:metadata.name\u0026#34;) View the logs for the BOA to track session progress.\nncn-m001# kubectl logs -f -n services $BOA_POD -c boa List the CFS sessions started by the BOA. Skip this step if CFS was not enabled in the boot session template used to boot the UANs.\nIf CFS was enabled in the boot session template, the BOA will initiate a CFS session.\nIn the following command, pending and complete are also valid statuses to filter on.\nncn-m001# cray cfs sessions list --tags bos_session=$BOS_SESSION --status running --format json "
},
{
	"uri": "/docs-uan/en-271/operations/cve-2023-0461/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Mitigation for CVE-2023-0461 Description There is a use-after-free vulnerability in the Linux Kernel which can be exploited to achieve local privilege escalation. To reach the vulnerability kernel configuration flag CONFIG_TLS or CONFIG_XFRM_ESPINTCP has to be configured, but the operation does not require any privilege. There is a use-after-free bug of icsk_ulp_data of a struct inet_connection_sock. When CONFIG_TLS is enabled, a user can install a TLS context (struct tls_context) on a connected TCP socket. The context is not cleared if this socket is disconnected and reused as a listener. If a new socket is created from the listener, the context is inherited and vulnerable. The setsockopt TCP_ULP operation does not require any privilege. We recommend upgrading past commit 2c02d41d71f90a5168391b6a5f2954112ba2307c\nCVE-2023-0461\nStatus from SUSE: https://www.suse.com/security/cve/CVE-2023-0461.html\nMitigation While the underlying CVE is being addressed by SUSE, UANs can mitigate this issue by unbonding the CAN (if it is being used), and unloading the TLS kernel module after blocking the kernel from being loaded again.\nThe following mitigation script will perform the following actions:\nSelect the first BONDING_SLAVE0 from ifcfg-bond0 if it exists Replace bond0 in ifcfg-can0 with the BONDING_SLAVE0 interface Reload interfaces Add a deny list for the tls module Unload bonding and tls kernel modules provided mlx5_core is not present Fail if mlx5_core is detected to highlight that the mitigation failed Important: This mitigation is intended for UANs that meet the following criteria:\nThe UANs are connected to HPE Aruba network switches (lcap-individual must be set for other switch types to allow for unbonded CAN connections) The UANs use HPE Slingshot HSN NICs and not Mellanox ConnectX-5 HSN NICs. Procedure Update the active CFS configuration with the following changes so that Node Personalization applies the change to the UANs:\ndiff --git a/mitigate-uan-cve-2023-0461.sh b/mitigate-uan-cve-2023-0461.sh new file mode 100755 index 0000000..1ced634 --- /dev/null +++ b/mitigate-uan-cve-2023-0461.sh @@ -0,0 +1,32 @@ +#!/bin/bash + +# Select the BONDING_SLAVE0 as the unbonded interface and create a new ifcfg file +if [ -f /etc/sysconfig/network/ifcfg-bond0 ] \u0026amp;\u0026amp; grep -q BONDING_SLAVE0 /etc/sysconfig/network/ifcfg-bond0; then + ifname=$(grep BONDING_SLAVE0 /etc/sysconfig/network/ifcfg-bond0 | awk -F= \u0026#39;{print $2}\u0026#39; | tr -d \\\u0026#39;\\\u0026#34;) + sed -i -e \u0026#34;s/bond0/$ifname/g\u0026#34; /etc/sysconfig/network/ifcfg-can0 + cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/sysconfig/network/ifcfg-$ifname +STARTMODE=\u0026#39;auto\u0026#39; +BOOTPROTO=\u0026#39;static\u0026#39; +EOF + rm /etc/sysconfig/network/ifcfg-bond0 +fi + +# Reload interfaces to bring up the unbonded can +wicked ifreload all + +# Create a blacklist file and unload bonding tls +cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/modprobe.d/66-blacklist-tls.conf +blacklist tls +install tls /bin/true +EOF + +# This will fail on mellanox systems as mlx5_core depends on tls +# Failure of this script will in +if modinfo mlx5_core \u0026amp;\u0026gt; /dev/null; then + rmmod bonding tls +else + echo \u0026#34;Can\u0026#39;t rmmod tls as mlx5_core depends on it\u0026#34; + exit 1 +fi + +exit 0 diff --git a/site.yml b/site.yml index 674ab30..ad166cf 100644 --- a/site.yml +++ b/site.yml @@ -92,6 +92,8 @@ - uan_interfaces - uan_ldap - uan_hardening + - name: Bond interface mitigation + script: mitigate-uan-cve-2023-0461.sh After UAN images are built to address the CVE, remove this mitigation script.\n"
},
{
	"uri": "/docs-uan/en-271/advanced/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "advanced Topics: Enabling the Customer Access Network (CAN) or the Customer High Speed Network (CHN) Repurposing a Compute Node as a UAN Booting an Application Node with a SLE HPC Image (Technical Preview) Designating Application Nodes for K3s (Technical Preview) Configuring a UAN for K3s (Technical Preview) "
},
{
	"uri": "/docs-uan/en-271/advanced/enabling_can_chn/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Enabling the Customer Access Network (CAN) or the Customer High Speed Network (CHN) The HPE Cray Supercomputing UAN product provides a role, uan_interfaces in the Configuration Framework Service (CFS). This role is suitable for Application type nodes, and in some circumstances, configuring the CHN on Compute type nodes.\nEnable CAN or CHN by setting the following in the UAN CFS repo (the filename may be modified to whatever is appropriate):\nncn-m001:~/ $ cat group_vars/Application_UAN/can.yml uan_can_setup: true SLS will be configured with either CAN or CHN, uan_interfaces will use this setting to determine if a default route will be established over the NMN (CAN) or the HSN (CHN). To see how the site is configured, query SLS:\nncn-m001:~/ $ cray sls search networks list --name CHN --format json | jq -r \u0026#39;.[] | .Name\u0026#39; CHN (Optional) If the compute nodes are going to use the UAN CFS role uan_interfaces to set a default route on the CHN, make sure that there is an appropriate ansible setting for the compute nodes and the UANs:\nncn-m001:~/ $ cat group_vars/Compute/can.yml uan_can_setup: true After uan_can_setup has been enabled, update the CFS configuration used for the nodes to initiate a reconfiguration (see the Configuration Management section of the CSM documentation for more information).\nThe CSM documentation provides additional resources to validate the configuration of CAN and CHN for UANs and Computes. Consult the section titled \u0026ldquo;Enabling Customer High Speed Network Routing\u0026rdquo; in the CSM documentation for more information.\n"
},
{
	"uri": "/docs-uan/en-271/advanced/repurposing_compute_as_uan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Repurposing a Compute Node as a UAN This section describes how to repurpose a compute node to be used as a User Access Node (UAN). This reconfiguration is typically done when the processor type of the compute node is not yet available in a UAN server.\nOverview The following steps outline the process of repurposing a compute node to be used as a UAN.\nVerify that the System Default Route is set to CHN.\nChange the role of the compute node in the Hardware State Manager from Compute to Application and set the subrole to UAN.\nEnsure that IP addresses on the CHN exist for the compute nodes in SLS.\nBoot the repurposed compute node as a UAN.\nVerify the repurposed compute node functions as a UAN.\nPrerequisites There are no changes needed in hardware, network cabling, or UEFI/BIOS/BMC configuration to repurpose a compute node for use as a UAN. However, compute nodes do not have the necessary NICs to support user access over the Customer Access Network (CAN). Additionally, the network configuration of Mountain Cabinets does not support the CAN network. Therefore, repurposing a compute node as a UAN requires the system to be configured to use the Customer High-Speed Network (CHN). The compute nodes must also have a CHN IP address in SLS.\nThe SLS Networks setting for the SystemDefaultRoute must be CHN The repurposed compute nodes must have CHN IP addresses in SLS uan_can_setup must be set to true in the uan-config-management repo Procedure Perform the following steps to repurpose a compute node for use as a UAN.\nLog in to the master node ncn-m001. All commands in this procedure are run from the master node.\nVerify that the system is configured to use the CHN as the System Default Route. If the SystemDefaultRoute is not CHN, the compute nodes may not be repurposed as UAN.\nncn-m001# cray sls networks describe BICAN --format json | jq -r \u0026#39;.ExtraProperties.SystemDefaultRoute\u0026#39; Verify that a CHN IP address exists in SLS for each repurposed compute node. Repeat the following command and replace \u0026lt;XNAME\u0026gt; with the xname of each repurposed compute node. The compute node must have a CHN IP address in SLS or it cannot be repurposed as a UAN. See Add Compute IP addresses to CHN SLS data section of the Cray System Management documentation for information on adding compute nodes to the CHN.\nncn-m001# cray sls networks describe CHN | q -r \u0026#39;.ExtraProperties.Subnets[] | select(.FullName == \u0026#34;CHN Bootstrap DHCP Subnet\u0026#34;) | .IPReservations[] | select(.Comment == \u0026#34;\u0026lt;XNAME\u0026gt;\u0026#34;)\u0026#39; Verify that uan_can_setup: true is set in the uan-config-management CFS repo. See Enabling the Customer Access Network (CAN) or the Customer High Speed Network (CHN) for more information.\nChange the role and subrole in HSM of the compute node or nodes being repurposed as UANs to Application and UAN, respectively. Repeat the following command and replace \u0026lt;XNAME\u0026gt; with the xname of each repurposed compute node.\nncn-m001# cray hsm state components role update --role Application --sub-role UAN \u0026lt;XNAME\u0026gt; Verify the role and subrole in HSM of the repurposed compute node or nodes have been changed to \u0026lsquo;Application and 'UAN, respectively. Repeat the following command and replace \u0026lt;XNAME\u0026gt; with the xname of each repurposed compute node.\nncn-m001# cray hsm state components describe \u0026lt;XNAME\u0026gt; Run the BOS session template used to boot the UAN nodes. See Boot UAN Nodes for more information on booting UAN nodes with BOS. Replace \u0026lt;UAN_SESSIONTEMPLATE\u0026gt; with the name of the BOS session template used to boot the UAN nodes and \u0026lt;XNAME\u0026gt; with the xname of the repurposed compute node.\nncn-m001# cray bos session create --template-uuid \u0026lt;UAN_SESSIONTEMPLATE\u0026gt; --operation reboot --limit \u0026lt;XNAME\u0026gt; Verification as a UAN After the repurposed compute node is booted as a UAN, the following steps will verify it is configured as a UAN. These steps may vary dependent upon how the site has configured the UAN nodes.\nBasic UAN Configuration Checks Verify that the repurposed compute node has finished the configuration phase. This command must return configured.\nncn-m001# cray cfs components describe \u0026lt;XNAME\u0026gt; --format json | jq -r .configurationStatus Login to the repurposed compute node from the master node ncn-m001 as the root user.\nVerify that the hsn0 interface has the CHN IP address assigned to it in SLS.\nuan# ip a | grep hsn0 Verify that the default route is through hsn0\nuan# ip r | grep default Verify that all site UAN filesystems are mounted.\nCommon UAN Configuration Checks If LDAP is used for user authentication, verify that the LDAP service is reachable.\nuan# ping \u0026lt;ldap_service_ip\u0026gt; If Slurm is used, test sinfo and srun commands. This example srun command should return the hostname of four compute nodes.\nuan# sinfo uan# srun -N4 hostname Verify that Users can Login Login to the repurposed compute node as an authorized non-root user from any host that should have UAN access.\nIf Slurm is used, test sinfo and srun commands. This example srun command should return the hostname of four compute nodes.\nuan# sinfo uan# srun -N4 hostname "
},
{
	"uri": "/docs-uan/en-271/advanced/designating_application_nodes_for_k3s/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Designating Application Nodes for K3s (Technical Preview) WARNING: This feature is a Technical Preview. Future releases will streamline these manual configuration steps. Therefore, some of these configuration options may change in future releases.\nOverview Before K3s can be enabled to support UAIs on Application nodes, the Application nodes must be grouped in HSM as either k3s_server or k3s_agent nodes. The UAN Ansible K3s playbook uses these groups to determine what role they will have. Nodes grouped as k3s_server will become K3s control-plane (master) nodes, while nodes grouped as k3s_agent will become K3s agent (worker) nodes.\nWhen assigning roles, carefully consider the number of k3s_server nodes such that a quorum is maintained. A minimum of three k3s_server nodes are required for a quorum. Consult K3s High Availability documentation for more information.\nINFO: For more information on how CFS uses HSM node groups to create Ansible host groups, see the Cray System Management Documentation. Follow the links to the Cray System Management Administration Guide-\u0026gt;Configuration Management-\u0026gt;Ansible Inventory-\u0026gt;Dynamic inventory and host groups section.\nWhen HPE Cray Supercomputing UAN software is installed or upgraded using IUF, and these HSM node groups do not exist or have no members, one Application_UAN node type will be placed in the k3s_server group. The remaining nodes will be placed in the k3s_agent group. If these groups exist and are not empty, IUF will not change them.\nThis document provides procedures to manually change the membership of the k3s_server and k3s_agent HSM node groups.\nProcedures Getting a List of all Application_UAN Nodes This command gathers all nodes with the Application role and UAN subrole from HSM. The output is sorted by XNAME.\n$ cray hsm state components list \\ --role Application --subrole UAN \\ --format json | jq -r \u0026#39;.Components[].ID\u0026#39; \\ | sort Getting the Members of an HSM Node Group This command returns the members of a given HSM Node Group. Substitute either k3s_server or k3s_agent for GROUP_NAME.\n$ cray hsm groups members list $GROUP_NAME Creating the HSM Node Group If the HSM Node Group does not exist, the cray hsm groups create command can create it. The following two examples show how group members may be provided as a comma-separated string of XNAMEs on the command line, or listed in a file.\nNOTE: HSM Node Groups used for K3s must be exclusive. That is, a given node may be in either the k3s_server or k3s_agent group, not both. HSM provides the exclusive-group element to enforce this policy. By having the k3s_server and k3s_agent groups have the same k3s exclusive-group, nodes are prevented from being members of both.\nThis example creates the k3s_server group listing members on the command line.\n$ cray hsm groups create \\ --members-ids \u0026#34;XNAME1,XNAME2,...\u0026#34; \\ --exclusive-group \u0026#34;K3s\u0026#34; \\ --description \u0026#34;K3s Control-Plane Nodes\u0026#34;\\ --label \u0026#34;k3s_server\u0026#34; This example creates the k3s_agent group using members in a text file.\n$ cray hsm groups create \\ --members-file /path-to-member-file \\ --exclusive-group \u0026#34;K3s\u0026#34; \\ --description \u0026#34;K3s Agent Nodes\u0026#34;\\ --label \u0026#34;k3s_agent\u0026#34; Modifying the HSM Node Group The cray hsm groups members create and cray hsm groups members delete commands modify membership of an existing HSM Node Group. In these examples, GROUP_NAME would be either k3s_server or k3s_agent.\nAdding a Member $ cray hsm groups members create --id XNAME $GROUP_NAME Deleting a Member $ cray hsm groups members delete XNAME $GROUP_NAME Deleting an HSM Node Group This command deletes the node group contained in GROUP_NAME.\n$ cray hsm groups delete $GROUP_NAME "
},
{
	"uri": "/docs-uan/en-271/advanced/enabling_k3s/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Configuring a UAN for K3s (Technical Preview) WARNING: This feature is a Technical Preview, as such it requires completion of the Prerequisites section. Future releases will streamline these manual configuration steps and enhance the experience of using the rootless Podman containers. Therefore, some of these configuration options may change in future releases.\nUAI Experience on UANs In HPE Cray Supercomputing UAN release 2.6, a new playbook has been added to create a single node, K3s cluster. This K3s environment can then run the services necessary to replicate the experience of User Access Instances (UAIs) on one or more UANs.\nUse of K3s K3s will serve as the orchestrator of services necessary to replicate the capabilities of UAIs on UAN hardware. These services include HAProxy, MetalLB, and eventually DNS services like ExternalDNS and PowerDNS. Notably, this does not orchestrate instances of sshd and podman containers through K3s. K3s and the initial set of services mimic how the \u0026ldquo;Broker UAIs\u0026rdquo; in CSM to handle the SSH ingress and redirection of users into their interactive environment.\nUse of Podman Traditional UAIs in CSM required some level of privilege in CSM for access to host volume mounts, networking, and startup activities. Podman containers offer an attractive solution for an interactive environment in which to place users. They can be rootless containers that do not rely on privilege escalation. When running on UANs, Podman containers have access to a hosting environment that is already tailored to users.\nOverview The overall component flow for replicating containerized environments for End-Users on UANs is as follows:\nA user uses ssh to initiate a connection to the HAProxy load-balancer running in K3s. HAProxy, using the configured load balancing algorithms, will forward the SSH connection to an instance of sshd running on a UAN. sshd, running on a UAN through systemd, will initiate a rootless Podman container as the user using the ForceCommand configuration. The user is placed in a Podman container for an interactive session, or their SSH_ORIGINAL_COMMAND is run in the container. When the user disconnects, the Podman process exits, and the container is removed. There are alternate configurations of Podman that would allow for different workflows. For example, the main pid of the container could be long running, to facilitate easier re-entry to the container on subsequent logins.\nPrerequisites The following steps must be completed prior to configuring the UAN with K3s.\nDesignate a UAN to operate as the K3s control-plane node. See Designating Application Nodes for K3s.\nIdentify a pool of IP addresses for the services running in K3s.\nNote: The identification of a pool of IP addresses for the services running in K3s is best made at system installation time in order to avoid the possibility of IP collisions. If this is not possible, steps must be taken to ensure IP collisions are avoided.\nThis address pool must be routable from the UAN control-plane. It is suggested to split the [CAN|CHN] Dynamic MetalLB subnet into two subnets, [CAN|CHN] Dynamic MetalLB and [CAN|CHN] Dynamic MetalLB K3s, the latter being used for K3s services. Subnet definitions are initially uploaded to SLS at installation time by CSI. If the [CAN|CHN] Dynamic MetalLB subnet is split after initial installation, steps must be taken to ensure that IPs in the newly created [CAN|CHN] Dynamic MetalLB K3s subnet are not in use and the MetalLB Controller pod in CSM must be restarted. Alternatively, the starting and ending IPs for K3s may be defined directly in vars/uan_helm.yml. These steps are described below.\nHere is an example of splitting the existing [CAN|CHN] Dynamic MetalLB subnet. By default, the subnet with the FullName attribute of [CAN|CHN] Dynamic MetalLB K3s will be used for K3s, depending on whether CAN or CHN is being used for the customer access network. This default FullName may be overridden by setting the sls_can_metallb_fullname variable in CFS to the expected name.\nImportant: Before splitting [CAN|CHN] Dynamic MetalLB, be sure to verify none of the IP Addresses in the new [CAN|CHN] Dynamic MetalLB K3s subnet are being used. The ims and user namespaces are the most likely to contain these IPs.\nkubectl get services -n ims kubectl get services -n user kubectl get services -A Edit SLS to make the following changes in the CAN or CHN network allocations:\n### Existing [CAN|CHN] Dynamic MetalLB Subnet in SLS ### Split this as shown below into two subnets \u0026#34;CIDR\u0026#34;: \u0026#34;x.x.x.192/26\u0026#34;, \u0026#34;FullName\u0026#34;: \u0026#34;CAN Dynamic MetalLB\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;x.x.x.193\u0026#34;, \u0026#34;MetalLBPoolName\u0026#34;: \u0026#34;customer-access\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;can_metallb_address_pool\u0026#34;, \u0026#34;VlanID\u0026#34;: 6, ### New [CAN|CHN] Dynamic MetalLB Subnet \u0026#34;CIDR\u0026#34;: \u0026#34;x.x.x.192/27\u0026#34;, \u0026#34;FullName\u0026#34;: \u0026#34;CAN Dynamic MetalLB\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;x.x.x.193\u0026#34;, \u0026#34;MetalLBPoolName\u0026#34;: \u0026#34;customer-access\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;can_metallb_address_pool\u0026#34;, \u0026#34;VlanID\u0026#34;: 6, ### New [CAN|CHN] Dynamic MetalLB K3s \u0026#34;CIDR\u0026#34;: \u0026#34;x.x.x.224/27\u0026#34;, \u0026#34;FullName\u0026#34;: \u0026#34;CAN Dynamic MetalLB K3s\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;x.x.x.225\u0026#34;, \u0026#34;MetalLBPoolName\u0026#34;: \u0026#34;customer-access-k3s\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;can_metallb_k3s_address_pool\u0026#34;, \u0026#34;VlanID\u0026#34;: 6, Reallocate the CSM MetalLB pool customer-access from CSM to reflect the SLS changes above. To shrink the customer-access pool in CSM, edit the configmap and pick the new CIDR block for the [CAN|CHN] Dynamic MetalLB subnet in SLS. In this example the CIDR block was x.x.x.193/26 and is now x.x.x.193/27:\n# kubectl edit -n metallb-system cm/metallb ... data: config: | address-pools: - addresses: - x.x.x.193/27 name: customer-access protocol: bgp ... To complete migrating the IP Address range out of CSM, restart the MetalLB Controller pod in CSM.\nkubectl delete pod -n metallb-system -l app.kubernetes.io/component=controller Here\u0026rsquo;s an example of defining the MetalLB IP pool range directly in vars/uan_helm.yml.\nmetallb_ipaddresspool_range_start: \u0026#34;\u0026lt;start-of-range\u0026gt;\u0026#34; metallb_ipaddresspool_range_end: \u0026#34;\u0026lt;end-of-range\u0026gt;\u0026#34; By default, these arguments are commented out or omitted. MetalLB will be able to start, but the Custom Resource Definition for IPAddressPool and L2AdvertisementAddress will not be created and no Load Balancer IP address will be allocated.\nThis pool will allow for external LoadBalancer IP address to be assigned to services like HAProxy. Initially, these IP addresses will serve as the SSH ingress for instances of HAProxy.\nConfigure and create the /etc/subuid and /etc/subgid files.\nTo allow for users to run rootless Podman containers, these files must be present and configured with an entry for each user. These files must be uploaded to the user S3 bucket:\n$ cray artifacts list user --format json { \u0026#34;artifacts\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;subuid\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2023-02-21T23:41:43.948000+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;c543aebb9b40bcf48879885734447090\\\u0026#34;, \u0026#34;Size\u0026#34;: 145686, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34;, \u0026#34;Owner\u0026#34;: { \u0026#34;DisplayName\u0026#34;: \u0026#34;User Service User\u0026#34;, \u0026#34;ID\u0026#34;: \u0026#34;USER\u0026#34; } }, { \u0026#34;Key\u0026#34;: \u0026#34;subgid\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2023-02-21T23:41:43.948000+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;73032ede132e44d2c1bc567246901737\\\u0026#34;, \u0026#34;Size\u0026#34;: 145686, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34;, \u0026#34;Owner\u0026#34;: { \u0026#34;DisplayName\u0026#34;: \u0026#34;User Service User\u0026#34;, \u0026#34;ID\u0026#34;: \u0026#34;USER\u0026#34; } } ] } Place a Podman container image suitable for users within a container image registry accessible from the UAN.\nConfiguring with Configuration Framework Service (CFS) After completing the Prerequisites section, the following are available to proceed with configuring a UAN to run K3s.\nA fully configured UAN An IPAddress start and end range to assign to MetalLB Prepared subuid and subgid files Configuration Files and Playbook Configuration for K3s and related services are controlled in the following ansible files in the UAN VCS repository:\n$ ls uan-config-management/vars/ | egrep \u0026#34;k3s|sshd|helm\u0026#34; uan_helm.yml uan_k3s.yml uan_sshd.yml The ansible playbook to install and configure K3s may be found here:\n$ ls uan-config-management | grep k3s k3s.yml Ansible Roles The following Ansible roles are provided in the uan-config-management repository in VCS. There are README.md files in each Ansible role directory in this repository that provide further details.\nuan-k3s-install: Download and stage K3s assets necessary to initialize and configure k3s on a node uan-k3s-stage: Install and configure K3s on a node uan-helm: Perform tasks to initialize an environment to install helm charts uan-haproxy: Deploy a list of HAProxy charts to the k3s cluster uan-metallb: Deploy the MetalLB chart to the k3s cluster uan-sshd: Create and enable instances of sshd with systemd uan-podman: Prepare node for rootless podman Artifactory Assets UAN uploads artifacts to deploy to the UAN control-plane node in the nexus repository:\nuan-@product_version@-third-party The following Nexus group repository is created and references the previous UAN Nexus raw repos.\nuan-@product_version_short@-third-party This repository will contain the installer for K3s and Helm charts for HAProxy and MetalLB.\nValidation Tests To validate the K3s cluster after it is deployed, see the Validation Checks section of this document for details.\nConfiguring K3s, MetalLB, HAProxy, and SSHD for use with Podman Each of the following sections describes how the various components deployed to K3s and the UANs may be configured to enable users to SSH to rootless Podman containers. As there is no one configuration to fit any one use case, read and understand each section to modify the configuration as needed. After each section has been completed, see Deploy K3s to the UAN.\nMetalLB By default, metallb_ipaddresspool_range_start and metallb_ipaddresspool_end will be defined from the [CAN|CHN] Dynamic MetalLB K3s subnet in SLS, if it exists. Alternatively, they may be configured directly in vars/uan_helm.yml:\n$ grep \u0026#34;^metallb_ipaddresspool\u0026#34; vars/uan_helm.yml metallb_ipaddresspool_range_start: \u0026#34;x.x.x.x\u0026#34; metallb_ipaddresspool_range_end: \u0026#34;x.x.x.x\u0026#34; MetalLB will assign an IP address to each service running in K3s that requires and external IP address. For HAProxy, each instance of HAProxy will require an IP address. Podman containers do not require their own IP address.\nHAProxy Configuration Each SSH ingress is backed by a K3s deployment of HAProxy. By default, a single instance of HAProxy is enabled in vars/uan_helm.yml:\nuan_haproxy: - name: \u0026#34;haproxy-uai\u0026#34; namespace: \u0026#34;haproxy-uai\u0026#34; chart: \u0026#34;{{ haproxy_chart }}\u0026#34; chart_path: \u0026#34;{{ helm_install_path }}/charts/{{ haproxy_chart }}.tgz\u0026#34; args: \u0026#34;--set service.type=LoadBalancer\u0026#34; This file must be further configured with additional values to populate the HAProxy configuration. For example, to load-balance SSH to three UANs, the following configuration changes must be made:\nuan_haproxy: - name: \u0026#34;haproxy-uai\u0026#34; namespace: \u0026#34;haproxy-uai\u0026#34; chart: \u0026#34;{{ haproxy_chart }}\u0026#34; chart_path: \u0026#34;{{ helm_install_path }}/charts/{{ haproxy_chart }}.tgz\u0026#34; args: \u0026#34;--set service.type=LoadBalancer\u0026#34; config: | global log stdout format raw local0 maxconn 1024 defaults log global mode tcp timeout connect 10s timeout client 36h timeout server 36h option dontlognull listen ssh bind *:22 balance leastconn mode tcp option tcp-check tcp-check expect rstring SSH-2.0-OpenSSH.* server uan01 uan01.example.domain.com:9000 check inter 10s fall 2 rise 1 server uan02 uan02.example.domain.com:9000 check inter 10s fall 2 rise 1 server uan03 uan03.example.domain.com:9000 check inter 10s fall 2 rise 1 This example that must be tailored to the wanted configuration. See the SSHD Configuration section to create new instances of SSHD to respond to HAProxy connections outside of the standard SSHD running on port 22.\nFor more information on HAProxy configurations, see HAProxy Configuration\nTo enable additional instances of HAProxy representing alternate configurations, add an element to the list uan_haproxy.\nSSHD Configuration The role uan_sshd runs in the playbook k3s.yml to start and configure new instances of SSHD to respond to HAProxy forwarded connections. Each new instance of SSHD is defined in vars/uan_sshd.yml as an element in the list uan_sshd_configs:\nuan_sshd_configs: - name: \u0026#34;uai\u0026#34; config_path: \u0026#34;/etc/ssh/uan\u0026#34; port: \u0026#34;9000\u0026#34; This configuration will create a systemd unit file /usr/lib/systemd/system/sshd_uai.service and will mark the service as enabled. An SSH configuration file will also be created at /etc/ssh/uan/sshd_uai_config to start sshd listening on port 9000.\nThis default configuration will simply place users into their standard shell. To create a rootless Podman container upon logging in, specify an alternate configuration:\n- name: \u0026#34;uai\u0026#34; config_path: \u0026#34;/etc/ssh/uan\u0026#34; port: \u0026#34;9000\u0026#34; config: | Match User * AcceptEnv DISPLAY X11Forwarding yes AllowTcpForwarding yes PermitTTY yes ForceCommand podman --root /scratch/containers/$USER run -it -h uai --cgroup-manager=cgroupfs --userns=keep-id --network=host -e DISPLAY=$DISPLAY registry.local/cray/uai:latest Note: In the example above, the image registry.local/cray/uai:latest was provided as an example, but this example must be modified to reference an available container image.\nDeploy K3s to the UAN After the VCS repository has been updated with the appropriate values, generate a new image and reboot the UAN.\nAlternatively, update the active CFS configuration on a single running UAN to include the k3s.yml playbook and uncomment out the following line from k3s.yml:\ntasks: - name: Application node personalization play include_role: name: \u0026#34;{{ item }}\u0026#34; with_items: #- uan_k3s_stage # Uncomment to stage K3s assets without Image Customization - uan_k3s_install This configuration will download the necessary assets without requiring an image rebuild.\nAfter the node has been booted and configured, proceed with the Validation Checks section to verify the components have been configured correctly.\nValidation Checks K3s Validation To verify the k3s.yml playbook succeeded, perform the following verification checks.\nVerify kubectl from the UAN.\nuan01:~ # export KUBECONFIG=~/.kube/k3s.yml uan01:~ # kubectl get nodes NAME STATUS ROLES AGE VERSION uan01 Ready control-plane,master 3h58m v1.26.0+k3s1 Verify HAProxy and MetalLB are installed with helm\nuan01:~ # export KUBECONFIG=~/.kube/k3s.yml uan01:~ # helm ls -A NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION haproxy-uai\thaproxy-uai 1 2023-03-01 10:55:10.916137137 -0600 CST\tdeployed\thaproxy-1.17.3\t2.6.6 metallb metallb-system\t1 2023-03-01 10:40:15.548380973 -0600 CST\tdeployed\tmetallb-0.13.7\tv0.13.7 Check pod status of HAProxy and MetalLB\nuan01:~ # kubectl get pods -A | egrep \u0026#34;haproxy|metallb\u0026#34; metallb-system metallb-controller-5b89f7554c-mzjvt 1/1 Running 0 4h1m metallb-system metallb-speaker-ltnkx 1/1 Running 0 4h1m haproxy-uai haproxy-uai-7kg6p 1/1 Running 0 3h46m Verify MetalLB has assigned an external IP address to HAProxy:\nuan01:~ # kubectl get services -A -l app.kubernetes.io/name=haproxy NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE haproxy-uai haproxy-uai LoadBalancer x.x.x.x x.x.x.x 22:30886/TCP 3h47m Verify that the new instance of SSHD is running:\nuan01:~ # systemctl status sshd_uai  sshd_uai.service - OpenSSH Daemon Generated for uai Loaded: loaded (/usr/lib/systemd/system/sshd_uai.service; disabled; vendor preset: disabled) Active: active (running) since Wed 2023-03-01 12:43:31 CST; 2h 4min ago Finally, use SSH to log in through the HAProxy load balancer:\n$ ssh x.x.x.x Trying to pull registry.local/cray/uai:1.0... Getting image source signatures Copying blob 07a88a2f44f8 done Copying blob 99a1d1c8ca98 done Copying blob a028e278fdc1 done Copying blob 5caad07f5e12 done Copying blob 157b0fca679c done Copying config de48e6a913 done Writing manifest to image destination Storing signatures sh-4.4$ "
},
{
	"uri": "/docs-uan/en-271/upgrade/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "upgrade Topics: Notable Changes "
},
{
	"uri": "/docs-uan/en-271/upgrade/notable_changes/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Notable Changes The following guide describes changes included in a particular UAN version that may be of note during an install or upgrade.\nWhen an upgrade is being performed, review the notable changes for all the UAN versions up to the version being installed. If a particular version does not appear in this guide, it may have only had minor changes. For a full account of the changes involved in a release, consult the ChangeLog.md file at the root of the UAN product repository.\nUAN 2.2 UAN 2.2 was an internal release and was not made generally available. UAN 2.3.1 UAN 2.3 no longer ships a default recipe or image. To build a UAN image, administrators should select a COS recipe to build as the base of their UAN and Application Nodes. The role uan_packages will now install the rpms needed by UAN and Application Nodes The role uan_packages supports GPG checking when the CSM version is 1.2 or greater. uan_disable_gpg_check: yes must be set if CSM is earlier than 1.2 uan_disable_gpg_check: no must be set if CSM is 1.2 or greater UAN 2.4.0 UAN 2.4.0 adds support for a Bifurcated Customer Access Network (BiCAN) and the ability to specify a default route other than the CAN or CHN when they are selected. Application nodes may now choose to implement user access over either the existing Customer Access Network (CAN), the new Customer High Speed Network (CHN), or a direct connection to the customer user network. By default, a direct connection is selected as it was in previous releases. uan_can_setup, when set to yes, selects the customer access network implementation based on the setting of the BICAN System Default Route in SLS. Application nodes may now set a default route other than the CAN or CHN default route when uan_can_setup: yes. uan_customer_default_route: true will allow a customer-defined default route to be set using the customer_uan_routes structure when uan_can_setup is set to yes. sat bootprep is now used in the documentation to streamline the IMS, CFS, and BOS commands to create and customize images and creating sessiontemplate files. UAN 2.4.1 The UAN CFS playbook now supports a section for Compute nodes. The Compute section will run the role uan_interfaces to provide Customer High-Speed Network (CHN) routing. CHN on the Compute nodes requires: Customer High Speed Network has been enabled in CSM. See \u0026ldquo;Enabling Customer High Speed Network Routing\u0026rdquo; in the CSM Documentation UAN CFS configured with uan_can_setup: yes Fully configured HSN SLS has IP assignments for compute nodes on hsn0 Updates to GPU roles to match COS 2.3 UAN 2.4.2 There is a known issue with the version of GPU support included in the UAN CFS repo. The result is that both AMD and Nvidia SDKs will not project at the same time. Until this issue is resolved in a later release, modify the site.yml in the UAN CFS repo to only include either AMD or Nvidia. UAN 2.4.3 A new CFS role, uan_hardening adds iptables rules that will block SSH traffic to NCNs. See the README.md in the uan_hardening role for more information. UAN 2.5.3 A technical preview of a standard SLES image for UAN/Application nodes is included. Support SLES15SP4 COS based images UAN 2.6.0 UAN CFS configurations now require a CSM and two COS layers. Roles that were duplicated from COS CFS in the UAN CFS repo have been removed. Values for COS CFS roles that were previously set in the UAN CFS group_vars directory should now be set in COS CFS group_vars UAN CFS has been restructured to work for COS and Standard SLES images uan_packages variables are now vars/uan_packages.yml and vars/uan_repos.yml and have been renamed. Admins must migrate to the new settings. The NMN connection now supports bonding (optional). The default is a non-bonded single interface. "
},
{
	"uri": "/docs-uan/en-271/troubleshooting/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "troubleshooting Topics: Troubleshoot UAN Boot Issues Troubleshoot UAN CFS and Network Configuration Issues Troubleshoot UAN Disk Configuration Issues "
},
{
	"uri": "/docs-uan/en-271/troubleshooting/troubleshoot_uan_boot_issues/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAN Boot Issues The UAN boot process BOS boots UANs. BOS uses session templates to define various parameters such as:\nWhich nodes to boot Which image to boot Kernel parameters Whether to perform post-boot configuration (Node Personalization) of the nodes by CFS. Which CFS configuration to use if Node Personalization is enabled. UAN boots are performed in three phases:\nPXE booting an iPXE binary that will load the initrd of the UAN image that will boot. Booting the initrd (dracut) image which configures the UAN for booting the UAN image. This process consists of two phases. Configuring the UAN node to use the Content Projection Service (CPS) and Data Virtualization Service (DVS). These services manage the UAN image rootfs mounting and make that image available to the UAN nodes. Mounting the rootfs Booting the UAN image rootfs. PXE Issues Most PXE boot failures are the result of misconfigured network switches, BIOS settings or both. The UAN must PXE boot over the Node Management Network (NMN) and the switches must be configured to allow connectivity to the NMN. The cable for the NMN must be connected to the first port of the OCP card on HPE DL325 and DL385 servers. See \u0026ldquo;Prepare for UAN Product Installation\u0026rdquo; in the UAN Installation Guide for details on the switch and BIOS settings required to configure the UAN for PXE booting.\nUANs may fail to boot when the BIOS EFITIME is too far away from the time on management nodes. If there are x509 certificate problems, check that the BIOS time is correct. See \u0026ldquo;Configure the BIOS of an HPE UAN\u0026rdquo; in the UAN Installation Guide for examples of checking settings in the BIOS.\nInitrd (Dracut) Issues Dracut failures are often caused by the wrong interface being named nmn0, or to multiple entries for the UAN xname in DNS. The latter is a result of multiple interfaces making DHCP requests. Either condition can cause IP address mismatches in the dvs_node_map. DNS configures entries based on DHCP leases.\nWhen dracut starts, it renames the network device named by the ifmap=netX:nmn0 kernel parameter to nmn0. This interface is the only one dracut will enable DHCP on. The ip=nmn0:dhcp kernel parameter limits dracut to DHCP only nmn0. The ifmap value must be set correctly in the kernel_parameters field of the BOS session template.\nSee Create UAN Boot Images for details on how to configure the BOS session template manually when not using the IUF automation. For UAN nodes that have more than one PCI card installed, ifmap=net2:nmn0 is the correct setting. If only one PCI card is installed, ifmap=net0:nmn0 is normally the correct setting.\nUANs require CPS and DVS to boot from images. These services are configured in dracut to retrieve the rootfs and mount it. If the image fails to download, check that DVS and CPS are both healthy, and DVS is running on all worker nodes. Run the following commands to check DVS and CPS:\nncn-m001# kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; --no-headers ncn-w001 ncn-w002 ncn-m001# for node in `kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; \\ --no-headers`; do ssh $node \u0026#34;lsmod | grep \u0026#39;^dvs \u0026#39;\u0026#34; done ncn-w001 ncn-w002 If DVS and CPS are both healthy, then both of these commands will return all the worker NCNs in the HPE Cray EX system.\nImage Boot Issues After dracut exits, the UAN will boot the rootfs image. Failures seen in this phase tend to be failures of spire-agent, cfs-state-reporter, or both. The cfs-state-reporter tells BOA that the node is ready and allows BOA to start CFS for Node Personalization. If cfs-state-reporter does not start, check if the spire-agent has started. The cfs-state-reporter depends on the spire-agent. Running systemctl status spire-agent will show that that service is enabled and running if there are no issues with that service. Similarly, running systemctl status cfs-state-reporter will show a status of SUCCESS.\nuan# systemctl status spire-agent  spire-agent.service - SPIRE Agent Loaded: loaded (/usr/lib/systemd/system/spire-agent.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2021-02-24 14:27:33 CST; 19h ago Main PID: 3581 (spire-agent) Tasks: 57 CGroup: /system.slice/spire-agent.service 3581 /usr/bin/spire-agent run -expandEnv -config /root/spire/conf/spire-agent.conf uan# systemctl status cfs-state-reporter  cfs-state-reporter.service - cfs-state-reporter reports configuration level of the system Loaded: loaded (/usr/lib/systemd/system/cfs-state-reporter.service; enabled; vendor preset: enabled) Active: inactive (dead) since Wed 2021-02-24 14:29:51 CST; 19h ago Main PID: 3827 (code=exited, status=0/SUCCESS) There may be errors related to failing to load kernel modules during the boot:\nFAILED Failed to start Load Kernel Modules. See \u0026#39;systemctl status systemd-modules-load.service\u0026#39; for details. Provided the UAN boots and completes post-boot customizations, these messages may be ignored.\n"
},
{
	"uri": "/docs-uan/en-271/troubleshooting/troubleshoot_uan_cfs_and_network_configuration_issues/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAN CFS and Network Configuration Issues Examine the UAN CFS pod logs to help troubleshoot CFS and networking issues on UANs.\nRead Basic UAN Configuration before starting this procedure.\nObtain the name of the CFS session that failed by running the following command on a management or worker NCN:\nThis example sorts the list of CFS sessions so that the most recent one is at the bottom.\nncn# kubectl -n services get pods --sort-by=.metadata.creationTimestamp | grep ^cfs View the Ansible log of the CFS session found in the previous step (CFS_SESSION in the following example). Use the information in the log to guide troubleshooting.\nncn# kubectl -n services logs -f -c ansible-0 CFS_SESSION Optional: Troubleshoot uan_interfaces issues by logging into the affected node (usually with the conman console) and using standard network debugging techniques.\nNMN and CAN/CHN network setup errors can also result from incorrect switch configuration and network cabling.\n"
},
{
	"uri": "/docs-uan/en-271/troubleshooting/troubleshoot_uan_disk_configuration_issues/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAN Disk Configuration Issues Perform this procedure to enable uan_disk_config to run successfully by erasing existing disk partitions. UAN disk configuration will fail if the disk on the node is already partitioned. Manually erase any existing partitions to fix the issue.\nThis procedure currently only addresses uan_disk_config errors due to existing disk partitions.\nSee Basic UAN Configuration for an explanation of UAN disk configuration.\nThe most common cause of failure in the uan_disk_config role is the disk having been previously configured without a /scratch and /swap partition. Existing partitions prevent the parted command from dividing the disk into those two equal partitions. The solution is to log into the node and run parted manually to remove the existing partitions on that disk.\nExamine the CFS log and identify the failed disk device.\nLog into the affected UAN as root.\nUse parted to manually remove any existing partitions.\nThe following example uses /dev/sdb as the disk device. Also, as partitions are removed, the remaining partitions are renumbered. Therefore, rm 1 is issued twice to remove both partitions.\nuan# parted GNU Parted 3.2 Using /dev/sda Welcome to GNU Parted! Type \u0026#39;help\u0026#39; to view a list of commands. (parted) select /dev/sdb Using /dev/sdb (parted) print Model: ATA VK000480GWSRR (scsi) Disk /dev/sdb: 480GB Sector size (logical/physical): 512B/4096B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 240GB 240GB primary ext4 type=83 2 240GB 480GB 240GB primary ext4 type=83 (parted) rm 1 (parted) rm 1 (parted) print Model: ATA VK000480GWSRR (scsi) Disk /dev/sdb: 480GB Sector size (logical/physical): 512B/4096B Partition Table: msdos Disk Flags: (parted) quit uan01:~ # Either reboot the affected UAN or launch a CFS session against it to rerun the uan_disk_config role.\n"
},
{
	"uri": "/docs-uan/en-271/test-plan/test-plan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Test Plan for User Access Node (UAN) and Repurposed Compute Node as UAN The following is the test plan for the User Access Node (UAN) or a Compute Node that has been repurposed to function as a UAN. The tests are grouped in three categories:\nUnit tests Integration tests Functional tests Unit Tests When possible, unit tests are run when the various components of UAN are built (uan-rpms, uan, and uan-product-stream). The uan repository contains the ansible code used when UANs boot and configure and much of the testing for that compoment must occur on a fully configured system. Future enhancements are underway to test the various compoments of UAN on a Virtual Shasta enviroment in GCP.\nSummary Description Automated Notes uan builds Verify the uan repo is able to generate artifacts yes Run make in a local checkout uan-rpms builds Verify the uan-rpms repo is able to generate rpms yes Run make in a local checkout uan-product-stream builds Verify the uan-product-stream repo is able to generate a release yes Tag a new release in uan-product-stream or run make in a local checkout Integration Tests The following integration tests verify that the UAN software interacts correctly with the rest of the products. The result of the integration tests will be a fully built and customized UAN image that boots and configures correctly. Depending on the configuration of the test system, extra integrations tests may be performed (HSN booting, WLM configuration, GPU configuration, etc).\nSummary Description Automated Notes Install the UAN product. Install using IUF. See Install UAN Product Stream for details. Yes Example IUF command: iuf -a install-uan-x.y.z -m /etc/cray/upgrade/csm/uan-x.y.z/ run -rv /etc/cray/upgrade/csm/uan-x.y.z/product_vars.yaml -sv /etc/cray/upgrade/csm/uan-x.y.z/site_vars.yaml -s management-nodes-rollout post-install-service-check post-install-check -bc /etc/cray/upgrade/csm/uan-x.y.z/compute-and-uan-bootprep.yaml Run UAN CFS Verify the CFS layers run correctly (SHS, UAN, WLM, CPE, etc) no Create UAN Boot Images Boot UANs Verify the UAN boots successfully no Boot UANs Enable HSN booting Verify the UAN is able to HSN boot no Consult COS documentation Enable GPU Verify the UAN is able to configure GPUs no Consult GPU documentation Functional Tests With a fully configured Shasta system, the following functional tests determines that a UAN is able to perform its intended capabilities. These tests apply to both native UANs and Compute Nodes which have been repurposed as UANs.\nSummary Description Automated Notes User Authentication Verify a user is able to ssh to the UAN using LDAP authentication. no ssh user@uan Job launch Verify a user is able to submit a basic job. no srun hostname Verify CPE Verify the Cray Programming Environment is available no module list Verify GPU functionality Run the test suite if GPUs are configured yes /opt/cray/uan/tests/validate-gpu.sh \u0026lt;nvidia|amd\u0026gt; Verify CAN or CHN Configuration Inspect the network interfaces and default routes used for CAN or CHN no For systems running CAN, there must be a can0 interface present and the default route should be over that device.For systems running CHN (including Compute Nodes repurposed as UANs), the hsn0 interface must have a CHN IP in addition to the HSN IP and the default route should be over the hsn0 device. Verify NMN Bonding Inspect the network interfaces used for NMN no For systems running a bonded NMN, there must be a nmnb0 interface present and nmn0 must be one of the bond slaves. Run cat /proc/net/bonding/nmnb0For systems running non-bonded NMN, the nmn0 interface must present. "
},
{
	"uri": "/docs-uan/en-271/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Cray EX User Access Nodes Installation and Administration Guide This document describes the installation prequisites, installation procedures, and operational procedures for Cray EX User Access Nodes (UAN).\nThe latest version of this documentation is available here.\nTable of Contents Topics: Installation Prereqs Prepare for UAN Product Installation Configure the BMC for UANs with iLO Configure the BIOS of an HPE UAN Install Install or Upgrade UAN Operations Basic UAN Configuration Build a New UAN Image Using a COS Recipe Create UAN Boot Images Mount a New File System on a UAN Configure Interfaces on UANs Configure Pluggable Authentication Modules (PAM) on UANs Boot UANs Mitigation for CVE-2023-0461 Advanced Enabling the Customer Access Network (CAN) or the Customer High Speed Network (CHN) Repurposing a Compute Node as a UAN Booting an Application Node with a SLE HPC Image (Technical Preview) Designating Application Nodes for K3s (Technical Preview) Configuring a UAN for K3s (Technical Preview) Upgrade Notable Changes Troubleshooting Troubleshoot UAN Boot Issues Troubleshoot UAN CFS and Network Configuration Issues Troubleshoot UAN Disk Configuration Issues Test Plan Test Plan for User Access Node (UAN) and Repurposed Compute Node as UAN UAN Ansible Roles UAN gpg keys UAN ca cert UAN disk config UAN hardening UAN interfaces UAN LDAP UAN motd UAN packages UAN shadow "
},
{
	"uri": "/docs-uan/en-271/test-plan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "test-plan Topics: Test Plan for User Access Node (UAN) and Repurposed Compute Node as UAN "
},
{
	"uri": "/docs-uan/en-271/upgrade/upgrades/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Upgrades Performing an upgrade of the HPE Cray Supercomputing UAN product from one version to the next follows the same general process as a fresh install. Some considerations may need to be made when merging the existing CFS configuration with the latest CFS configuration provided by the release.\nThe overall workflow for completing a UAN upgrade involves:\nPerform the UAN Installation\nReview any Notable Changes\nMerge UAN CFS Configuration Data\nCreate UAN images and reboot\n"
},
{
	"uri": "/docs-uan/en-271/operations/uan_shadow/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "uan_shadow The uan_shadow role configures the root password on UAN nodes.\nRequirements The root password hash has to be installed in HashiCorp Vault at secret/uan root_password.\nRole Variables Available variables are in the following list, including default values (see defaults/main.yml):\nuan_vault_url The URL for the HashiCorp Vault\nExample:\nuan_vault_url: \u0026#34;http://cray-vault.vault:8200\u0026#34; uan_vault_role_file The required Kubernetes role file for HashiCorp Vault access.\nExample:\nuan_vault_role_file: /var/run/secrets/kubernetes.io/serviceaccount/namespace uan_vault_jwt_file The path to the required Kubernetes token file for HashiCorp Vault access.\nExample:\nuan_vault_jwt_file: /var/run/secrets/kubernetes.io/serviceaccount/token uan_vault_path The path to use for storing data for UANs in HashiCorp Vault.\nExample:\nuan_vault_path: secret/uan uan_vault_key The key used for storing the root password in HashiCorp Vault.\nExample:\nuan_vault_key: root_password Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_shadow } This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-271/operations/uan_motd/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "uan_motd The uan_motd role appends text to the /etc/motd file.\nRequirements None.\nRole Variables Available variables are in the following list, including default values (see defaults/main.yml):\nuan_motd_content: [] uan_motd_content Contains text to be added to the end of the /etc/motd file.\nDependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_motd, uan_motd_content: \u0026#34;MOTD CONTENT\u0026#34; } This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-271/operations/uan_packages/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "uan_packages The uan_packages role adds or removes additional repositories and RPMs on UANs using the Ansible zypper_repository and zypper module.\nRepositories and packages added to this role will be installed or removed during image customization. Installing RPMs during post-boot node configuration can cause high system loads on large systems so these tasks run only during image customizations.\nThis role will only run on SLES-based nodes.\nRequirements Zypper must be installed.\nThe csm.gpg_keys Ansible role must be installed if uan_disable_gpg_check is false.\nRole Variables Available variables are in the following list, including default values (see defaults/main.yml):\nThis role uses the zypper_repository module. The name, description, repo, disable_gpg_check, and priority fields are supported.\nThis role uses the zypper modules. The name and disable_gpg_check fields are supported.\nuan_disable_gpg_check Sets the disable_gpg_check field on Zypper repos and packages listed in the uan_sles15_repositories add and uan_sles15_packages_add lists. The disable_gpg_check field can be overridden for each repo or package.\nuan_sles15_repositories_add List of repositories to add.\nuan_sles15_packages_add List of RPM packages to add.\nDependencies None.\nExample Playbook - hosts: Application_UAN roles: - role: uan_packages vars: uan_sles15_packages_add: - name: \u0026#34;foo\u0026#34; disable_gpg_check: yes - name: \u0026#34;bar\u0026#34; uan_sles15_packages_remove: - baz uan_sles15_repositories_add: - name: \u0026#34;uan-2.5.0-sle-15sp4\u0026#34; description: \u0026#34;UAN SUSE Linux Enterprise 15 SP4 Packages\u0026#34; repo: \u0026#34;https://packages.local/repository/uan-2.5.0-sle-15sp4\u0026#34; disable_gpg_check: no priority: 2 This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-271/operations/uan_hardening/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "uan_hardening The uan_hardening role configures site or customer-defined network security of UANs, for example preventing SSH access from the UAN over the NMN to NCN nodes.\nRequirements None.\nRole Variables Available variables are in the following list, including default values (see defaults/main.yml):\ndisable_ssh_out_nmn_to_management_ncns Boolean variable controlling whether firewall rules are applied at the UAN to prevent SSH outbound over the NMN to the NCN management nodes.\nThe default value of disable_ssh_out_nmn_to_management_ncns is yes.\ndisable_ssh_out_nmn_to_management_ncns: yes disable_ssh_out_uan_to_nmn_lb Boolean variable controlling whether firewall rules are applied at the UAN to prevent SSH outbound over the NMN to NMN LB IP addresses.\nThe default value of disable_ssh_out_uan_to_nmn_lb is yes.\ndisable_ssh_out_uan_to_nmn_lb: yes Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_hardening} This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-271/operations/uan_interfaces/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "uan_interfaces The uan_interfaces role configures site or customer-defined network interfaces and Customer Access Network (CAN) network interfaces on UAN nodes.\nRequirements None.\nRole Variables Available variables are in the following list, including default values (see defaults/main.yml):\nuan_nmn_bond A Boolean variable controlling the configuration of the Node Management Network (NMN). When true, the NMN network connection will be configured as a bonded pair of interfaces defined by the members of the uan_nmn_bond_slaves variable. The bonded NMN interface is named nmnb0. When false, the NMN network connection will be configured as a single interface named nmn0.\nThe default value of uan_nmn_bond is no.\nuan_nmn_bond: no uan_nmn_bond_slaves A list of the interfaces to use as the bond slave pair when uan_nmn_bond is true.\nThe interface names must be in a format which does not change between reboots of the node, such as ens10f0 which is the first port of the NIC in slot 10.\nNOTE: ens10f0 is typically the first port of the OCP 25Gb card that the node PXE boots.\nIMPORTANT: The first interface in the list must be the nmn0 interface which is configured during the initial image boot, typically ens10f0. This interface order is required because the MAC address of the nmn0 interface is the MAC associated with the IP address of the UAN. The bonded nmnb0 interface and the bond slaves will assume this MAC and the IP address of nmn0 to preserve connectivity.\nThe second interface is typically the first port of a different 25Gb NIC for resiliency.\nThe default values of uan_nmn_bond_slaves are shown here. Change them as needed to match the actual node cabling and NIC configuration.\nuan_nmn_bond_slaves: - \u0026#34;ens10f0\u0026#34; - \u0026#34;ens1f0\u0026#34; uan_can_setup Boolean variable controlling the configuration of user access to UAN nodes. When true, user access is configured over either the Customer Access Network (CAN) or Customer High Speed Network (CHN), depending on which is configured on the system.\nWhen uan_can_setup is false, user access over the CAN or CHN is not configured on the UAN nodes, and no default route is configured. The Admin must then specify the default route in customer_uan_routes.\nThe default value of uan_can_setup is no.\nuan_can_setup: no uan_can_bond_slaves A list of the interfaces to use as the bond slave pair when uan_can_setup is true, uan_nmn_bond is true, and the Customer Access Network (CAN) is configured on the system. This variable is ignored if uan_nmn_bond is false.\nThe interface names must be in a format which does not change between reboots of the node, such as ens10f1 which is the second port of the NIC in slot 10.\nNOTE: ens10f1 is typically the second port of the OCP 25Gb card and is used as one of the bond slaves in the CAN bond0 interface.\nThe second interface is typically the second port of a different 25Gb NIC for resiliency.\nThe default values of uan_can_bond_slaves are shown here. They may need to be changed to match the actual node cabling and NIC configuration.\nuan_can_bond_slaves: - \u0026#34;ens10f1\u0026#34; - \u0026#34;ens1f1\u0026#34; uan_chn_device The default CHN device on the UAN nodes. Overwrite the default value to use a different device for the CHN on UAN nodes.\nThe default value of uan_chn_device is shown here.\nuan_chn_device: \u0026#34;hsn0\u0026#34; uan_customer_default_route Boolean variable that allows the default route to be set by the customer_uan_routes data when uan_can_setup is true.\nBy default, no default route is setup unless uan_can_setup is true, which sets the default route to the CAN or CHN.\nuan_customer_default_route: no sls_nmn_name Node Management Network name used by SLS. This value must not be changed.\nsls_nmn_name: \u0026#34;NMN\u0026#34; sls_nmn_svcs_name Node Management Services Network name used by SLS. This value must not be changed.\nsls_nmn_svcs_name: \u0026#34;NMNLB\u0026#34; sls_mnmn_svcs_name Mountain Node Management Services Network name used by SLS. This value must not be changed.\nsls_mnmn_svcs_name: \u0026#34;NMN_MTN\u0026#34; uan_required_dns_options List of DNS options. By default, single-request is set and must not be removed.\nuan_required_dns_options: - \u0026#39;single-request\u0026#39; customer_uan_interfaces List of interface names used for constructing ifcfg-\u0026lt;customer_uan_interfaces.name\u0026gt; files. Define the ifcfg fields for each interface here. Field names are converted to uppercase in the generated ifcfg-\u0026lt;name\u0026gt; file or files.\nInterfaces must be defined in order of dependency.\ncustomer_uan_interfaces: [] # Example: customer_uan_interfaces: - name: \u0026#34;net1\u0026#34; settings: bootproto: \u0026#34;static\u0026#34; device: \u0026#34;net1\u0026#34; ipaddr: \u0026#34;1.2.3.4\u0026#34; startmode: \u0026#34;auto\u0026#34; - name: \u0026#34;net2\u0026#34; settings: bootproto: \u0026#34;static\u0026#34; device: \u0026#34;net2\u0026#34; ipaddr: \u0026#34;5.6.7.8\u0026#34; startmode: \u0026#34;auto\u0026#34; `customer_uan_routes List of interface routes used for constructing ifroute-\u0026lt;customer_uan_routes.name\u0026gt; files.\ncustomer_uan_routes: [] # Example customer_uan_routes: - name: \u0026#34;net1\u0026#34; routes: - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; - name: \u0026#34;net2\u0026#34; routes: - \u0026#34;default 10.103.8.20 255.255.255.255 - table 3\u0026#34; - \u0026#34;10.103.8.128/25 10.103.8.20 255.255.255.255 net2\u0026#34; customer_uan_rules List of interface rules used for constructing ifrule-\u0026lt;customer_uan_routes.name\u0026gt; files.\ncustomer_uan_rules: [] # Example customer_uan_rules: - name: \u0026#34;net1\u0026#34; rules: - \u0026#34;from 10.1.0.0/16 lookup 1\u0026#34; - name: \u0026#34;net2\u0026#34; rules: - \u0026#34;from 10.103.8.0/24 lookup 3\u0026#34; customer_uan_global_routes List of global routes used for constructing the \u0026ldquo;routes\u0026rdquo; file.\ncustomer_uan_global_routes: [] # Example customer_uan_global_routes: - routes: - \u0026#34;10.92.100.0 10.252.0.1 255.255.255.0 -\u0026#34; - \u0026#34;10.100.0.0 10.252.0.1 255.255.128.0 -\u0026#34; external_dns_searchlist List of customer-configurable fields to be added to the /etc/resolv.conf DNS search list.\nexternal_dns_searchlist: [ \u0026#39;\u0026#39; ] # Example external_dns_searchlist: - \u0026#39;my.domain.com\u0026#39; - \u0026#39;my.other.domain.com\u0026#39; external_dns_servers List of customer-configurable fields to be added to the /etc/resolv.conf DNS server list.\nexternal_dns_servers: [ \u0026#39;\u0026#39; ] # Example external_dns_servers: - \u0026#39;1.2.3.4\u0026#39; - \u0026#39;5.6.7.8\u0026#39; external_dns_options List of customer-configurable fields to be added to the /etc/resolv.conf DNS options list.\nexternal_dns_options: [ \u0026#39;\u0026#39; ] # Example external_dns_options: - \u0026#39;single-request\u0026#39; uan_access_control Boolean variable to control whether non-root access control is enabled. Default is no.\nuan_access_control: no api_gateways List of API gateway DNS names to block non-user access\napi_gateways: - \u0026#34;api-gw-service\u0026#34; - \u0026#34;api-gw-service.local\u0026#34; - \u0026#34;api-gw-service-nmn.local\u0026#34; - \u0026#34;kubeapi-vip\u0026#34; api_gw_ports List of gateway ports to protect.\napi_gw_ports: \u0026#34;80,443,8081,8888\u0026#34; sls_url The SLS URL.\nsls_url: \u0026#34;http://cray-sls\u0026#34; Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_interfaces } This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-271/operations/uan_ldap/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "uan_ldap The uan_ldap role configures LDAP and AD groups on UAN nodes.\nRequirements NSCD, pam-config, sssd.\nRole Variables Available variables are in the following list, including default values (see defaults/main.yml):\nuan_ldap_setup A Boolean variable to selectively skip the setup of LDAP on nodes it would otherwise be configured due to uan_ldap_config being defined. The default setting is to setup LDAP when uan_ldap_config is not empty.\nExample:\nuan_ldap_setup: yes uan_ldap_config Configures LDAP domains and servers. If this list is empty, no LDAP configuration will be applied to the UAN targets, and all role tasks will be skipped.\nExample:\nuan_ldap_config: - domain: \u0026#34;mydomain-ldaps\u0026#34; search_base: \u0026#34;dc=...,dc=...\u0026#34; servers: [\u0026#34;ldaps://123.123.123.1\u0026#34;, \u0026#34;ldaps://213.312.123.132\u0026#34;] chpass_uri: [\u0026#34;ldaps://123.123.123.1\u0026#34;] - domain: \u0026#34;mydomain-ldap\u0026#34; search_base: \u0026#34;dc=...,dc=...\u0026#34; servers: [\u0026#34;ldap://123.123.123.1\u0026#34;, \u0026#34;ldap://213.312.123.132\u0026#34;] uan_ad_groups Configures active directory groups on UAN nodes.\nExample:\nuan_ad_groups: - { name: admin_grp, origin: ALL } - { name: dev_users, origin: ALL } uan_pam_modules Configures PAM modules on the UAN nodes in /etc/pam.d.\nExample:\nuan_pam_modules: - name: \u0026#34;common-account\u0026#34; lines: - \u0026#34;account required\\tpam_access.so\u0026#34; Dependencies None.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_ldap } This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-271/operations/uan_disk_config/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "uan_disk_config The uan_disk_config role configures swap and scratch disk partitions on UAN nodes.\nRequirements There must be disk devices found on the UAN node by the device_filter module or this role will exit with failure. This condition can be ignored by setting uan_require_disk to false. See the following list for variable definitions.\nSee the library/device_filter.py file for more information on this module.\nThe device that is found will be unmounted if mounted and a swap partition will be created on the first half of the disk. A scratch partition will be created on the second half. ext4 filesystems are created on each partition.\nRole Variables Available variables are in the following list, including default values (see defaults/main.yml):\nuan_require_disk Boolean to determine if this role continues to setup disk if no disks were found by the device filter. Set to true to exit with error when no disks are found.\nExample:\nuan_require_disk: false uan_device_name_filter Regular expression of disk device name for this role to filter. Input to the device_filter module.\nExample:\nuan_device_name_filter: \u0026#34;^sd[a-f]$\u0026#34; uan_device_host_filter Regular expression of host for this role to filter. Input to the device_filter module.\nExample:\nuan_device_host_filter: \u0026#34; uan_device_model_filter Regular expression of the device model for this role to filter. Input to the device_filter module.\nExample:\nuan_device_model_filter: \u0026#34; uan_device_vendor_filter Regular expression of the disk vendor for this role to filter. Input to the device_filter module.\nExample:\nuan_device_vendor_filter: \u0026#34; uan_device_size_filter Regular expression of disk size for this role to filter. Input to the device_filter module.\nExample:\nuan_device_size_filter: \u0026#34;\u0026lt;1TB\u0026#34; uan_swap Filesystem location to mount the swap partition.\nExample:\nuan_swap: \u0026#34;/swap\u0026#34; uan_scratch Filesystem location to mount the scratch partition.\nExample:\nuan_scratch: \u0026#34;/scratch\u0026#34; swap_file Name of the swap file to create. Full path is \u0026lt;uan_swap\u0026gt;/\u0026lt;swapfile\u0026gt;.\nExample:\nswap_file: \u0026#34;swapfile\u0026#34; swap_dd_command dd command to create the swapfile.\nExample:\nswap_dd_command: \u0026#34;/usr/bin/dd if=/dev/zero of={{ uan_swap }}/{{ swap_file }} bs=1GB count=10\u0026#34; swap_swappiness Value to set the swapiness in sysctl.\nExample:\nswap_swappiness: \u0026#34;10\u0026#34; Dependencies library/device_filter.py is required to find eligible disk devices.\nExample Playbook - hosts: Application_UAN roles: - { role: uan_disk_config } This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-271/operations/uan_gpg_keys/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "uan_gpg_keys The uan_gpg_keys role installs the CSM GPG signing public key. This role is a dependency of the uan_packages role.\nRequirements The Kubernetes secret must be available in the namespace and field specified by the following uan_gpg_key_* variables. The key must be stored as a base64-encoded string.\nRole Variables Available variables are in the following list, including default values (defined in defaults/main.yml):\nuan_gpg_key_k8s_secret The Kubernetes secret which contains the GPG public key.\nExample:\nuan_gpg_key_k8s_secret: \u0026#34;hpe-signing-key\u0026#34; uan_gpg_key_k8s_namespace The Kubernetes namespace which contains the secret.\nExample:\nuan_gpg_key_k8s_namespace: \u0026#34;services\u0026#34; uan_gpg_key_k8s_field The field in the Kubernetes secret that holds the GPG public key.\nExample:\nuan_gpg_key_k8s_field: \u0026#34;gpg-pubkey\u0026#34; Dependencies None.\nExample Playbook - hosts: Application roles: - role: uan_gpg_key This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-271/operations/uan_ca_cert/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "uan_ca_cert The uan_ca_cert role installs a CA certificate onto the system.\nRequirements None.\nRole Variables None.\nDependencies None.\nExample Playbook - hosts: Application roles: - role: uan_ca_cert This role is included in the UAN site.yml play.\n"
},
{
	"uri": "/docs-uan/en-271/advanced/sles_image/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Booting an Application Node with a SLE HPC Image (Technical Preview) A SLE HPC image is available for use with Application Node types such as gateways and LNet routers. This image is considered a \u0026ldquo;Technical Preview\u0026rdquo; as the initial support for booting with SLE HPC Images without COS. This Application Node image differs from the standard COS-based image because this image does not include the COS kernel and libraries that Compute Nodes (CNs) have.\nThe image is built from the same pipeline as Non-Compute Node (NCN) Images. Similarities may be noticed including the kernel and package versions.\nThis procedure documents how to boot and configure this image.\nLimitations As this feature is a \u0026ldquo;Technical Preview\u0026rdquo; of supporting SLE HPC Images on Application Nodes, there are several limitations:\nCFS Configurations that operate on COS and NCN images are not yet supported. CFS Node Personalization must be started manually. Procedure Overview The following steps outline the process of configuring and booting an Application Node with the SLE HPC Image.\nDetermine the image to use.\nCustomize the image with SAT and generate a BOS Session Template\nUpdate BOS Session Template with the necessary parameters.\nReboot the node.\nProcedure Perform the following steps to configure and boot a SLE HPC image on an Application type node.\nLog in to the master node ncn-m001. All commands in this procedure are run from the master node.\nVerify that the UAN release contains a SLES image.\nncn-m001# UAN_RELEASE=@product_version@ ncn-m001# sat showrev --no-headings --filter \u0026#34;product_version = $UAN_RELEASE\u0026#34; --filter \u0026#34;product_name = uan\u0026#34; Select an Image to boot or customize. ncn-m001# APP_IMAGE_NAME=cray-application-sles15sp5.x86_64-5.2.42 ncn-m001# APP_IMAGE_ID=$(cray ims images list --format json | jq --arg APP_IMAGE_NAME \u0026#34;$APP_IMAGE_NAME\u0026#34; -r \u0026#39;sort_by(.created) | .[] | select(.name == $APP_IMAGE_NAME ) | .id\u0026#39; | head -1) ncn-m001# cray ims images describe $APP_IMAGE_ID --format json { \u0026#34;arch\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;created\u0026#34;: \u0026#34;2023-11-06T15:47:10.564555+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;51b19502-a13b-42ab-a653-b1ee7e6a7fb5\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/51b19502-a13b-42ab-a653-b1ee7e6a7fb5/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;cray-application-sles15sp5.aarch64-5.2.42\u0026#34; } Customize the image using SAT Bootprep. These commands will add a root password to the image as one is not included. Support for additional product layers will be added in subsequent releases. Update the fields below for the correct software versions, branch names, and ncn-m001# cat product_vars.yml recipe: version: app-sle-1 csm: version: 1.5.0 branch: cray/csm/1.16.22 uan: version: @product_version@ branch: integration-@product_version@ ncn-m001# cat bootprep.yml configurations: - name: \u0026#34;{{recipe.version}}\u0026#34; layers: - name: csm playbook: csm_packages.yml product: name: csm version: \u0026#34;{{csm.version}}\u0026#34; branch: \u0026#34;{{csm.branch}}\u0026#34; - name: uan playbook: site.yml product: name: uan version: \u0026#34;{{uan.version}}\u0026#34; branch: \u0026#34;{{uan.branch}}\u0026#34; - name: uan-{{uan.version}} playbook: rebuild-initrd.yml product: name: uan version: \u0026#34;{{uan.version}}\u0026#34; branch: \u0026#34;{{uan.branch}}\u0026#34; images: - name: \u0026#34;{{recipe.version}}\u0026#34; ims: is_recipe: false name: cray-application-sles15sp5.x86_64-5.2.42 configuration: \u0026#34;{{recipe.version}}\u0026#34; configuration_group_names: - Application - Application_UAN session_templates: - name: \u0026#34;{{recipe.version}}\u0026#34; image: ims: name: \u0026#34;{{recipe.version}}\u0026#34; configuration: \u0026#34;{{recipe.version}}\u0026#34; bos_parameters: boot_sets: uan: kernel_parameters: console=ttyS0,115200 bad_page=panic crashkernel=512M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu.passthrough=on modprobe.blacklist=amdgpu numa_interleave_omit=headless oops=panic pageblock_order=14 rd.neednet=1 rd.retry=10 rd.shell split_lock_detect=off systemd.unified_cgroup_hierarchy=1 ip=:::::eth0:dhcp:10.92.100.225:169.254.169.254 quiet spire_join_token=${SPIRE_JOIN_TOKEN} root=live:s3://boot-images/REPLACE_ME/rootfs psi=1 node_roles_groups: - Application rootfs_provider_passthrough: \u0026#34; rootfs_provider: \u0026#34; ncn-m001# sat bootprep run --vars-file product_vars.yml bootprep.yml Once SAT has completed successfully, the BOS session template must be updated to reference the correct root kernel parameter. This involves replacing \u0026ldquo;REPLACE_ME\u0026rdquo; with the resultant image ID from SAT bootprep. The image ID may be determined by first querying IMS. ncn-m001# RECIPE_NAME=app-sle-1 ncn-m001# RESULTANT_ID=$(cray ims images list --format json | jq --arg imgname \u0026#34;$RECIPE_NAME\u0026#34; -r \u0026#39;.[] | select(.name == $imgname) | .id\u0026#39; | head -1) ncn-m001# cray bos sessiontemplates describe $RECIPE_NAME --format json | jq \u0026#39;del(.name,.tenant)\u0026#39; | sed -e \u0026#34;s/REPLACE_ME/$RESULTANT_ID/g\u0026#34; \u0026gt; $RECIPE_NAME.json ncn-m001# cray bos sessiontemplates update $RECIPE_NAME --file $RECIPE_NAME.json Use the session template to boot one or more nodes. ncn-m001# cray bos sessions create --operation reboot --template-name $RECIPE_NAME --limit \u0026lt;xname(s)\u0026gt; If the node does not complete the boot successfully, proceed to the troubleshooting section in this guide. Troubleshooting Some general troubleshooting tips may help in getting started using the SLE HPC image.\nDracut failures during booting Could not find the kernel or the initrd. Verify the BSS boot parameters for the node. Specifically, check that the IMS Image ID is correct.\nhttp://rgw-vip.nmn/boot-images/13964414-bbad-40e9-9e31-a3683010febbasdf/kernel...HTTP 0x7f0fa808 status 404 Not Found No such file or directory (http://ipxe.org/2d0c618e) http://rgw-vip.nmn/boot-images/13964414-bbad-40e9-9e31-a3683010febbasdf/initrd...HTTP 0x7f0fa808 status 404 Not Found No such file or directory (http://ipxe.org/2d0c618e) The dracut module livenet is missing from the initrd. Make sure that the initrd was regenerated with /srv/cray/scripts/common/create-ims-initrd.sh if CFS was used.\n2022-08-24 14:48:53 [ 5.784023] dracut: FATAL: Don\u0026#39;t know how to handle \u0026#39;root=live:http://rgw-vip/boot-images/e88ed416-5d58-4421-9013-fa2171ac11b8/rootfs?AWSAccessKeyId=I43RBLH07R65TRO3AL02\u0026amp;Signature=bL661kZHPyEgBsLLEuJHFz3zKVs%3D\u0026amp;Expires=1661438587 2022-08-24 14:48:53 [ 5.805063] dracut: Refusing to continue Unable to log in to the node The node is not up. Connect to the console and determine why the node has not booted, starting with the troubleshooting tips. ncn-m001:# ssh app01 ssh: connect to host uan01 port 22: No route to host Unable to log in to the node with a password. No root password is defined in the image by default, one must be added by CFS or by modifying the squashfs filesystem. ncn-m001:# ssh app01 Password: Password: Password: root@app01\u0026#39;s password: Permission denied, please try again DHCP hostname is not set If the node does not have a hostname assigned from DHCP, try verifying the DHCP settings and restarting wicked.\nx3000c0s13b0n0:~ # grep -R ^DHCLIENT_SET_HOSTNAME= /etc/sysconfig/network/dhcp DHCLIENT_SET_HOSTNAME=\u0026#34;yes\u0026#34; x3000c0s13b0n0:# systemctl restart wicked x3000c0s13b0n0:# hostnamectl Static hostname: x3000c0s13b0n0 Transient hostname: app01 Icon name: computer-server Chassis: server Machine ID: 9bd0aacf29d04dd4827bc464121b130b Boot ID: af753b4e6fa9419bb14d55a029d0f526 Operating System: SUSE Linux Enterprise High Performance Computing 15 SP3 CPE OS Name: cpe:/o:suse:sle_hpc:15:sp3 Kernel: Linux 5.3.18-150300.59.43-default Architecture: x86-64 x3000c0s13b0n0:# hostname app01 Spire is not running Check the spire-agent logs for error messages.\napp01# systemctl status spire-agent "
},
{
	"uri": "/docs-uan/en-271/install/install_the_uan_product_stream/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Install or Upgrade UAN Install and Upgrade Framework The Install and Upgrade Framework (IUF) provides commands which install, upgrade, and deploy products on systems managed by CSM. IUF capabilities are described in detail in the IUF section of the Cray System Management Documentation. The initial install and upgrade workflows described in the HPE Cray EX System Software Stack Installation and Upgrade Guide for CSM (S-8052) detail when and how to use IUF with a new release of UAN or any other HPE Cray EX product.\nThis document does not replicate the install, upgrade, or deployment procedures detailed in the IUF section of the Cray System Management Documentation. This document provides details regarding software and configuration content specific to UAN which may be needed when installing, upgrading, or deploying a UAN release. The IUF section of the Cray System Management Documentation will indicate when sections of this document must be seen for detailed information.\nIUF will perform the following tasks for a release of the HPE Cray Supercomputing UAN product software.\nIUF process-media stage: Inventory and extract the UAN products in the media directory for use in subsequent stages IUF pre-install-check stage: Perform pre-install readiness checks IUF deliver-product stage: Uploads UAN configuration content to VCS Uploads UAN information to the CSM product catalog Uploads UAN content to Nexus repositories Uploads the UAN Stock Kernel image to IMS IUF update-vcs-config stage: Updates the VCS integration branch with new UAN configuration content IUF update-cfs-config stage: Creates new CFS configurations with new UAN configuration content IUF prepare-images stage: Creates updated UAN images based on COS with new UAN content IUF managed-nodes-rollout stage: Boots UAN nodes with an image containing new UAN content IUF uses a variety of CSM and SAT tools when performing these tasks. The IUF section of the Cray System Management Documentation describes how to use these tools directly if it is desirable to use them instead of IUF.\nIUF Resource Files IUF uses the following files to drive the install or upgrade of UAN. These files are provided by the hpc-csm-software-recipe VCS repository.\nproduct_vars.yaml (Required) Contains the list of products and versions to be installed together. It also contains the working_branch variables for the products. This file is in a directory under /etc/cray/upgrade/csm/. For example, /etc/cray/upgrade/csm/admin. The path to this file is defined on the iuf command line using the IUF recipe vars (-rv) option. For example, -rv /etc/cray/upgrade/csm/admin. site_vars.yaml (Required) This file allows the administrator to override values in product_vars.yaml and defines the site VCS branching strategy. It may be placed in any directory under /etc/cray/upgrade/csm. The path to this file is defined on the iuf command line using the IUF site vars (-sv) option. For example, -sv /etc/cray/upgrade/csm/admin/site_vars.yaml. compute-and-uan-bootprep.yaml (Required) This file is typically installed in the /etc/cray/upgrade/csm/bootprep directory and defines variables for the following tasks: Create a compute node image which will be configured for use on UAN Create the UAN CFS Configuration Create the UAN BOS session template This path to this file is defined on the iuf command line using the IUF bootprep-config-managed (-bc) options. For example, -bc /etc/cray/upgrade/csm/bootprep/compute-and-uan-bootprep.yaml. IUF Stage Details for UAN This section describes any UAN details that an administrator must be aware of before executing IUF stages. Entries are prefixed with Information if no administrative action is required or Action if an administrator must perform tasks outside of IUF.\nprocess-media Action: Before executing this stage, the administrator must ensure that the UAN product tar file is in a media directory under /etc/cray/upgrade/csm/. When more than one product is being installed, place all the product tar files in the same directory.\nupdate-vcs-config Action: Before executing this stage, the administrator must ensure the IUF site variables file (see iuf -sv SITE_VARS) is updated to reflect site preferences, including the wanted VCS branching configuration. The update-vcs-config stage will use the branching configuration when modifying UAN branches in VCS.\nupdate-cfs-config Action: Before executing this stage, any site-local UAN configuration changes must be made so that the following stages execute using the wanted UAN configuration values. See the Basic UAN Configuration section of this documentation for UAN configuration content details. The Prepare for UAN Product Installation section is required for fresh installation scenarios.\nUAN Content Installed The following subsections describe most of the UAN content installed and configured on the system by IUF. The new version of UAN (2.6.XX) and its artifacts will be displayed in the CSM product catalog alongside any previously released version of UAN and its artifacts.\nConfiguration UAN provides configuration content in the form of Ansible roles and plays. This content is uploaded to a VCS repository in a branch with a specific UAN version number. That release number, such as (2.6.XX) distinguishes it from any previously released UAN configuration content. This content is described in detail in the Basic UAN Configuration section.\nFor application nodes based on COS, the COS compute image is used as the base application node image and two COS CFS layers are required. The first COS CFS layer runs the cos-application.yml Ansible playbook and ensures that the COS content is applied as part of the image customization and node personalization processes. This COS CFS layer must precede the UAN CFS layer in the UAN CFS configuration. A second COS CFS layer running the Ansible playbook, cos-application-after.yml, runs after the UAN CFS layer of the UAN CFS configuration. This second COS CFS layer ensures that the application node initrd is rebuilt and that any customer-defined filesystems are configured.\nInput files for sat bootprep are provided in the hpc-csm-software-recipe VCS repository and include COS components in the CFS configuration, node image, and BOS session template definitions. The compute-and-uan-bootprep.yaml input file is used for compute and application nodes.\nProcedure to Set Root Password For UAN/Application Nodes The following instructions describe how to set the root password for UAN/Application nodes.\nObtain the HashiCorp Vault root token.\nncn-m001# kubectl get secrets -n vault cray-vault-unseal-keys \\ -o jsonpath=\u0026#39;{.data.vault-root}\u0026#39; | base64 -d; echo Log into the HashiCorp Vault pod.\nncn-m001# kubectl exec -itn vault cray-vault-0 -c vault -- sh After you are attached to the pod\u0026rsquo;s shell, log into vault and read the secret/uan key by executing the following commands. If the secret is empty, \u0026ldquo;No value found at secret/uan\u0026rdquo; will be displayed.\npod# export VAULT_ADDR=http://cray-vault:8200 pod# vault login pod# vault read secret/uan If no value is found for this key, complete the following steps in another shell on the NCN management node.\na. Generate the password HASH for the root user. Replace \u0026lsquo;PASSWORD\u0026rsquo; with a chosen root password.\nncn-m001# openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc A-Z-a-z-0-9 | head -c4) PASSWORD b. Take the HASH value returned from the previous command and enter the following in the vault pod\u0026rsquo;s shell. Instead of HASH, use the value returned from the previous step. You must escape the HASH value with the single quote to preserve any special characters that are part of the HASH value. If you previously have exited the pod, repeat the previous Steps 1-3; there is no need to perform the vault read since the content is empty.\npod# vault write secret/uan root_password=\u0026#39;HASH\u0026#39; c. Verify that the new hash value is stored.\npod# vault read secret/uan ... pod# exit Optional Write any uan_ldap sensitive data, such as the ldap_default_authtok value, to the HashiCorp Vault.\nThe vault login command will request a token. That token value is the output of the Step 1. The vault read secret/uan_ldap command verifies that the uan_ldap data was stored correctly. Any values stored here will be written to the UAN /etc/sssd/sssd.conf file in the [domain] section by CFS.\nThis example shows storing a value for ldap_default_authtok. If more than one variable must be stored, they must be written in space separated key=value pairs on the same vault write secret/uan_ldap command line.\nncn-m001# kubectl exec -it -n vault cray-vault-0 -- sh export VAULT_ADDR=http://cray-vault:8200 vault login vault write secret/uan_ldap ldap_default_authtok=\u0026#39;TOKEN\u0026#39; vault read secret/uan_ldap SLE HPC Image for Application Nodes The UAN product provides an Application Node image. This image is based on SUSE Linux Enterprise High Performance Computing 15 (SLE HPC 15) and uses a \u0026ldquo;stock\u0026rdquo; (that is, not customized for HPE Cray systems) kernel. Customers can use this image on Application nodes that do not require any COS compatibility as UAN does. Unlike the default UAN image, this Application Node image is not based on the COS image. However, like the UAN default image, it is uploaded to IMS as part of the installation process.\nCustomers must finish the installation or upgrade of the UAN product before booting an Application Node with SLE HPC 15 provided by that UAN product release. See Booting an Application Node with a SLES Image (Technical Preview) for more information about this image and instructions on deploying it to Application Nodes.\nRPMs UAN provides RPMs used on UAN nodes. The RPMs are uploaded to Nexus as part of the installation process.\nThe following Nexus raw repositories are created:\nuan-2.6.XX-sle-15sp4 uan-2.6.XX-sle-15sp3 The following Nexus group repositories are created and reference the preceding Nexus raw repos.\nuan-2.6-sle-15sp4 uan-2.6-sle-15sp3 The uan-2.6-sle-15sp4 and uan-2.6-sle-15sp3 Nexus group repositories are used when building UAN node images and are accessible on UAN nodes after boot.\n"
},
{
	"uri": "/docs-uan/en-271/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/docs-uan/en-271/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]